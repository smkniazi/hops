diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java
index 05edd10e756..38e37fc3839 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/StorageType.java
@@ -37,7 +37,8 @@
   SSD(false),
   RAID5(false),
   ARCHIVE(false),
-  DB(false);
+  DB(false),
+  CLOUD(false);
 
   private final boolean isTransient;
 
@@ -92,4 +93,4 @@ public static StorageType parseStorageType(String s) {
     }
     return nonTransientTypes;
   }
-}
\ No newline at end of file
+}
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
index 74b73b51dda..fd734a74a8b 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java
@@ -2826,7 +2826,10 @@ void logException(Log logger, Throwable e, Call call) {
       // on the server side, as opposed to just a normal exceptional
       // result.
       logger.warn(logMsg, e);
-    } else {
+    } else if(e instanceof  IOException && e.getMessage().contains("Not replicated yet")){
+      logger.debug(logMsg, e);
+    }
+    else {
       logger.info(logMsg, e);
     }
   }
diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java
index 32733906da5..5af70813c52 100644
--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java
+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsClientConfigKeys.java
@@ -109,7 +109,7 @@
     String  RETRIES_KEY = PREFIX + "retries";
     int     RETRIES_DEFAULT = 3;
     String  LOCATEFOLLOWINGBLOCK_RETRIES_KEY = PREFIX + "locateFollowingBlock.retries";
-    int     LOCATEFOLLOWINGBLOCK_RETRIES_DEFAULT = 5;
+    int     LOCATEFOLLOWINGBLOCK_RETRIES_DEFAULT = 10;
     String  LOCATEFOLLOWINGBLOCK_INITIAL_DELAY_MS_KEY = PREFIX + "locateFollowingBlock.initial.delay.ms";
     int     LOCATEFOLLOWINGBLOCK_INITIAL_DELAY_MS_DEFAULT = 400;
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/Block.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/Block.java
index a5d320f8b13..66a095911af 100644
--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/Block.java
+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/Block.java
@@ -18,7 +18,12 @@
 package org.apache.hadoop.hdfs.protocol;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableFactories;
+import org.apache.hadoop.io.WritableFactory;
 
 import java.io.DataInput;
 import java.io.DataOutput;
@@ -26,7 +31,6 @@
 import java.io.IOException;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
-import org.apache.hadoop.io.*;
 
 /**************************************************
  * A Block is a Hadoop FS primitive, identified by a
@@ -37,6 +41,9 @@
 @InterfaceAudience.Private
 @InterfaceStability.Evolving
 public class Block implements Writable, Comparable<Block> {
+
+  static final Log LOG = LogFactory.getLog(Block.class);
+
   public static final String BLOCK_FILE_PREFIX = "blk_";
   public static final String METADATA_EXTENSION = ".meta";
 
@@ -95,30 +102,34 @@ public static long getBlockId(String metaOrBlockFile) {
   }
 
   private static long NON_EXISTING_BLK_ID = Long.MIN_VALUE;
+  public static short NON_EXISTING_BUCKET_ID = -1;
   private long blockId;
   private long numBytes;
   private long generationStamp;
+  private short cloudBucketID = NON_EXISTING_BUCKET_ID;
 
   public Block() {
-    this(NON_EXISTING_BLK_ID, 0, 0);
+    this(NON_EXISTING_BLK_ID, 0, 0, NON_EXISTING_BUCKET_ID);
   }
 
-  public Block(final long blkid, final long len, final long generationStamp) {
-    setNoPersistance(blkid, len, generationStamp);
+  public Block(final long blkid, final long len, final long generationStamp,
+               final short cloudBucketID) {
+    setNoPersistance(blkid, len, generationStamp, cloudBucketID);
   }
 
   public Block(final long blkid) {
-    this(blkid, 0, HdfsConstantsClient.GRANDFATHER_GENERATION_STAMP);
+    this(blkid, 0, HdfsConstantsClient.GRANDFATHER_GENERATION_STAMP, NON_EXISTING_BUCKET_ID);
   }
 
   public Block(Block blk) {
-    this(blk.blockId, blk.numBytes, blk.generationStamp);
+    this(blk.blockId, blk.numBytes, blk.generationStamp, blk.cloudBucketID);
   }
 
-  public void setNoPersistance(long blkid, long len, long genStamp) {
+  public void setNoPersistance(long blkid, long len, long genStamp, short cloudBucketID) {
     this.blockId = blkid;
     this.numBytes = len;
     this.generationStamp = genStamp;
+    this.cloudBucketID = cloudBucketID;
   }
 
   /**
@@ -155,6 +166,19 @@ public void setGenerationStampNoPersistance(long stamp) {
     generationStamp = stamp;
   }
 
+  public short getCloudBucketID(){
+    return cloudBucketID;
+  }
+
+  public void setCloudBucketIDNoPersistance(final short cloudBucketID){
+    this.cloudBucketID = cloudBucketID;
+  }
+
+  public boolean isProvidedBlock(){
+    //this block is stored in a cloud bucket
+    return this.cloudBucketID != NON_EXISTING_BUCKET_ID;
+  }
+
   /**
    */
   @Override
@@ -237,4 +261,13 @@ public int hashCode() {
     //GenerationStamp is IRRELEVANT and should not be used here
     return (int) (blockId ^ (blockId >>> 32));
   }
+
+  public static short getCloudBucket(int maxBuckets, long blockID) {
+    if(maxBuckets > Short.MAX_VALUE){
+      UnsupportedOperationException up = new UnsupportedOperationException("Number of buckets can not be greater than" +
+              " "+ Short.MAX_VALUE);
+      throw up;
+    }
+    return (short) (blockID % maxBuckets);
+  }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CloudBlock.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CloudBlock.java
new file mode 100644
index 00000000000..4019cbd2bef
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/CloudBlock.java
@@ -0,0 +1,54 @@
+package org.apache.hadoop.hdfs.protocol;
+
+import com.google.common.base.Preconditions;
+
+public class CloudBlock {
+
+  private boolean metaObjectFound;
+  private boolean blockObjectFound;
+  private long lastModified;
+  private Block block;
+
+  public CloudBlock(){
+    this.block = null;
+    this.metaObjectFound = false;
+    this.blockObjectFound = false;
+    this.lastModified = -1;
+  }
+
+  public CloudBlock(Block block, long lastModified){
+    Preconditions.checkNotNull(block);
+    this.metaObjectFound = true;
+    this.blockObjectFound = true;
+    this.block = block;
+    this.lastModified = lastModified;
+  }
+
+  public void setLastModified(long lastModified) {
+    this.lastModified = lastModified;
+  }
+
+  public long getLastModified() {
+    return lastModified;
+  }
+
+  public void setMetaObjectFound(boolean metaObjectFound) {
+    this.metaObjectFound = metaObjectFound;
+  }
+
+  public void setBlockObjectFound(boolean blockObjectFound) {
+    this.blockObjectFound = blockObjectFound;
+  }
+
+  public void setBlock(Block block) {
+    this.block = block;
+  }
+
+  public Block getBlock() {
+    return block;
+  }
+
+  public boolean isPartiallyListed() {
+    return !metaObjectFound || !blockObjectFound;
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ExtendedBlock.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ExtendedBlock.java
index 9cf99d9faab..d921c5623f2 100644
--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ExtendedBlock.java
+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/ExtendedBlock.java
@@ -30,7 +30,7 @@
   private Block block;
 
   public ExtendedBlock() {
-    this(null, 0, 0, 0);
+    this(null, 0, 0, 0, Block.NON_EXISTING_BUCKET_ID);
   }
 
   public ExtendedBlock(final ExtendedBlock b) {
@@ -38,7 +38,7 @@ public ExtendedBlock(final ExtendedBlock b) {
   }
 
   public ExtendedBlock(final String poolId, final long blockId) {
-    this(poolId, blockId, 0, 0);
+    this(poolId, blockId, 0, 0, Block.NON_EXISTING_BUCKET_ID);
   }
 
   public ExtendedBlock(String poolId, Block b) {
@@ -47,9 +47,9 @@ public ExtendedBlock(String poolId, Block b) {
   }
 
   public ExtendedBlock(final String poolId, final long blkid, final long len,
-      final long genstamp) {
+      final long genstamp, short cloudBucketID) {
     this.poolId = poolId;
-    block = new Block(blkid, len, genstamp);
+    block = new Block(blkid, len, genstamp, cloudBucketID);
   }
 
   public String getBlockPoolId() {
@@ -100,6 +100,18 @@ public Block getLocalBlock() {
     return block;
   }
 
+  public short getCloudBucketID(){
+    return block.getCloudBucketID();
+  }
+
+  public void setCloudBucketID(short cloudBucketID){
+    block.setCloudBucketIDNoPersistance(cloudBucketID);
+  }
+
+  public boolean isProvidedBlock() {
+    return block.isProvidedBlock();
+  }
+
   @Override // Object
   public boolean equals(Object o) {
     if (this == o) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java
index 218550d2d0d..6b452627450 100644
--- a/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java
+++ b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/web/JsonUtilClient.java
@@ -146,7 +146,9 @@ static ExtendedBlock toExtendedBlock(final Map<?, ?> m) {
     final long numBytes = ((Number) m.get("numBytes")).longValue();
     final long generationStamp =
         ((Number) m.get("generationStamp")).longValue();
-    return new ExtendedBlock(blockPoolId, blockId, numBytes, generationStamp);
+    final short cloudBucketID = ((Number) m.get("cloudBucketID")).shortValue();
+    return new ExtendedBlock(blockPoolId, blockId,
+            numBytes, generationStamp, cloudBucketID);
   }
 
   static int getInt(Map<?, ?> m, String key, final int defaultValue) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/pom.xml b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
index 6b86b18ee68..47689e779c8 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/pom.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
@@ -245,6 +245,11 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
       <type>test-jar</type>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>com.amazonaws</groupId>
+      <artifactId>aws-java-sdk-s3</artifactId>
+      <version>1.11.627</version>
+    </dependency>
   </dependencies>
 
   <build>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/HdfsVariables.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/HdfsVariables.java
index 31b9abd4048..2b03bd8e490 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/HdfsVariables.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/HdfsVariables.java
@@ -99,6 +99,24 @@ private static Object handleVariableWithWriteLock(Handler handler)
     return handleVariable(handler, true);
   }
 
+  public static long getMaxBlockID() throws IOException {
+    return (long) new LightWeightRequestHandler(
+            HDFSOperationType.GET_BLOCK_ID_COUNTER) {
+      @Override
+      public Object performTask() throws IOException {
+        return (long) handleVariableWithWriteLock(new Handler() {
+          @Override
+          public Object handle(VariableDataAccess<Variable, Variable.Finder> vd)
+                  throws StorageException {
+            long maxBlockID = (long) vd.getVariable(Variable.Finder.BlockID).getValue();
+            LOG.debug("HopsFS-Cloud. Get Max Bock ID "+maxBlockID);
+            return maxBlockID;
+          }
+        });
+      }
+    }.handle();
+  }
+
   public static CountersQueue.Counter incrementBlockIdCounter(
       final long increment) throws IOException {
     return (CountersQueue.Counter) new LightWeightRequestHandler(
@@ -617,7 +635,7 @@ public Object handle(VariableDataAccess<Variable, Variable.Finder> vd)
       }
     }.handle();
   }
-  
+
   public static void setBlockTotal(final int value) throws IOException {
     new LightWeightRequestHandler(HDFSOperationType.SET_BLOCK_TOTAL) {
       @Override
@@ -795,6 +813,10 @@ public static void registerDefaultValues(Configuration conf) {
         new IntVariable(0).getBytes());
     Variable.registerVariableDefaultValue(Variable.Finder.curScanCount,
         new IntVariable(-1).getBytes());
+    Variable.registerVariableDefaultValue(Variable.Finder.providedBlockReportsCount,
+            new LongVariable(0).getBytes());
+    Variable.registerVariableDefaultValue(Variable.Finder.providedBlocksCheckStartTime,
+            new LongVariable(0).getBytes());
     VarsRegister.registerHdfsDefaultValues();
     // This is a workaround that is needed until HA-YARN has its own format command
     VarsRegister.registerYarnDefaultValues();
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/BlockInfoDALAdaptor.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/BlockInfoDALAdaptor.java
index 9e165512849..4d63d47c563 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/BlockInfoDALAdaptor.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/BlockInfoDALAdaptor.java
@@ -20,6 +20,7 @@
 import io.hops.metadata.hdfs.dal.BlockInfoDataAccess;
 import io.hops.metadata.hdfs.entity.BlockInfo;
 import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;
 import org.apache.hadoop.hdfs.server.namenode.UnsupportedActionException;
 import org.apache.zookeeper.KeeperException;
@@ -78,6 +79,12 @@ public int countAllCompleteBlocks() throws StorageException {
         dataAccess.findAllBlocks());
   }
 
+  @Override
+  public List<BlockInfoContiguous> findAllBlocks(long startID, long endID) throws StorageException {
+    return (List<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous>) convertDALtoHDFS(
+            dataAccess.findAllBlocks(startID, endID));
+  }
+
   @Override
   public List<org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous> findBlockInfosByStorageId(
       int storageId) throws StorageException {
@@ -128,6 +135,11 @@ public void prepare(
         convertHDFStoDAL(modified));
   }
 
+  @Override
+  public void deleteBlocksForFile(long inodeID) throws StorageException {
+    dataAccess.deleteBlocksForFile(inodeID);
+  }
+
   @Override
   public BlockInfo convertHDFStoDAL(
       org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous hdfsClass)
@@ -136,7 +148,7 @@ public BlockInfo convertHDFStoDAL(
       BlockInfo hopBlkInfo =
           new BlockInfo(hdfsClass.getBlockId(), hdfsClass.getBlockIndex(),
               hdfsClass.getInodeId(), hdfsClass.getNumBytes(),
-              hdfsClass.getGenerationStamp(),
+              hdfsClass.getGenerationStamp(), hdfsClass.getCloudBucketID(),
               hdfsClass.getBlockUCState().ordinal(), hdfsClass.getTimestamp());
       if (hdfsClass instanceof BlockInfoContiguousUnderConstruction) {
         BlockInfoContiguousUnderConstruction ucBlock =
@@ -160,7 +172,7 @@ public BlockInfo convertHDFStoDAL(
       BlockInfo dalClass) throws StorageException {
     if (dalClass != null) {
       Block b = new Block(dalClass.getBlockId(), dalClass.getNumBytes(),
-          dalClass.getGenerationStamp());
+          dalClass.getGenerationStamp(), dalClass.getCloudBucketID());
       org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous blockInfo = null;
 
       if (dalClass.getBlockUCState() >
@@ -174,7 +186,10 @@ public BlockInfo convertHDFStoDAL(
         ((BlockInfoContiguousUnderConstruction) blockInfo)
             .setBlockRecoveryIdNoPersistance(dalClass.getBlockRecoveryId());
         if(dalClass.getTruncateBlockNumBytes()>0){
-          Block truncateBlock = new Block(dalClass.getBlockId(), dalClass.getTruncateBlockNumBytes(), dalClass.getTruncateBlockGenerationStamp());
+          Block truncateBlock = new Block(dalClass.getBlockId(),
+                  dalClass.getTruncateBlockNumBytes(),
+                  dalClass.getTruncateBlockGenerationStamp(),
+                  dalClass.getCloudBucketID());
           ((BlockInfoContiguousUnderConstruction) blockInfo).setTruncateBlock(truncateBlock);
         }
       } else if (dalClass.getBlockUCState() ==
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/INodeDALAdaptor.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/INodeDALAdaptor.java
index a4118a000fd..8bd7288aeb1 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/INodeDALAdaptor.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/INodeDALAdaptor.java
@@ -184,6 +184,12 @@ public boolean hasChildren(long parentId, boolean areChildrenRandomlyPartitioned
     return (List) convertDALtoHDFS(dataAccess.allINodes());
   }
 
+  //only for testing
+  @Override
+  public List<org.apache.hadoop.hdfs.server.namenode.INode> findINodes(String name) throws StorageException {
+    return (List) convertDALtoHDFS(dataAccess.findINodes(name));
+  }
+
   @Override
   public void deleteInode(String name) throws StorageException {
     dataAccess.deleteInode(name);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/ReplicaUnderConstructionDALAdaptor.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/ReplicaUnderConstructionDALAdaptor.java
index d4e998e16f1..9c464386c9b 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/ReplicaUnderConstructionDALAdaptor.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/metadata/adaptor/ReplicaUnderConstructionDALAdaptor.java
@@ -15,6 +15,7 @@
  */
 package io.hops.metadata.adaptor;
 
+import com.google.common.annotations.VisibleForTesting;
 import io.hops.exception.StorageException;
 import io.hops.metadata.DalAdaptor;
 import io.hops.metadata.hdfs.dal.ReplicaUnderConstructionDataAccess;
@@ -76,6 +77,15 @@ public void removeByBlockIdAndInodeId(long blockId, long inodeId)
     dataAccces.removeByBlockIdAndInodeId(blockId, inodeId);
   }
 
+  //only for testing
+  @VisibleForTesting
+  @Override
+  public List<org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction> findAll()
+          throws StorageException {
+    return (List<org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction>)
+            convertDALtoHDFS(dataAccces.findAll());
+  }
+
   @Override
   public ReplicaUnderConstruction convertHDFStoDAL(
       org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction hdfsClass)
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/context/BlockInfoContext.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/context/BlockInfoContext.java
index 14abafc9686..2b721134694 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/context/BlockInfoContext.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/context/BlockInfoContext.java
@@ -71,7 +71,8 @@ public void update(BlockInfoContiguous blockInfo) throws TransactionContextExcep
     updateInodeBlocks(blockInfo);
     if(isLogTraceEnabled()) {
       log("updated-blockinfo", "bid", blockInfo.getBlockId(), "inodeId",
-              blockInfo.getInodeId(), "blk index", blockInfo.getBlockIndex());
+              blockInfo.getInodeId(), "blk index", blockInfo.getBlockIndex(),
+              "cloudBucketID", blockInfo.getCloudBucketID());
     }
 
   }
@@ -270,9 +271,14 @@ public boolean apply(BlockInfoContiguous input) {
                 return input.getInodeId() == inodeId;
               }
             });
-    BlockInfoContiguous result = Collections.max(notRemovedBlks, BlockInfoContiguous.Order.ByBlockIndex);
-    hit(bFinder, result, "inodeId", inodeId);
-    return result;
+    if(notRemovedBlks.size()>0) {
+      BlockInfoContiguous result = Collections.max(notRemovedBlks, BlockInfoContiguous.Order.ByBlockIndex);
+      hit(bFinder, result, "inodeId", inodeId);
+      return result;
+    } else {
+      miss(bFinder, (BlockInfoContiguous) null, "inodeId", inodeId);
+      return null;
+    }
   }
 
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/context/InvalidatedBlockContext.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/context/InvalidatedBlockContext.java
index 8907c76c8f0..6f255607de5 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/context/InvalidatedBlockContext.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/context/InvalidatedBlockContext.java
@@ -47,7 +47,8 @@ public void update(InvalidatedBlock hopInvalidatedBlock)
     super.update(hopInvalidatedBlock);
     if(isLogTraceEnabled()) {
       log("added-invblock", "bid", hopInvalidatedBlock.getBlockId(), "sid",
-         hopInvalidatedBlock.getStorageId());
+         hopInvalidatedBlock.getStorageId(), " CloudBucketID",
+         hopInvalidatedBlock.getCloudBucketID());
     }
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/handler/HDFSOperationType.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/handler/HDFSOperationType.java
index 3601679b534..65f76203b38 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/handler/HDFSOperationType.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/handler/HDFSOperationType.java
@@ -108,6 +108,12 @@
   REMOVE_CORRUPT_REPLICA,
   GET_NUM_INVALIDATED_BLKS,
   GET_INV_BLKS_BY_STORAGEID,
+  GET_INV_BLKS_IN_CLOUD,
+  UPDATE_CLOUD_BLKS_CACHE_LOC,
+  GET_CLOUD_BLKS_CACHE_LOC,
+  DELETE_CLOUD_BLKS_CACHE_LOC,
+  BATCH_READ_CLOUD_BLKS_CACHE_LOCS,
+  CLOUD_ADD_CORRUPT_BLOCKS,
   RM_INV_BLKS,
   GET_ALL_INV_BLKS,
   DEL_ALL_INV_BLKS,
@@ -137,6 +143,10 @@
   RESOLVE_INODES_FROM_BLOCKIDS,
   GET_BLOCKS,
   REMOVE_UNDER_REPLICATED_BLOCK,
+  UPDATE_PROVIDED_BLOCK_REPORTS_COUNT,
+  GET_PROVIDED_BLOCK_REPORTS_COUNT,
+  UPDATE_PROVIDED_BLOCK_CHECK_START_TIME,
+  GET_PROVIDED_BLOCK_CHECK_START_TIME,
   // DatanodeManager
   REMOVE_DATANODE,
   REFRESH_NODES,
@@ -246,6 +256,7 @@
   GET_SET_STORAGE,
   UPDATE_INODE_ID_COUNTER,
   UPDATE_BLOCK_ID_COUNTER,
+  GET_BLOCK_ID_COUNTER,
   UPDATE_CACHE_DIRECTIVE_ID_COUNTER,
   GET_INODES_BATCH,
   COUNT_REPLICAS_ON_NODE,
@@ -330,6 +341,12 @@
   BR_LB_GET_ALL,
   BR_LB_ADD,
   BR_LB_REMOVE,
+  BR_GET_RANGE_OF_BLOCKS,
+  BR_GET_ALL_TASKS,
+  BR_ADD_TASKS,
+  BR_POP_TASK,
+  BR_COUNT_TASKS,
+  BR_DELETE_ALL_TASKS,
   GET_ALL_MACHINE_BLOCKS_IN_BUCKET,
   GET_ALL_STORAGE_BLOCKS_IN_BUCKETS,
   // Block Report Hashes
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/lock/IndividualHashBucketLock.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/lock/IndividualHashBucketLock.java
index 27cd7491d8c..b8c4a10b842 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/lock/IndividualHashBucketLock.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/io/hops/transaction/lock/IndividualHashBucketLock.java
@@ -38,7 +38,8 @@ protected void acquire(TransactionLocks locks) throws IOException {
     if (EntityManager.find(HashBucket.Finder.ByStorageIdAndBucketId,
         storageId, bucketId) == null){
       EntityManager.update(new HashBucket(storageId, bucketId, HashBuckets.initalizeHash()));
-      LOG.warn("The accessed bucket had not been initialized. There might be a misconfiguration.");
+      LOG.warn("The accessed bucket (StorageID: "+storageId+" bucketID: "+bucketId+") has not " +
+              "been initialized. There might be a misconfiguration.");
     }
   }
   
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/fs/CloudProvider.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/fs/CloudProvider.java
new file mode 100644
index 00000000000..ad722bd1822
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/fs/CloudProvider.java
@@ -0,0 +1,10 @@
+package org.apache.hadoop.fs;
+
+public enum CloudProvider {
+  AWS("AWS");
+
+  private String name;
+
+  CloudProvider(String name) { name = name;}
+  //GCE
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
index 75a62f03697..c7d05672052 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java
@@ -618,7 +618,8 @@ void stopProxy(ClientProtocol proxy){
   }
 
   /** Abort and release resources held.  Ignore all errors. */
-  void abort() {
+  @VisibleForTesting
+  public void abort() {
     clientRunning = false;
     closeAllFilesBeingWritten(true);
     try {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
index b533bc2c900..178282a3d46 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java
@@ -85,6 +85,93 @@
   public static final String  DFS_DIR_DELETE_BATCH_SIZE= "dfs.dir.delete.batch.size";
   public static final int DFS_DIR_DELETE_BATCH_SIZE_DEFAULT = 50;
 
+  //HopsFS Cloud
+  public static final String DFS_ENABLE_CLOUD_PERSISTENCE = "dfs.enable.cloud.persistence";
+  public static final boolean DFS_ENABLE_CLOUD_PERSISTENCE_DEFAULT = false;
+
+  public static final String DFS_CLOUD_PROVIDER = "dfs.cloud.provider";
+  public static final String DFS_CLOUD_PROVIDER_DEFAULT = "AWS";
+
+  public static final String S3_BUCKET_PREFIX = "dfs.cloud.aws.s3.bucket.prefix";
+  public static final String S3_BUCKET_PREFIX_DEFAULT = "hopsfs.bucket";
+
+  public static final String DFS_CLOUD_AWS_S3_REGION = "dfs.cloud.aws.s3.region";
+  public static final String DFS_CLOUD_AWS_S3_REGION_DEFAULT = "eu-north-1";
+
+  public static final String DFS_CLOUD_AWS_S3_NUM_BUCKETS = "dfs.cloud.aws.s3.num.buckets";
+  public static final int DFS_CLOUD_AWS_S3_NUM_BUCKETS_DEFAULT = 1;
+
+  public static final String DFS_NAMENODE_INVALIDATE_PROVIDED_BLOCKS_PER_ITERATION =
+          "dfs.namenode.invalidate.cloud.blocks.per.iteration";
+  public static final int DFS_NAMENODE_INVALIDATE_PROVIDED_BLOCKS_PER_ITERATION_DEFAULT = 10000;
+
+  public static final String DFS_CLOUD_MULTIPART_SIZE = "dfs.cloud.multipart.size";
+  public static final long DFS_CLOUD_MULTIPART_SIZE_DEFAULT = 10*1024*1024; // 10 MB
+
+  public static final String DFS_CLOUD_MIN_MULTIPART_THRESHOLD = "dfs.cloud.multipart.threshold";
+  public static final long DFS_CLOUD_MIN_MULTIPART_THRESHOLD_DEFAULT = Integer.MAX_VALUE;
+
+  // the maximum number of threads to allow in the pool used by TransferManager
+  public static final String DFS_DN_CLOUD_MAX_TRANSFER_THREADS = "dfs.dn.cloud.max.upload.threads";
+  public static final int DFS_DN_CLOUD_MAX_TRANSFER_THREADS_DEFAULT = 20;
+
+  // the time an idle thread waits before terminating
+  public static final String DFS_CLOUD_KEEPALIVE_TIME = "dfs.cloud.threads.keepalivetime.sec";
+  public static final int DFS_CLOUD_KEEPALIVE_TIME_DEFAULT = 60;
+
+  // Data node provided blocks cache params
+  public static final String DFS_DN_CLOUD_BYPASS_CACHE_KEY =
+          "dfs.dn.cloud.bypass.cache";
+  public static final boolean DFS_DN_CLOUD_BYPASS_CACHE_DEFAULT = false;
+
+  public static final String DFS_DN_CLOUD_CACHE_CHECK_INTERVAL_KEY =
+          "dfs.dnfcloud.cache.check.interval";
+  public static final long DFS_DN_CLOUD_CACHE_CHECK_INTERVAL_DEFAULT = 3000;
+
+  public static final String DFS_DN_CLOUD_CACHE_DELETE_ACTIVATION_PRECENTAGE_KEY =
+          "dfs.dn.cloud.cache.delete.activation.percentage";
+  public static final int DFS_DN_CLOUD_CACHE_DELETE_ACTIVATION_PRECENTAGE_DEFAULT = 80;
+
+  public static final String DFS_DN_CLOUD_CACHE_DELETE_BATCH_SIZE_KEY =
+          "dfs.dn.cloud.cache.delete.batch.size";
+  public static final int DFS_DN_CLOUD_CACHE_DELETE_BATCH_SIZE_DEFAULT = 100;
+
+  public static final String DFS_DN_CLOUD_CACHE_DELETE_WAIT_KEY =
+          "dfs.dn.cloud.cache.delete.wait";
+  public static final int DFS_DN_CLOUD_CACHE_DELETE_WAIT_DEFAULT = 30*60*1000;
+  // block file must be an 30 mins old before it can be deleted.
+  // This is to make sure that we do not delete newly downloaded
+  // blocks in the cache that are being read by remote clients
+
+  public static final String DFS_NN_MAX_THREADS_FOR_FORMATTING_CLOUD_BUCKETS_KEY =
+          "dfs.nn.max.threads.for.formatting.cloud.buckets";
+  public static final int DFS_NN_MAX_THREADS_FOR_FORMATTING_CLOUD_BUCKETS_DEFAULT = 30;
+
+  public static final String DFS_CLOUD_PREFIX_SIZE_KEY =
+          "dfs.cloud.prefix.size";
+  public static final int DFS_CLOUD_PREFIX_SIZE_DEFAULT = 500;
+
+  public static final String DFS_CLOUD_BLOCK_REPORT_DELAY_KEY =
+          "dfs.cloud.block.report.delay";
+  public static final long DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT = 60 * 60 * 1000L;
+
+  public static final String DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY =
+          "dfs.cloud.block.report.thread.sleep.interval";
+  public static final long DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_DEFAULT = 10 * 1000L;
+
+  public static final String DFS_CLOUD_MARK_PARTIALLY_LISTED_BLOCKS_CORRUPT_AFTER_KEY =
+          "dfs.cloud.mark.partially.listed.blocks.missing.after";
+  public static final long DFS_CLOUD_MARK_PARTIALLY_LISTED_BLOCKS_CORRUPT_AFTER_DEFAULT =
+          60 * 60 * 1000L;
+
+  public static final String DFS_CLOUD_MAX_BR_SUB_TASKS_KEY =
+          "dfs.cloud.max.br.sub.tasks";
+  public static final long DFS_CLOUD_MAX_BR_SUB_TASKS_DEFAULT = 1000L;
+
+  public static final String DFS_CLOUD_MAX_BR_THREADS_KEY =
+          "dfs.cloud.max.br.threads";
+  public static final int DFS_CLOUD_MAX_BR_THREADS_DEFAULT = 10;
+
   /*for client failover api*/
   // format {ip:port, ip:port, ip:port} comma separated
   public static final String DFS_NAMENODES_RPC_ADDRESS_KEY = "dfs.namenodes.rpc.addresses";
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
index 7b79eff2aa5..148673e333f 100755
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java
@@ -47,9 +47,13 @@
 import org.apache.hadoop.hdfs.protocol.NSQuotaExceededException;
 import org.apache.hadoop.hdfs.protocol.QuotaByStorageTypeExceededException;
 import org.apache.hadoop.hdfs.protocol.UnresolvedPathException;
+import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;
+import org.apache.hadoop.hdfs.client.HdfsDataOutputStream.SyncFlag;
+import org.apache.hadoop.hdfs.protocol.*;
 import org.apache.hadoop.hdfs.protocol.datatransfer.PacketHeader;
 import org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier;
 import org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite;
+import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;
 import org.apache.hadoop.hdfs.server.datanode.CachingStrategy;
 import org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException;
 import org.apache.hadoop.hdfs.server.namenode.RetryStartFileException;
@@ -289,7 +293,14 @@ private DFSOutputStream(DFSClient dfsClient, String src,
     initialFileSize = stat.getLen(); // length of file when opened
     this.shouldSyncBlock = flags.contains(CreateFlag.SYNC_BLOCK);
 
-    boolean toNewBlock = flags.contains(CreateFlag.NEW_BLOCK);
+    //incase of provided blocks always create a new block. We can not append to
+    //objects stored in S3
+    boolean toNewBlock;
+    if (lastBlock != null && lastBlock.getBlock().isProvidedBlock()) {
+      toNewBlock = true;
+    } else {
+      toNewBlock = flags.contains(CreateFlag.NEW_BLOCK);
+    }
 
     // The last partial block of the file has to be filled.
     if (!toNewBlock && lastBlock != null && policySuite.getPolicy(stat.getStoragePolicy()).getStorageTypes()[0] != StorageType.DB) {
@@ -601,6 +612,16 @@ private void flushOrSync(boolean isSync, EnumSet<SyncFlag> syncFlags)
       long toWaitFor;
       long lastBlockLength = -1L;
       boolean updateLength = syncFlags.contains(SyncFlag.UPDATE_LENGTH);
+
+      if(getStat().getStoragePolicy() == HdfsConstants.CLOUD_STORAGE_POLICY_ID ||
+              getStat().getStoragePolicy() == HdfsConstants.DB_STORAGE_POLICY_ID){
+        //force to create a new block on sync / flush
+        if(!syncFlags.contains(SyncFlag.END_BLOCK)){
+
+          syncFlags.add(SyncFlag.END_BLOCK);
+        }
+      }
+
       boolean endBlock = syncFlags.contains(SyncFlag.END_BLOCK);
       synchronized (this) {
         // flush checksum buffer, but keep checksum buffer intact if we do not
@@ -1011,6 +1032,10 @@ ExtendedBlock getBlock() {
     return streamer.getBlock();
   }
 
+  HdfsFileStatus getStat() {
+    return streamer.getStat();
+  }
+
   @VisibleForTesting
   public long getFileId() {
     return fileId;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java
index b938338e27c..00db87a83c2 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java
@@ -1396,7 +1396,8 @@ private boolean setupPipelineForAppendOrRecovery() throws IOException {
     if (success) {
       // update pipeline at the namenode
       ExtendedBlock newBlock = new ExtendedBlock(
-          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(), newGS);
+          block.getBlockPoolId(), block.getBlockId(), block.getNumBytes(),
+          newGS, block.getCloudBucketID());
       dfsClient.namenode.updatePipeline(dfsClient.clientName, block, newBlock,
           nodes, storageIDs);
       // update client side generation stamp
@@ -1714,7 +1715,7 @@ protected LocatedBlock locateFollowingBlock(DatanodeInfo[] excludedNodes)
               throw e;
             } else {
               --retries;
-              LOG.info("Exception while adding a block", e);
+              LOG.debug("Exception while adding a block", e);
               long elapsed = Time.monotonicNow() - localstart;
               if (elapsed > 5000) {
                 LOG.info("Waiting for replication for "
@@ -1781,6 +1782,15 @@ ExtendedBlock getBlock() {
     return block;
   }
 
+  /**
+   * get the file status
+   *
+   * @return file status 
+   */
+  HdfsFileStatus getStat() {
+    return stat;
+  }
+
   /**
    * return the target datanodes in the pipeline
    *
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsConstants.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsConstants.java
index 151b1f44c14..effd6cb2bf7 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsConstants.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/HdfsConstants.java
@@ -146,6 +146,7 @@ public static RollingUpgradeAction fromString(String s) {
   10	      One_SSD	      SSD: 1,DISK: n-1           SSD, DISK	        SSD, DISK
   7	        Hot (default)	DISK: n	                   <none>	            ARCHIVE
   5	        Warm	        DISK: 1, ARCHIVE: n-1    	 ARCHIVE, DISK	    ARCHIVE, DISK
+  3         Cloud         S3 or other block stores   DISK               DISK
   2	        Cold	        ARCHIVE: n	               <none>	            <none>
   */
   public static final String DB_STORAGE_POLICY_NAME = "DB";
@@ -166,6 +167,9 @@ public static RollingUpgradeAction fromString(String s) {
   public static final String RAID5_STORAGE_POLICY_NAME = "RAID5";
   public static final byte RAID5_STORAGE_POLICY_ID = 4;
 
+  public static final String CLOUD_STORAGE_POLICY_NAME = "CLOUD";
+  public static final byte CLOUD_STORAGE_POLICY_ID = 3;
+
   public static final String COLD_STORAGE_POLICY_NAME = "COLD";
   public static final byte COLD_STORAGE_POLICY_ID = 2;
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolTranslatorPB.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolTranslatorPB.java
index cd7414ee009..3080f9f7450 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolTranslatorPB.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InterDatanodeProtocolTranslatorPB.java
@@ -97,7 +97,8 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)
     
     BlockProto b = resp.getBlock();
     return new ReplicaRecoveryInfo(b.getBlockId(), b.getNumBytes(),
-        b.getGenStamp(), PBHelper.convert(resp.getState()));
+        b.getGenStamp(), (short)b.getCloudBucketID(),
+        PBHelper.convert(resp.getState()));
   }
 
   @Override
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
index 31489b2b84b..ddd5e03f78e 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java
@@ -344,12 +344,14 @@ public static DatanodeIDProto convert(DatanodeID dn) {
   // Block
   public static BlockProto convert(Block b) {
     return BlockProto.newBuilder().setBlockId(b.getBlockId())
-        .setGenStamp(b.getGenerationStamp()).setNumBytes(b.getNumBytes())
+        .setGenStamp(b.getGenerationStamp())
+        .setNumBytes(b.getNumBytes())
+        .setCloudBucketID(b.getCloudBucketID())
         .build();
   }
 
   public static Block convert(BlockProto b) {
-    return new Block(b.getBlockId(), b.getNumBytes(), b.getGenStamp());
+    return new Block(b.getBlockId(), b.getNumBytes(), b.getGenStamp(), (short) b.getCloudBucketID());
   }
 
   public static BlockWithLocationsProto convert(BlockWithLocations blk) {
@@ -456,7 +458,7 @@ public static ExtendedBlock convert(ExtendedBlockProto eb) {
       return null;
     }
     return new ExtendedBlock(eb.getPoolId(), eb.getBlockId(), eb.getNumBytes(),
-        eb.getGenerationStamp());
+        eb.getGenerationStamp(), (short)eb.getCloudBucketID());
   }
 
   public static ExtendedBlockProto convert(final ExtendedBlock b) {
@@ -468,6 +470,7 @@ public static ExtendedBlockProto convert(final ExtendedBlock b) {
         setBlockId(b.getBlockId()).
         setNumBytes(b.getNumBytes()).
         setGenerationStamp(b.getGenerationStamp()).
+        setCloudBucketID(b.getCloudBucketID()).
         build();
   }
 
@@ -669,6 +672,7 @@ public static LocatedBlockProto convert(LocatedBlock b) {
     if(b.isPhantomBlock() && b.isDataSet()){
       builder.setData(ByteString.copyFrom(b.getData()));
     }
+
     return builder.build();
   }
 
@@ -1726,6 +1730,8 @@ public static StorageTypeProto convertStorageType(StorageType type) {
         return StorageTypeProto.ARCHIVE;
       case DB:
         return StorageTypeProto.DB;
+      case CLOUD:
+        return StorageTypeProto.CLOUD;
       default:
         Preconditions.checkState(
             false,
@@ -1747,6 +1753,8 @@ public static StorageType convertStorageType(StorageTypeProto type) {
         return StorageType.ARCHIVE;
       case DB:
         return StorageType.DB;
+      case CLOUD:
+        return StorageType.CLOUD;
       default:
         throw new IllegalStateException(
             "BUG: StorageTypeProto not found, type=" + type);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguous.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguous.java
index bf012f3be41..99d533f2c33 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguous.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguous.java
@@ -16,23 +16,30 @@
  */
 package org.apache.hadoop.hdfs.server.blockmanagement;
 
+import com.google.common.collect.Iterables;
 import io.hops.exception.StorageException;
 import io.hops.exception.TransactionContextException;
 import io.hops.metadata.HdfsStorageFactory;
 import io.hops.metadata.common.FinderType;
-import io.hops.metadata.hdfs.dal.BlockInfoDataAccess;
+import io.hops.metadata.hdfs.entity.ProvidedBlockCacheLoc;
 import io.hops.metadata.hdfs.entity.Replica;
 import io.hops.metadata.hdfs.entity.ReplicaBase;
 import io.hops.transaction.EntityManager;
+import io.hops.transaction.handler.HDFSOperationType;
+import io.hops.transaction.handler.LightWeightRequestHandler;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.fs.StorageType;
 import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.BlockUCState;
+import org.apache.hadoop.hdfs.server.common.Storage;
 import org.apache.hadoop.hdfs.server.namenode.INodeFile;
 import org.apache.hadoop.hdfs.server.protocol.DatanodeStorage;
 
+import java.io.IOException;
 import java.util.*;
 
 /**
@@ -148,6 +155,7 @@ public Comparator descending() {
   public BlockInfoContiguous(Block blk, long inodeId) {
     super(blk);
     this.inodeId = inodeId;
+    this.timestamp = System.currentTimeMillis();
     if (blk instanceof BlockInfoContiguous) {
       this.bc = ((BlockInfoContiguous) blk).bc;
       this.blockIndex = ((BlockInfoContiguous) blk).blockIndex;
@@ -223,11 +231,61 @@ public int numNodes(DatanodeManager datanodeMgr)
    * @return array of storages that store a replica of this block
    */
   public DatanodeStorageInfo[] getStorages(DatanodeManager datanodeMgr)
-      throws StorageException, TransactionContextException {
+          throws TransactionContextException, StorageException {
     List<Replica> replicas = getReplicas(datanodeMgr);
+    if (replicas.size() == 0 && isProvidedBlock()) {
+      // get a phantom block
+      return phantomStoragesForProvidedBlocks(datanodeMgr);
+    }
     return getStorages(datanodeMgr, replicas);
   }
 
+  public DatanodeStorageInfo[] phantomStoragesForProvidedBlocks(DatanodeManager dnMgm) throws StorageException {
+    //NOTE: The solution for HopsFS-Cloud assumes that a single replica (DN)
+    //manipulates a provided block. Returning multiple replica locations
+    //will break the solution
+
+    final int STORAGES = 1;
+
+    //returning a phantom 'cloud' storage
+    List<DatanodeInfo> existing = new ArrayList<>();
+    List<DatanodeStorageInfo> ret = new ArrayList<>();
+
+    ProvidedBlockCacheLoc loc =
+            ProvidedBlocksCacheHelper.getProvidedBlockCacheLocation(getBlockId());
+    if (loc != null) {
+      // check that the datanode is alive
+      DatanodeDescriptor dns = dnMgm.getDatanodeBySid(loc.getStorageID());
+      if(dns.isAlive && !dns.isStale(dnMgm.getStaleInterval())){
+        LOG.debug("HopsFS-Cloud. The block ID: "+getBlockId()+" is cached on DN: "+dns.toString());
+        DatanodeStorageInfo storageInfo = dnMgm.getStorage(loc.getStorageID());
+        ret.add(storageInfo);
+      }
+    }
+
+    while (ret.size() < STORAGES) {
+      DatanodeDescriptor dnDesc = dnMgm.getRandomDN(existing);
+      if (dnDesc == null) {
+        break;
+      }
+      existing.add(dnDesc);
+
+      //A data node may have multiple CLOUD volumes. Retruen a random
+      //CLOUD Volumen on the datanode.
+      List<DatanodeStorageInfo> storages = Arrays.asList(dnDesc.getStorageInfos());
+      Collections.shuffle(storages);
+      for (DatanodeStorageInfo info : storages) {
+        if (info.getStorageType() == StorageType.CLOUD) {
+          LOG.debug("HopsFS-Cloud. The block ID: "+getBlockId()+" will be cached on DN: "+info.toString());
+          ret.add(info);
+          break;
+        }
+      }
+    }
+
+    return Iterables.toArray(ret, DatanodeStorageInfo.class);
+  }
+
   /**
    * Returns the Storages on which the replicas of this block are stored.
    * @param datanodeMgr
@@ -564,9 +622,9 @@ public void setGenerationStamp(long stamp)
     save();
   }
 
-  public void set(long blkid, long len, long genStamp)
+  public void set(long blkid, long len, long genStamp, short cloudBucketID)
       throws StorageException, TransactionContextException {
-    setNoPersistance(blkid, len, genStamp);
+    setNoPersistance(blkid, len, genStamp, cloudBucketID);
     save();
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java
index 6068a0d1a7c..842593e6cca 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoContiguousUnderConstruction.java
@@ -205,7 +205,8 @@ void commitBlock(Block block, DatanodeManager datanodeMgr) throws IOException {
               ", expected id = " + getBlockId());
     }
     blockUCState = BlockUCState.COMMITTED;
-    this.set(getBlockId(), block.getNumBytes(), block.getGenerationStamp());
+    this.set(getBlockId(), block.getNumBytes(),
+            block.getGenerationStamp(), block.getCloudBucketID());
     // Sort out invalid replicas.
     setGenerationStampAndVerifyReplicas(block.getGenerationStamp(), datanodeMgr);
   }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
index c23ea1ffc16..5f93882ff20 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java
@@ -28,10 +28,7 @@
 import io.hops.metadata.blockmanagement.ExcessReplicasMap;
 import io.hops.metadata.common.entity.Variable;
 import io.hops.metadata.hdfs.dal.MisReplicatedRangeQueueDataAccess;
-import io.hops.metadata.hdfs.entity.EncodingStatus;
-import io.hops.metadata.hdfs.entity.HashBucket;
-import io.hops.metadata.hdfs.entity.INodeIdentifier;
-import io.hops.metadata.hdfs.entity.MisReplicatedRange;
+import io.hops.metadata.hdfs.entity.*;
 import io.hops.metadata.security.token.block.NameNodeBlockTokenSecretManager;
 import io.hops.transaction.EntityManager;
 import io.hops.transaction.handler.HDFSOperationType;
@@ -285,6 +282,8 @@ public long getPostponedMisreplicatedBlocksCount() {
   final float blocksInvalidateWorkPct;
   final int blocksReplWorkMultiplier;
 
+  final int providedBlocksInvalidateWorkPct;
+
   /**
    * variable to enable check for enough racks
    */
@@ -333,7 +332,10 @@ public long getPostponedMisreplicatedBlocksCount() {
   private final int numBuckets;
   private final int blockFetcherNBThreads;
   private final int blockFetcherBucketsPerThread;
-  
+
+  private boolean isCloudEnabled = false;
+  private ProvidedBlocksChecker providedBlocksChecker;
+
   public BlockManager(final Namesystem namesystem, final Configuration conf)
     throws IOException {
     this.namesystem = namesystem;
@@ -412,6 +414,10 @@ public BlockManager(final Namesystem namesystem, final Configuration conf)
         DFSUtil.getInvalidateWorkPctPerIteration(conf);
     this.blocksReplWorkMultiplier = DFSUtil.getReplWorkMultiplier(conf);
 
+    this.providedBlocksInvalidateWorkPct =
+            conf.getInt(DFSConfigKeys.DFS_NAMENODE_INVALIDATE_PROVIDED_BLOCKS_PER_ITERATION,
+                    DFSConfigKeys.DFS_NAMENODE_INVALIDATE_PROVIDED_BLOCKS_PER_ITERATION_DEFAULT);
+
     this.replicationRecheckInterval =
         conf.getInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,
             DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_DEFAULT) * 1000L;
@@ -435,7 +441,11 @@ public BlockManager(final Namesystem namesystem, final Configuration conf)
     this.slicerNbThreads = conf.getInt(
         DFSConfigKeys.DFS_NAMENODE_SLICER_NB_OF_THREADS,
         DFSConfigKeys.DFS_NAMENODE_SLICER_NB_OF_THREADS_DEFAULT);
-            
+
+    this.isCloudEnabled = conf.getBoolean(
+            DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE,
+            DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE_DEFAULT);
+
     LOG.info("defaultReplication         = " + defaultReplication);
     LOG.info("maxReplication             = " + maxReplication);
     LOG.info("minReplication             = " + minReplication);
@@ -445,8 +455,8 @@ public BlockManager(final Namesystem namesystem, final Configuration conf)
     LOG.info("encryptDataTransfer        = " + encryptDataTransfer);
     LOG.info("maxNumBlocksToLog          = " + maxNumBlocksToLog);
     LOG.info("slicerBatchSize            = " + slicerBatchSize);
-    LOG.info("misReplicatedNoOfBatchs    = " + processMisReplicatedNoOfBatchs);   
-    LOG.info("slicerNbOfBatchs           = " + processMisReplicatedNoOfBatchs);   
+    LOG.info("misReplicatedNoOfBatchs    = " + processMisReplicatedNoOfBatchs);
+    LOG.info("slicerNbOfBatchs           = " + processMisReplicatedNoOfBatchs);
   }
 
   private NameNodeBlockTokenSecretManager createBlockTokenSecretManager(
@@ -729,7 +739,7 @@ private BlockInfoContiguous completeBlock(final BlockCollection bc,
     BlockInfoContiguous curBlock = bc.getBlock(blkIndex);
     if (curBlock.isComplete())
       return curBlock;
-    BlockInfoContiguousUnderConstruction ucBlock = 
+    BlockInfoContiguousUnderConstruction ucBlock =
         (BlockInfoContiguousUnderConstruction) curBlock;
     int numNodes = ucBlock.numNodes(datanodeManager);
     if (!force && numNodes < minReplication)
@@ -801,7 +811,7 @@ public LocatedBlock convertLastBlockToUnderConstruction(
 
     DatanodeStorageInfo[] targets = getStorages(oldBlock);
 
-    BlockInfoContiguousUnderConstruction ucBlock = 
+    BlockInfoContiguousUnderConstruction ucBlock =
         bc.setLastBlock(oldBlock, targets);
 
     // Remove block from replication queue.
@@ -924,7 +934,7 @@ private LocatedBlock createLocatedBlock(final BlockInfoContiguous[] blocks,
 
     List<DatanodeInfo> machines = new ArrayList<>(file.getBlockReplication());
     for(int i = 0; i < file.getBlockReplication(); i++){
-      DatanodeInfo randomDatanode =  datanodeManager.getRandomDN(machines, file.getBlockReplication());
+      DatanodeInfo randomDatanode =  datanodeManager.getRandomDN(machines);
       if(randomDatanode != null){
         machines.add(randomDatanode);
       }
@@ -989,7 +999,8 @@ private LocatedBlock createLocatedBlock(final BlockInfoContiguous blk, final lon
     final DatanodeStorageInfo[] storages = new DatanodeStorageInfo[numMachines];
     int j = 0;
     if (numMachines > 0) {
-      for (final DatanodeStorageInfo storage : blocksMap.storageList(blk)){
+      List<DatanodeStorageInfo> locations =  blocksMap.storageList(blk);
+      for (final DatanodeStorageInfo storage : locations){
         final boolean replicaCorrupt = corruptReplicas.isReplicaCorrupt(blk,
             storage.getDatanodeDescriptor());
         if (isCorrupt || (!replicaCorrupt)){
@@ -1176,7 +1187,7 @@ private BlocksWithLocations getBlocksWithLocations(final DatanodeID datanode,
       return new BlocksWithLocations(new BlockWithLocations[0]);
     }
     int startBlock = DFSUtil.getRandom().nextInt(numBlocks); // starting from a random block
-    Iterator<BlockInfoContiguous> iter = node.getBlockIterator(startBlock);    
+    Iterator<BlockInfoContiguous> iter = node.getBlockIterator(startBlock);
     List<BlockWithLocations> results = new ArrayList<>();
     long totalSize = 0;
     BlockInfoContiguous curBlock;
@@ -1284,8 +1295,8 @@ public Object call() throws Exception {
           removeBlocks(allBlocksAndInodesIds, node);
 
           HashBuckets.getInstance().resetBuckets(storageInfo.getSid());
-              
-          namesystem.checkSafeMode();          
+
+          namesystem.checkSafeMode();
           return null;
 
         } catch (Throwable t) {
@@ -1300,7 +1311,7 @@ public Object call() throws Exception {
   /*
    * Removing blocks in the database can take a lot of time. To avoid the all NN hanging on this function we make it
    * asynchronous. If the node is reconnected while this function is running, some block may be reported and then removed
-   * this will result in these block being wrongly seen as under replicated. Which in the worse case will result in the 
+   * this will result in these block being wrongly seen as under replicated. Which in the worse case will result in the
    * blocks being replicated and detected as over replicated the next time the node does a block report. This is not
    * ideal for disk usage, but this will not result in any data lost.
    */
@@ -1311,7 +1322,7 @@ void removeBlocksAssociatedTo(final int sid)
       public Object call() throws Exception {
         try {
           Map<Long, Long> allBlocksAndInodesIds = DatanodeStorageInfo.getAllStorageReplicas(numBuckets, sid,
-              blockFetcherNBThreads, blockFetcherBucketsPerThread, 
+              blockFetcherNBThreads, blockFetcherBucketsPerThread,
               ((FSNamesystem) namesystem).getFSOperationsExecutor());
           
           removeBlocks(allBlocksAndInodesIds, sid);
@@ -1368,11 +1379,15 @@ private void addToInvalidates(Block b)
     StringBuilder datanodes = new StringBuilder();
     BlockInfoContiguous block = getBlockInfo(b);
 
-    DatanodeStorageInfo[] storages = getBlockInfo(block).getStorages(datanodeManager, DatanodeStorage.State.NORMAL);
-    for(DatanodeStorageInfo storage : storages) {
-      final DatanodeDescriptor node = storage.getDatanodeDescriptor();
-      invalidateBlocks.add(block, storage, false);
-      datanodes.append(node).append(" ");
+    if( block.isProvidedBlock()) {
+      invalidateBlocks.addProvidedBlock(block);
+    } else {
+      DatanodeStorageInfo[] storages = getBlockInfo(block).getStorages(datanodeManager, DatanodeStorage.State.NORMAL);
+      for(DatanodeStorageInfo storage : storages) {
+        final DatanodeDescriptor node = storage.getDatanodeDescriptor();
+        invalidateBlocks.add(block, storage, false);
+        datanodes.append(node).append(" ");
+      }
     }
     if (datanodes.length() != 0) {
       blockLog.info("BLOCK* addToInvalidates: {} {}", block, datanodes.toString());
@@ -1622,15 +1637,16 @@ public int getUnderReplicatedNotMissingBlocks() throws IOException {
    *     number of datanodes to schedule deletion work
    * @return total number of block for deletion
    */
-  int computeInvalidateWork(int nodesToProcess) throws IOException {
-    final Map<DatanodeInfo, List<Integer>> nodesToSids = invalidateBlocks.getDatanodes(datanodeManager);
-    List<Map.Entry<DatanodeInfo, List<Integer>>> nodes = new ArrayList<>(nodesToSids.entrySet());
+  int computeInvalidateWorkForDNs(int nodesToProcess) throws IOException {
+    final Map<DatanodeInfo, Set<Integer>> nodesToSids =
+            invalidateBlocks.getDatanodes(datanodeManager);
+    List<Map.Entry<DatanodeInfo, Set<Integer>>> nodes = new ArrayList<>(nodesToSids.entrySet());
     Collections.shuffle(nodes);
 
     nodesToProcess = Math.min(nodes.size(), nodesToProcess);
 
     int blockCnt = 0;
-    for (Map.Entry<DatanodeInfo, List<Integer>> dnInfo : nodes) {
+    for (Map.Entry<DatanodeInfo, Set<Integer>> dnInfo : nodes) {
 
       int blocks = invalidateWorkForOneNode(dnInfo);
       if (blocks > 0) {
@@ -1643,6 +1659,18 @@ int computeInvalidateWork(int nodesToProcess) throws IOException {
     return blockCnt;
   }
 
+  int computeInvalidateWorkForCloud() throws IOException {
+
+    final List<Block> toInvalidate = invalidateBlocks.
+            invalidateWorkForCloud(providedBlocksInvalidateWorkPct, getDatanodeManager());
+
+    if (blockLog.isInfoEnabled() && toInvalidate.size() > 0) {
+      blockLog.info("BLOCK* Deleting provided blocks {}", toInvalidate);
+    }
+
+    return toInvalidate.size();
+  }
+
   /**
    * Scan blocks in {@link #neededReplications} and assign replication
    * work to data-nodes they belong to.
@@ -1711,6 +1739,12 @@ private int computeReplicationWorkForBlockInternal(Block blk, int priority1)
       containingNodes = new ArrayList<>();
       List<DatanodeStorageInfo> liveReplicaNodes = new ArrayList<>();
       NumberReplicas numReplicas = new NumberReplicas();
+
+      if (blk.isProvidedBlock()) {
+        LOG.info("HopsFS-Cloud. Block " + blk + " cannot be replicated from any storage");
+        return scheduledWork;
+      }
+
       srcNode = chooseSourceDatanode(blk, containingNodes, liveReplicaNodes,
           numReplicas, priority1);
       if (srcNode == null) { // block can not be replicated from any storage
@@ -1992,7 +2026,7 @@ DatanodeDescriptor chooseSourceDatanode(Block b,
     Collection<DatanodeDescriptor> nodesCorrupt = corruptReplicas.getNodes(block);
     for(DatanodeStorageInfo storage : block.getStorages(datanodeManager)) {
       final DatanodeDescriptor node = storage.getDatanodeDescriptor();
-      int countableReplica = storage.getState() == State.NORMAL ? 1 : 0; 
+      int countableReplica = storage.getState() == State.NORMAL ? 1 : 0;
       if ((nodesCorrupt != null) && (nodesCorrupt.contains(node))) {
         corrupt += countableReplica;
       } else if (node.isDecommissionInProgress()) {
@@ -2015,8 +2049,8 @@ DatanodeDescriptor chooseSourceDatanode(Block b,
       if ((nodesCorrupt != null) && nodesCorrupt.contains(node)) {
         continue;
       }
-      if(priority != UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY 
-          && !node.isDecommissionInProgress() 
+      if(priority != UnderReplicatedBlocks.QUEUE_HIGHEST_PRIORITY
+          && !node.isDecommissionInProgress()
           && node.getNumberOfBlocksToBeReplicated() >= maxReplicationStreams)
       {
         continue; // already reached replication limit
@@ -2364,8 +2398,8 @@ public Object performTask() throws IOException {
    * Mark block replicas as corrupt except those on the storages in 
    * newStorages list.
    */
-  public void markBlockReplicasAsCorrupt(BlockInfoContiguous block, 
-      long oldGenerationStamp, long oldNumBytes, 
+  public void markBlockReplicasAsCorrupt(BlockInfoContiguous block,
+      long oldGenerationStamp, long oldNumBytes,
       DatanodeStorageInfo[] newStorages) throws IOException {
     BlockToMarkCorrupt b = null;
     if (block.getGenerationStamp() != oldGenerationStamp) {
@@ -2542,7 +2576,7 @@ public void removeBlocks(List<Long> allBlockIds, final DatanodeDescriptor node)
     final List<Long> inodeIds = new ArrayList<>(inodeIdsToBlockMap.keySet());
 
     try {
-      Slicer.slice(inodeIds.size(), slicerBatchSize, slicerNbThreads, 
+      Slicer.slice(inodeIds.size(), slicerBatchSize, slicerNbThreads,
           ((FSNamesystem) namesystem).getFSOperationsExecutor(),
           new Slicer.OperationHandler() {
         @Override
@@ -2835,7 +2869,7 @@ public Object performTask() throws IOException {
         for (BlockListAsLongs.BlockReportReplica brb : reportedBlocks) {
           Block block = new Block();
           block.setNoPersistance(brb.getBlockId(), brb.getBytesOnDisk(),
-                  brb.getGenerationStamp());
+                  brb.getGenerationStamp(), brb.getCloudBucketID());
           BlockInfoContiguous storedBlock =
                   processReportedBlock(storage,
                           block, brb.getState(),
@@ -2956,7 +2990,7 @@ private HashMatchingResult calculateMismatchedHashes(DatanodeStorageInfo storage
   private BlockInfoContiguous processIncrementallyReportedBlock(
       final DatanodeStorageInfo storageInfo,
       final Block block, final ReplicaState reportedState,
-      final Collection<BlockInfoContiguous> toAdd, 
+      final Collection<BlockInfoContiguous> toAdd,
       final Collection<Block> toInvalidate,
       final Collection<BlockToMarkCorrupt> toCorrupt,
       final Collection<StatefulBlockInfo> toUC)
@@ -3001,7 +3035,7 @@ private BlockInfoContiguous processIncrementallyReportedBlock(
 
     if (isBlockUnderConstruction(storedBlock, ucState, reportedState)) {
       toUC.add(new StatefulBlockInfo(
-          (BlockInfoContiguousUnderConstruction) storedBlock, 
+          (BlockInfoContiguousUnderConstruction) storedBlock,
           block, reportedState));
       return storedBlock;
     }
@@ -3207,7 +3241,7 @@ void addStoredBlockUnderConstruction(StatefulBlockInfo ucBlock,
     BlockInfoContiguousUnderConstruction block = ucBlock.storedBlock;
     block.addReplicaIfNotPresent(
         storageInfo, ucBlock.reportedState, ucBlock.reportedBlock.getGenerationStamp());
-    if (ucBlock.reportedState == ReplicaState.FINALIZED && 
+    if (ucBlock.reportedState == ReplicaState.FINALIZED &&
         !block.isReplicatedOnStorage(storageInfo)) {
       addStoredBlock(block, storageInfo, null, true);
     }
@@ -3225,10 +3259,10 @@ void addStoredBlockUnderConstruction(StatefulBlockInfo ucBlock,
    * @throws IOException
    */
   private void addStoredBlockImmediate(BlockInfoContiguous storedBlock,
-      DatanodeStorageInfo storage, boolean logEveryBlock) 
+      DatanodeStorageInfo storage, boolean logEveryBlock)
       throws IOException {
     assert (storedBlock != null);
-    if (!namesystem.isInStartupSafeMode() 
+    if (!namesystem.isInStartupSafeMode()
         || namesystem.isPopulatingReplQueues()) {
       addStoredBlock(storedBlock, storage, null, logEveryBlock);
       return;
@@ -3513,7 +3547,6 @@ private void stopReplicationInitializer() {
       }
     }
   }
-  
   private List<MisReplicatedRange> checkMisReplicatedRangeQueue() throws IOException {
     final LightWeightRequestHandler cleanMisReplicatedRangeQueueHandler = new LightWeightRequestHandler(
         HDFSOperationType.UPDATE_MIS_REPLICATED_RANGE_QUEUE) {
@@ -3556,7 +3589,7 @@ private void processMisReplicatesAsync() throws InterruptedException, IOExceptio
     
     long totalBlocks = blocksMap.size();
     replicationQueuesInitProgress = 0;
-    final AtomicLong totalProcessed = new AtomicLong(0);   
+    final AtomicLong totalProcessed = new AtomicLong(0);
     boolean haveMore;
     final int filesToProcess = slicerBatchSize * processMisReplicatedNoOfBatchs;
 
@@ -3627,11 +3660,11 @@ private void processMissreplicatedInt(long filesToProcessStartIndex, long filesT
     final List<INodeIdentifier> allINodes = blocksMap
         .getAllINodeFiles(filesToProcessStartIndex, filesToProcessEndIndex);
     LOG.info("processMisReplicated read  " + allINodes.size() + "/" + filesToProcess + " in the Ids range ["
-        + filesToProcessStartIndex + " - " + filesToProcessEndIndex + "] (max inodeId when the process started: " + 
+        + filesToProcessStartIndex + " - " + filesToProcessEndIndex + "] (max inodeId when the process started: " +
         maxInodeId + ")");
 
     try {
-      Slicer.slice(allINodes.size(), slicerBatchSize, slicerNbThreads, 
+      Slicer.slice(allINodes.size(), slicerBatchSize, slicerNbThreads,
           ((FSNamesystem) namesystem).getFSOperationsExecutor(),
           new Slicer.OperationHandler() {
         @Override
@@ -3699,7 +3732,7 @@ public Object performTask() throws IOException {
   
   /**
    * Get the progress of the Replication queues initialisation
-   * 
+   *
    * @return Returns values between 0 and 1 for the progress.
    */
   public double getReplicationQueuesInitProgress() {
@@ -4321,7 +4354,7 @@ private void processAndHandleReportedBlock(
    * This includes blocks that are starting to be received, completed being
    * received, or deleted.
    */
-  public void processIncrementalBlockReport(DatanodeRegistration nodeID,
+  public void processIncrementalBlockReport(final DatanodeRegistration nodeID,
       final StorageReceivedDeletedBlocks blockInfos)
     throws IOException {
     //hack to have the variables final to pass then to the handler.
@@ -4399,44 +4432,49 @@ public Object performTask() throws IOException {
                 " dataNode=" + node.getXferAddr() + " storage=" + storage.getStorageID() +
                     " sid: " + storage.getSid() + " status=" + rdbi.getStatus());
             HashBuckets hashBuckets = HashBuckets.getInstance();
+
+            addSubopName(rdbi.getStatus().toString());
+            if(rdbi.getBlock().isProvidedBlock()){
+              boolean processed = processIncrementalProvidedBlockReport( storage, rdbi,
+                      receiving, received, deleted);
+              if(processed){
+                return null;
+              }
+              // else continue processing as normal block.
+            }
+
             switch (rdbi.getStatus()) {
               case RECEIVING_BLOCK:
-                addSubopName(ReceivedDeletedBlockInfo.BlockStatus.RECEIVING_BLOCK.name());
                 processAndHandleReportedBlock(storage, rdbi.getBlock(),
                         ReplicaState.RBW, null);
                 received[0]++;
                 break;
               case APPENDING:
-                addSubopName(ReceivedDeletedBlockInfo.BlockStatus.APPENDING.name());
                 processAndHandleReportedBlock(storage, rdbi.getBlock(),
                     ReplicaState.RBW, null);
                 received[0]++;
                 break;
               case RECOVERING_APPEND:
-                addSubopName(ReceivedDeletedBlockInfo.BlockStatus.RECOVERING_APPEND.name());
                 processAndHandleReportedBlock(storage, rdbi.getBlock(),
                     ReplicaState.RBW, null);
                 received[0]++;
                 break;
               case RECEIVED_BLOCK:
-                addSubopName(ReceivedDeletedBlockInfo.BlockStatus.RECEIVED_BLOCK.name());
                 addBlock(storage, rdbi.getBlock(), rdbi.getDelHints());
                 hashBuckets.applyHash(storage.getSid(), ReplicaState.FINALIZED, rdbi.getBlock());
                 received[0]++;
                 break;
               case UPDATE_RECOVERED:
-                addSubopName(ReceivedDeletedBlockInfo.BlockStatus.UPDATE_RECOVERED.name());
                 addBlock(storage, rdbi.getBlock(), rdbi.getDelHints());
                 received[0]++;
                 break;
               case DELETED_BLOCK:
-                addSubopName(ReceivedDeletedBlockInfo.BlockStatus.DELETED_BLOCK.name());
                 removeStoredBlock(rdbi.getBlock(), storage.getDatanodeDescriptor());
                 deleted[0]++;
                 break;
               default:
                 String msg =
-                    "Unknown block status code reported by " + storage.getStorageID() + ": " + 
+                    "Unknown block status code reported by " + storage.getStorageID() + ": " +
                         rdbi;
                 blockLog.warn(msg);
                 assert false : msg; // if assertions are enabled, throw.
@@ -4471,6 +4509,142 @@ public Object performTask() throws IOException {
     }
   }
 
+
+  public boolean processIncrementalProvidedBlockReport(DatanodeStorageInfo storage,
+                                                    ReceivedDeletedBlockInfo rdbi,
+                                                    int[] receiving,
+                                                    int[] received,
+                                                    int[] deleted) throws IOException {
+
+    LOG.debug("HopsFS-Cloud. ProcessIncrementalProvidedBlockReport.  Status:" + rdbi.getStatus()+
+            " Block ID: "+rdbi.getBlock().getBlockId()+" GenStamp: "+rdbi.getBlock().getGenerationStamp());
+    UnsupportedOperationException up = new UnsupportedOperationException("Operation not yet " +
+            "supported for provided blocks");
+    switch (rdbi.getStatus()) {
+      case RECEIVING_BLOCK:
+        return false; //did not process here
+      case APPENDING:
+        throw up;
+        //received[0]++;
+        //break;
+      case RECOVERING_APPEND:
+        throw up;
+        //received[0]++;
+        //break;
+      case RECEIVED_BLOCK:
+        processAndHandleReportedProvidedBlock(storage, rdbi.getBlock(), ReplicaState.FINALIZED );
+        //No Hashes
+        received[0]++;
+        return true;
+      case UPDATE_RECOVERED:
+        processAndHandleReportedProvidedBlock(storage, rdbi.getBlock(), ReplicaState.FINALIZED );
+        //No Hashes
+        received[0]++;
+        return true;
+      case DELETED_BLOCK:
+        throw up;
+        //deleted[0]++;
+        //break;
+      default:
+        String msg = "Unknown block status code reported by "
+                + storage.getStorageID() + ": " + rdbi;
+        blockLog.warn(msg);
+        assert false : msg; // if assertions are enabled, throw.
+        return false;
+    }
+
+  }
+
+  private void processAndHandleReportedProvidedBlock(
+          DatanodeStorageInfo storageInfo, Block reportedBlock,
+          ReplicaState reportedState)
+          throws IOException {
+    // When a provided block's replica is reported to be FINALIZED then it means
+    // that the datanode has also successfully uploaded the block to the cloud.
+    // In this case
+    // Now as the block is backed by the cloud, it is save to remove all the
+    // replica information for this block. Now the block can be read by any
+    // datanode. After removing the replica information we also mark the 'block'
+    // to be COMPLETE. COMMITTED state of the block is skipped because before
+    // the NN receives the 'incrementalBR' or 'getAdditionalBlock' request the
+    // block has been uploaded to the cloud (i.e. COMPLETED).
+
+    //remove form pending
+    DatanodeDescriptor node = storageInfo.getDatanodeDescriptor();
+    node.decrementBlocksScheduled(storageInfo.getStorageType());
+
+    //Check for block corruption
+    BlockInfoContiguous storedBlock = blocksMap.getStoredBlock(reportedBlock);
+
+    if (storedBlock == null) {
+      // If blocksMap does not contain reported block id,
+      // the replica should be removed from the data-node.
+      blockLog.info("BLOCK* processReport: " + reportedBlock + " on " + storageInfo + " size " +
+              reportedBlock.getNumBytes() + " does not belong to any file");
+      LOG.error("HopsFS-Cloud. Delete This block");
+      invalidateBlocks.addProvidedBlock(reportedBlock);
+      return;
+    }
+
+    BlockUCState ucState = storedBlock.getBlockUCState();
+
+    Preconditions.checkArgument(ucState != BlockUCState.COMPLETE);
+    Preconditions.checkArgument(ucState != BlockUCState.COMMITTED);
+    //this is under construction block
+    BlockInfoContiguousUnderConstruction ucBlock = (BlockInfoContiguousUnderConstruction) storedBlock;
+    //RUR checks
+    if (ucBlock.getBlockUCState() == BlockUCState.UNDER_RECOVERY) {
+      Preconditions.checkArgument(ucBlock.getGenerationStamp() < reportedBlock.getGenerationStamp());
+    } else {
+      Preconditions.checkArgument(ucBlock.getGenerationStamp() == reportedBlock.getGenerationStamp());
+      Preconditions.checkArgument(ucBlock.getTruncateBlock() == null);
+    }
+
+    // check for corruption
+    BlockToMarkCorrupt c = checkReplicaCorrupt(reportedBlock, reportedState, storedBlock, ucState,
+            storageInfo);
+    if (c != null) {
+      LOG.error("HopsFS-Cloud. Incrmental block report. Unexptected corrupt replica received");
+      //Block report will mark this block corrupt.
+      return;
+    }
+
+    //
+    // remove the replica information from all tables except the block info table
+    //
+    BlockInfoContiguous completedBlock = commitOrCompleteLastProvidedBlock(reportedBlock, ucBlock);
+    completedBlock.removeAllReplicas();
+
+    corruptReplicas.removeFromCorruptReplicasMap(completedBlock);
+    pendingReplications.remove(completedBlock);
+    neededReplications.remove(completedBlock);
+    if (postponedMisreplicatedBlocks.remove(completedBlock)) {
+      postponedMisreplicatedBlocksCount.decrementAndGet();
+    }
+  }
+
+  public BlockInfoContiguous commitOrCompleteLastProvidedBlock(Block reportedBlock,
+                                                BlockInfoContiguousUnderConstruction ucBlock)
+          throws IOException {
+
+    //skip the COMMITTED state and the block is in the cloud storage
+    //Marking the block COMPLETED, Changing it to BlockInfoContiguous
+
+    INodeFile inode = (INodeFile) EntityManager.find(INode.Finder.ByINodeIdFTIS,
+            ucBlock.getInodeId());
+
+    BlockInfoContiguous completeBlock = ucBlock.convertToCompleteBlock();
+    // replace penultimate block in file
+    inode.setBlock(ucBlock.getBlockIndex(), completeBlock);
+    completeBlock.setNumBytes(reportedBlock.getNumBytes());
+
+    //we take the gen stamp from the reported block because in the case
+    //of block recovery the genstamp is increased.
+    completeBlock.setGenerationStamp(reportedBlock.getGenerationStamp());
+
+    return completeBlock;
+  }
+
   /**
    * Return the number of nodes hosting a given block, grouped
    * by the state of those replicas.
@@ -4568,7 +4742,7 @@ void processOverReplicatedBlocksOnReCommission(
     final List<Long> inodeIds = new ArrayList<>(inodeIdsToBlockMap.keySet());
 
     try {
-      Slicer.slice(inodeIds.size(), slicerBatchSize, slicerNbThreads, 
+      Slicer.slice(inodeIds.size(), slicerBatchSize, slicerNbThreads,
           ((FSNamesystem) namesystem).getFSOperationsExecutor(),
           new Slicer.OperationHandler() {
         @Override
@@ -4644,7 +4818,7 @@ boolean isNodeHealthyForDecommission(DatanodeDescriptor node) throws IOException
     if (pendingReplicationBlocksCount == 0 &&
         underReplicatedBlocksCount == 0) {
       LOG.info("Node {} is dead and there are no under-replicated" +
-          " blocks or blocks pending replication. Safe to decommission.", 
+          " blocks or blocks pending replication. Safe to decommission.",
           node);
       return true;
     }
@@ -4759,7 +4933,7 @@ public boolean checkBlocksProperlyReplicated(
     return true;
   }
 
-  /** 
+  /**
    * @return 0 if the block is not found;
    * otherwise, return the replication factor of the block.
    */
@@ -4776,7 +4950,7 @@ private int getReplication(Block block)
    *
    * @return number of blocks scheduled for removal during this iteration.
    */
-  private int invalidateWorkForOneNode(Map.Entry<DatanodeInfo, List<Integer>> entry) throws IOException {
+  private int invalidateWorkForOneNode(Map.Entry<DatanodeInfo, Set<Integer>> entry) throws IOException {
     // blocks should not be replicated or removed if safe mode is on
     if (namesystem.isInSafeMode()) {
       LOG.debug("In safemode, not computing replication work");
@@ -4784,12 +4958,27 @@ private int invalidateWorkForOneNode(Map.Entry<DatanodeInfo, List<Integer>> entr
     }
     // get blocks to invalidate for the nodeId
 
+    //HopsFS-Cloud
+    //Invalidated Blocks that are stored in the cloud have SID
+    //set to -1 (StorageId.CLOUD_STORAGE_ID). This SID belongs to
+    //a phantom datanode for cloud storage
+    //Blocks stored in the cloud are handled separately
+    //in the replication monitor.
+    //Simply return 0 for provided blocks
+
     DatanodeDescriptor dnDescriptor = datanodeManager.getDatanode(entry.getKey());
-
     if (dnDescriptor == null) {
+      List<Integer> sidsToRemove = new ArrayList<>();
+      //remove all sids except StorageId.CLOUD_STORAGE_ID
+      for(int sid : entry.getValue()){
+        if(sid != StorageId.CLOUD_STORAGE_ID){
+          sidsToRemove.add(sid);
+        }
+      }
+
       LOG.warn("DataNode " + entry.getKey() + " cannot be found for sids " +
-            Arrays.toString(entry.getValue().toArray()) + ", removing block invalidation work.");
-      invalidateBlocks.remove(entry.getValue());
+            Arrays.toString(sidsToRemove.toArray()) + ", removing block invalidation work.");
+      invalidateBlocks.remove(sidsToRemove);
       return 0;
     }
     
@@ -4856,8 +5045,8 @@ public long getMissingReplOneBlocksCount() throws IOException {
     // not locking
     return this.neededReplications.getCorruptReplOneBlockSize();
   }
-  
-  public BlockInfoContiguous addBlockCollection(BlockInfoContiguous block, 
+
+  public BlockInfoContiguous addBlockCollection(BlockInfoContiguous block,
       BlockCollection bc)
       throws StorageException, TransactionContextException {
     return blocksMap.addBlockCollection(block, bc);
@@ -5035,7 +5224,12 @@ int computeDatanodeWork() throws IOException {
     // Update counters
     this.updateState();
     this.scheduledReplicationBlocksCount = workFound;
-    workFound += this.computeInvalidateWork(nodesToProcess);
+    workFound += this.computeInvalidateWorkForDNs(nodesToProcess);
+
+    // Additional work assigned to DNs for deleting objects
+    // stored in the cloud
+    workFound += this.computeInvalidateWorkForCloud();
+
     return workFound;
   }
 
@@ -5253,41 +5447,6 @@ public Object performTask() throws IOException {
     }.handle(namesystem);
   }
 
-  public BlockInfoContiguous tryToCompleteBlock(final BlockCollection bc,
-      final int blkIndex) throws IOException {
-
-    if (blkIndex < 0) {
-      return null;
-    }
-    BlockInfoContiguous curBlock = bc.getBlock(blkIndex);
-    LOG.debug("tryToCompleteBlock. blkId = " + curBlock.getBlockId());
-    if (curBlock.isComplete()) {
-      return curBlock;
-    }
-    BlockInfoContiguousUnderConstruction ucBlock = (BlockInfoContiguousUnderConstruction) curBlock;
-    int numNodes = ucBlock.numNodes(datanodeManager);
-    if (numNodes < minReplication) {
-      return null;
-    }
-    if (ucBlock.getBlockUCState() != BlockUCState.COMMITTED) {
-      return null;
-    }
-    BlockInfoContiguous completeBlock = ucBlock.convertToCompleteBlock();
-    // replace penultimate block in file
-    bc.setBlock(blkIndex, completeBlock);
-
-    // Since safe-mode only counts complete blocks, and we now have
-    // one more complete block, we need to adjust the total up, and
-    // also count it as safe, if we have at least the minimum replica
-    // count. (We may not have the minimum replica count yet if this is
-    // a "forced" completion when a file is getting closed by an
-    // OP_CLOSE edit on the standby).
-    namesystem.adjustSafeModeBlockTotals(null, 1);
-    namesystem.incrementSafeBlockCount(Math.min(numNodes, minReplication),curBlock);
-
-    return completeBlock;
-  }
-
   @VisibleForTesting
   public void processTimedOutPendingBlock(final long timedOutItemId)
       throws IOException {
@@ -5549,6 +5708,10 @@ public void shutdown() {
       datanodeRemover.shutdown();
     }
     stopReplicationInitializer();
+
+    if (providedBlocksChecker != null){
+      providedBlocksChecker.shutDown();
+    }
   }
   
   public int getNumBuckets() {
@@ -5588,4 +5751,22 @@ public void blockReportCompleted(final DatanodeID nodeID, DatanodeStorage[] stor
       }
     }
   }
+
+  public InvalidateBlocks getInvalidateBlocks(){
+    return invalidateBlocks;
+  }
+
+  public void startProvidedBlocksChecker(Configuration conf){
+    if(isCloudEnabled){
+      LOG.info("HopsFS-Cloud. Starting Provide Blocks Checker");
+      providedBlocksChecker = new ProvidedBlocksChecker(conf, namesystem, this);
+      providedBlocksChecker.start();
+    }
+  }
+
+  @VisibleForTesting
+  public ProvidedBlocksChecker getProvidedBlocksChecker() {
+    return providedBlocksChecker;
+  }
+
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java
index ec341e02b5f..a4e9cbe65d5 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockStoragePolicySuite.java
@@ -44,6 +44,12 @@ public static BlockStoragePolicySuite createDefaultSuite() {
     final BlockStoragePolicy[] policies =
         new BlockStoragePolicy[1 << ID_BIT_LENGTH];
 
+    final byte cloud = HdfsConstants.CLOUD_STORAGE_POLICY_ID;
+    policies[cloud] = new BlockStoragePolicy(cloud,
+            HdfsConstants.CLOUD_STORAGE_POLICY_NAME,
+            new StorageType[]{StorageType.CLOUD},
+            new StorageType[]{StorageType.CLOUD},
+            new StorageType[]{StorageType.CLOUD});
     final byte db = HdfsConstants.DB_STORAGE_POLICY_ID;
     policies[db] = new BlockStoragePolicy(db,
         HdfsConstants.DB_STORAGE_POLICY_NAME,
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java
index 6225de6aef8..7e7329488ee 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlocksMap.java
@@ -48,7 +48,7 @@
       Collections.unmodifiableList(new ArrayList<DatanodeDescriptor>());
   private final static List<DatanodeStorageInfo> empty_storage_list =
       Collections.unmodifiableList(new ArrayList<DatanodeStorageInfo>());
-  
+
   BlocksMap(DatanodeManager datanodeManager) {
     this.datanodeManager = datanodeManager;
   }
@@ -122,7 +122,9 @@ BlockInfoContiguous getStoredBlock(Block b)
     if (storedBlock == null) {
       return null;
     }
+
     DatanodeStorageInfo[] desc = storedBlock.getStorages(datanodeManager);
+
     if (desc == null) {
       return empty_storage_list;
     } else {
@@ -165,7 +167,15 @@ BlockInfoContiguous getStoredBlock(Block b)
    */
   int numNodes(Block b) throws StorageException, TransactionContextException {
     BlockInfoContiguous info = getStoredBlock(b);
-    return info == null ? 0 : info.numNodes(datanodeManager);
+    if (info == null) {
+      return 0;
+    } else {
+      if(info.isProvidedBlock()){
+        return 1;
+      } else {
+        return info.numNodes(datanodeManager);
+      }
+    }
   }
 
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
index 0342ceee9b4..e6b1c5639f1 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java
@@ -750,10 +750,17 @@ void addBlockToBeRecovered(BlockInfoContiguousUnderConstruction block) {
    */
   void addBlocksToBeInvalidated(List<Block> blocklist) {
     assert (blocklist != null && blocklist.size() > 0);
+    for (Block blk : blocklist) {
+      addBlockToBeInvalidated(blk);
+    }
+  }
+
+  /**
+   * Store block invalidation work.
+   */
+  void addBlockToBeInvalidated(Block block) {
     synchronized (invalidateBlocks) {
-      for (Block blk : blocklist) {
-        invalidateBlocks.add(blk);
-      }
+        invalidateBlocks.add(block);
     }
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
index 19e5b0d21a3..b878454915f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java
@@ -1741,13 +1741,13 @@ void addDnToStorageMapInDB(DatanodeDescriptor nodeDescr) throws IOException {
   }
 
   Random rand = new Random(System.currentTimeMillis());
-  public DatanodeDescriptor getRandomDN(List<DatanodeInfo> existing, int tries){
+  public DatanodeDescriptor getRandomDN(List<DatanodeInfo> existing){
     if(datanodeMap.isEmpty()){
         return null;
     }else{
-      for(int i = 0; i < tries; i++){
-        DatanodeDescriptor dd = (DatanodeDescriptor) datanodeMap.values()
-                .toArray()[rand.nextInt(datanodeMap.size())];
+      Object[] dns = datanodeMap.values().toArray();
+      for(int i = 0; i < datanodeMap.size(); i++){
+        DatanodeDescriptor dd = (DatanodeDescriptor)dns[rand.nextInt(dns.length)];
         if(!existing.contains(dd)){
           return dd;
         }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java
index 4f1af960433..a32c02394ae 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java
@@ -22,6 +22,8 @@
 import io.hops.metadata.HdfsStorageFactory;
 import io.hops.metadata.hdfs.dal.InvalidateBlockDataAccess;
 import io.hops.metadata.hdfs.entity.InvalidatedBlock;
+import io.hops.metadata.hdfs.entity.ProvidedBlockCacheLoc;
+import io.hops.metadata.hdfs.entity.StorageId;
 import io.hops.transaction.EntityManager;
 import io.hops.transaction.handler.HDFSOperationType;
 import io.hops.transaction.handler.LightWeightRequestHandler;
@@ -32,19 +34,13 @@
 
 import java.io.IOException;
 import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Calendar;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.GregorianCalendar;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
 
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.DFSUtil;
+import org.apache.hadoop.hdfs.server.namenode.INode;
+import org.apache.hadoop.hdfs.server.namenode.INodeFile;
 import org.apache.hadoop.util.Time;
 
 import com.google.common.annotations.VisibleForTesting;
@@ -132,6 +128,7 @@ void add(final BlockInfoContiguous block, final DatanodeStorageInfo storage,
         storage.getSid(),
         block.getBlockId(),
         block.getGenerationStamp(),
+        block.getCloudBucketID(),
         block.getNumBytes(),
         block.getInodeId());
 
@@ -142,6 +139,34 @@ void add(final BlockInfoContiguous block, final DatanodeStorageInfo storage,
     }
   }
 
+  /**
+   * Add a block to the block collection
+   * which will be invalidated on the specified storage.
+   */
+  void addProvidedBlock(final Block block)
+          throws StorageException, TransactionContextException {
+
+    long inodeID = INode.NON_EXISTING_INODE_ID;
+    if(block instanceof BlockInfoContiguous){
+      inodeID = ((BlockInfoContiguous) block).getInodeId();
+    }
+
+    InvalidatedBlock invBlk = new InvalidatedBlock(
+            StorageId.CLOUD_STORAGE_ID,
+            block.getBlockId(),
+            block.getGenerationStamp(),
+            block.getCloudBucketID(),
+            block.getNumBytes(),
+            inodeID);
+
+    if (add(invBlk)) {
+      LOG.info("BLOCK* HopsFS-Cloud. Provided block scheduled for deletion. Added to Inv Table. " +
+              "BlockID:" + " "+block.getBlockId());
+    } else {
+      LOG.info("BLOCK* HopsFS-Cloud. Failed to schedule deletion of provided block. Block ID: "+block.getBlockId());
+    }
+  }
+
   /**
    * Remove a list of storages (typically all storages on a datanode) from the
    * invalidatesSet
@@ -230,14 +255,82 @@ long getInvalidationDelay() {
     final Iterator<InvalidatedBlock> it = invBlocks.iterator();
     for (int count = 0; count < limit && it.hasNext(); count++) {
       InvalidatedBlock invBlock = it.next();
-      toInvalidate.add(new Block(invBlock.getBlockId(), invBlock.getNumBytes(), invBlock.getGenerationStamp()));
+      toInvalidate.add(new Block(invBlock.getBlockId(), invBlock.getNumBytes(),
+              invBlock.getGenerationStamp(), invBlock.getCloudBucketID()));
       toInvblks.add(invBlock);
     }
     removeInvBlocks(toInvblks);
     dn.addBlocksToBeInvalidated(toInvalidate);
     return toInvalidate;
   }
-  
+
+
+  List<Block> invalidateWorkForCloud(int maxWork, DatanodeManager dnManager) throws IOException {
+    List<InvalidatedBlock> invBlocks = findInvBlksInCloud(maxWork);
+
+    if ( invBlocks.size() > 0) {
+      List<Block> deleteBlocks = new ArrayList<>(invBlocks.size());
+
+      for (InvalidatedBlock invBlk : invBlocks) {
+        deleteBlocks.add(new Block(invBlk.getBlockId(), invBlk.getNumBytes(),
+                invBlk.getGenerationStamp(), invBlk.getCloudBucketID()));
+      }
+
+      // remove invalidated blocks from the database
+      removeInvBlocks(invBlocks);
+
+      //get cached locations
+      Map<Long, ProvidedBlockCacheLoc> cacheLocMap =
+              ProvidedBlocksCacheHelper.batchReadCacheLocs(deleteBlocks);
+
+      //remove the cache mapping
+      ProvidedBlocksCacheHelper.deleteProvidedBlockCacheLocation(deleteBlocks);
+
+      // if the block is cahed on a alive datanode then schedule deletion through
+      // this datanode, else, assign the deletion request to a random datanode.
+
+      for (Block deletedBlock : deleteBlocks){
+        boolean assigned = false;
+        ProvidedBlockCacheLoc loc = cacheLocMap.get(deletedBlock.getBlockId());
+        if (loc != null){
+          int sid = loc.getStorageID();
+          DatanodeDescriptor dn = dnManager.getDatanodeBySid(sid);
+          if ( dn.isAlive ){
+            dn.addBlockToBeInvalidated(deletedBlock);
+            assigned = true;
+            LOG.debug("HopsFS-Cloud. Replication Monitor. Deletion of Blk: "+deletedBlock+
+                    " is assigned to "+dn);
+          }
+        }
+
+        if (!assigned) {
+          DatanodeDescriptor randDN = dnManager.getRandomDN(Collections.EMPTY_LIST);
+          randDN.addBlockToBeInvalidated(deletedBlock);
+          LOG.debug("HopsFS-Cloud. Replication Monitor. Deletion of Blk: "+deletedBlock+
+                  " is assigned to "+randDN);
+        }
+      }
+
+      return deleteBlocks;
+    } else {
+      return Collections.EMPTY_LIST;
+    }
+  }
+
+  private List<InvalidatedBlock> findInvBlksInCloud(final int limit)
+          throws IOException {
+    return (List<InvalidatedBlock>) new LightWeightRequestHandler(
+            HDFSOperationType.GET_INV_BLKS_IN_CLOUD) {
+      @Override
+      public Object performTask() throws StorageException, IOException {
+        InvalidateBlockDataAccess da =
+                (InvalidateBlockDataAccess) HdfsStorageFactory
+                        .getDataAccess(InvalidateBlockDataAccess.class);
+        return da.findInvalidatedBlockInCloudList(StorageId.CLOUD_STORAGE_ID, limit);
+      }
+    }.handle();
+  }
+
   void clear() throws IOException {
     new LightWeightRequestHandler(HDFSOperationType.DEL_ALL_INV_BLKS) {
       @Override
@@ -250,9 +343,19 @@ public Object performTask() throws StorageException, IOException {
       }
     }.handle();
   }
-  
-  
+
   void add(final Collection<Block> blocks, final DatanodeStorageInfo storage)
+          throws IOException {
+    List<InvalidatedBlock> invblks = new ArrayList<>();
+    for (Block blk : blocks) {
+      invblks.add(new InvalidatedBlock(storage.getSid(), blk.getBlockId(),
+              blk.getGenerationStamp(), blk.getCloudBucketID(), blk.getNumBytes(),
+              INode.NON_EXISTING_INODE_ID));
+    }
+    addAll(invblks);
+  }
+
+  public void addAll(final List<InvalidatedBlock> invblks)
       throws IOException {
     new LightWeightRequestHandler(HDFSOperationType.ADD_INV_BLOCKS) {
       @Override
@@ -260,11 +363,6 @@ public Object performTask() throws StorageException, IOException {
         InvalidateBlockDataAccess da =
             (InvalidateBlockDataAccess) HdfsStorageFactory
                 .getDataAccess(InvalidateBlockDataAccess.class);
-        List<InvalidatedBlock> invblks = new ArrayList<>();
-        for (Block blk : blocks) {
-          invblks.add(new InvalidatedBlock(storage.getSid(), blk.getBlockId(),
-              blk.getGenerationStamp(), blk.getNumBytes(), BlockInfoContiguous.NON_EXISTING_ID));
-        }
         da.prepare(Collections.EMPTY_LIST, invblks, Collections.EMPTY_LIST);
         return null;
       }
@@ -360,19 +458,18 @@ public Object performTask() throws StorageException, IOException {
         .findList(InvalidatedBlock.Finder.All);
   }
 
-  public Map<DatanodeInfo, List<Integer>> getDatanodes(DatanodeManager manager)
+  public Map<DatanodeInfo, Set<Integer>> getDatanodes(DatanodeManager manager)
       throws IOException {
-    Map<DatanodeInfo, List<Integer>> nodes = new HashMap<DatanodeInfo, List<Integer>>();
+    Map<DatanodeInfo, Set<Integer>> nodes = new HashMap<>();
     for(int sid : getSids()) {
       DatanodeInfo node = manager.getDatanodeBySid(sid);
       
-      List<Integer> sids = nodes.get(node);
+      Set<Integer> sids = nodes.get(node);
       if(sids==null){
-        sids = new ArrayList<>();
+        sids = new HashSet<>();
         nodes.put(node, sids);
       }
       sids.add(sid);
-      
     }
 
     return nodes;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedBlocksCacheHelper.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedBlocksCacheHelper.java
new file mode 100644
index 00000000000..7397317e919
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedBlocksCacheHelper.java
@@ -0,0 +1,126 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.blockmanagement;
+
+import io.hops.exception.StorageException;
+import io.hops.metadata.HdfsStorageFactory;
+import io.hops.metadata.hdfs.dal.ProvidedBlockCacheLocDataAccess;
+import io.hops.metadata.hdfs.entity.InvalidatedBlock;
+import io.hops.metadata.hdfs.entity.ProvidedBlockCacheLoc;
+import io.hops.metadata.hdfs.entity.ReplicaBase;
+import io.hops.transaction.handler.HDFSOperationType;
+import io.hops.transaction.handler.HopsTransactionalRequestHandler;
+import io.hops.transaction.handler.LightWeightRequestHandler;
+import io.hops.transaction.lock.TransactionLocks;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hdfs.protocol.Block;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+
+public class ProvidedBlocksCacheHelper {
+  public static final Log LOG = LogFactory.getLog(ProvidedBlocksCacheHelper.class);
+
+  public static ProvidedBlockCacheLoc getProvidedBlockCacheLocation(final long blkID)
+          throws StorageException {
+    try {
+      LightWeightRequestHandler h =
+              new LightWeightRequestHandler(HDFSOperationType.GET_CLOUD_BLKS_CACHE_LOC) {
+                @Override
+                public Object performTask() throws IOException {
+                  ProvidedBlockCacheLocDataAccess da = (ProvidedBlockCacheLocDataAccess) HdfsStorageFactory
+                          .getDataAccess(ProvidedBlockCacheLocDataAccess.class);
+                  return da.findByBlockID(blkID);
+                }
+              };
+      return (ProvidedBlockCacheLoc) h.handle();
+    } catch (IOException e) {
+      LOG.error(e, e);
+      StorageException up = new StorageException(e);
+      throw up;
+    }
+  }
+
+  public static void updateProvidedBlockCacheLocation(final Block newBlock,
+                                                      final DatanodeStorageInfo[] targets) throws IOException {
+    new LightWeightRequestHandler(HDFSOperationType.UPDATE_CLOUD_BLKS_CACHE_LOC) {
+      @Override
+      public Object performTask() throws IOException {
+        ProvidedBlockCacheLocDataAccess da = (ProvidedBlockCacheLocDataAccess) HdfsStorageFactory
+                .getDataAccess(ProvidedBlockCacheLocDataAccess.class);
+        LOG.debug("HopsFS-Cloud. Added  provided block cache entry for Block ID: " + newBlock.getBlockId());
+        assert targets.length == 1;
+        List<ProvidedBlockCacheLoc> locs = new ArrayList<>();
+        locs.add(new ProvidedBlockCacheLoc(newBlock.getBlockId(), targets[0].getSid()));
+        da.update(locs);
+        return null;
+      }
+    }.handle();
+  }
+
+  public static void deleteProvidedBlockCacheLocation(final List<Block> deletedBlocks) throws IOException {
+    new LightWeightRequestHandler(HDFSOperationType.DELETE_CLOUD_BLKS_CACHE_LOC) {
+      @Override
+      public Object performTask() throws IOException {
+        ProvidedBlockCacheLocDataAccess da = (ProvidedBlockCacheLocDataAccess) HdfsStorageFactory
+                .getDataAccess(ProvidedBlockCacheLocDataAccess.class);
+        List<Long> locs = new ArrayList<>();
+        for (Block block : deletedBlocks) {
+          if(da.findByBlockID(block.getBlockId()) != null) {
+            locs.add(block.getBlockId());
+          }
+        }
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("HopsFS-Cloud. Deleting cache entry for block ID: " +
+                  Arrays.toString(locs.toArray()));
+        }
+        da.delete(locs);
+        return null;
+      }
+    }.handle();
+  }
+
+  public static Map<Long, ProvidedBlockCacheLoc> batchReadCacheLocs(final List<Block> blocks)
+          throws IOException {
+    HopsTransactionalRequestHandler h =
+            new HopsTransactionalRequestHandler(
+                    HDFSOperationType.BATCH_READ_CLOUD_BLKS_CACHE_LOCS) {
+              @Override
+              public Object performTask() throws IOException {
+                ProvidedBlockCacheLocDataAccess da = (ProvidedBlockCacheLocDataAccess) HdfsStorageFactory
+                        .getDataAccess(ProvidedBlockCacheLocDataAccess.class);
+                long locs[] = new long[blocks.size()];
+                for (int i = 0; i < blocks.size(); i++) {
+                  locs[i] = blocks.get(i).getBlockId();
+                }
+                if (LOG.isDebugEnabled()) {
+                  LOG.debug("HopsFS-Cloud. Batch read cache entries for block IDs: " +
+                          Arrays.toString(locs));
+                }
+                return da.findByBlockIDs(locs);
+              }
+
+              @Override
+              public void acquireLock(TransactionLocks locks) throws IOException {
+              }
+            };
+    return (Map<Long, ProvidedBlockCacheLoc>) h.handle();
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedBlocksChecker.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedBlocksChecker.java
new file mode 100644
index 00000000000..18a8255e848
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedBlocksChecker.java
@@ -0,0 +1,670 @@
+/*
+ * Copyright (C) 2019 Logical Clocks AB.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.blockmanagement;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
+import io.hops.exception.StorageException;
+import io.hops.leaderElection.LeaderElection;
+import io.hops.metadata.HdfsStorageFactory;
+import io.hops.metadata.HdfsVariables;
+import io.hops.metadata.Variables;
+import io.hops.metadata.common.entity.LongVariable;
+import io.hops.metadata.common.entity.Variable;
+import io.hops.metadata.hdfs.dal.BlockInfoDataAccess;
+import io.hops.metadata.hdfs.dal.ProvidedBlockReportTasksDataAccess;
+import io.hops.metadata.hdfs.entity.INodeIdentifier;
+import io.hops.metadata.hdfs.entity.InvalidatedBlock;
+import io.hops.metadata.hdfs.entity.ProvidedBlockReportTask;
+import io.hops.metadata.hdfs.entity.StorageId;
+import io.hops.transaction.EntityManager;
+import io.hops.transaction.handler.HDFSOperationType;
+import io.hops.transaction.handler.HopsTransactionalRequestHandler;
+import io.hops.transaction.handler.LightWeightRequestHandler;
+import io.hops.transaction.lock.LockFactory;
+import io.hops.transaction.lock.TransactionLocks;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.CloudBlock;
+import org.apache.hadoop.hdfs.server.common.CloudHelper;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudPersistenceProviderFactory;
+import org.apache.commons.logging.Log;
+
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;
+import org.apache.hadoop.hdfs.server.namenode.INode;
+import org.apache.hadoop.hdfs.server.namenode.Namesystem;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+
+import static org.apache.hadoop.hdfs.DFSConfigKeys.*;
+
+/*
+ Compare the blocks in the DB and in the cloud bucket(s)
+ */
+public class ProvidedBlocksChecker extends Thread {
+  private final Namesystem ns;
+  private boolean run = true;
+  private final int prefixSize;
+  private final int numBuckets;
+  private final long blockReportDelay;
+  private final long sleepInterval;
+  private final long markMissingBlockDelay;
+  private final long maxSubTasks;
+  private final BlockManager bm;
+  private final int maxProvidedBRThreads;
+  private boolean isBRInProgress = false;
+
+  CloudPersistenceProvider cloudConnector;
+
+  static final Log LOG = LogFactory.getLog(ProvidedBlocksChecker.class);
+
+  public ProvidedBlocksChecker(Configuration conf, Namesystem ns, BlockManager bm) {
+    this.ns = ns;
+    this.bm = bm;
+    this.prefixSize = conf.getInt(
+            DFS_CLOUD_PREFIX_SIZE_KEY,
+            DFS_CLOUD_PREFIX_SIZE_DEFAULT);
+    this.numBuckets = conf.getInt(
+            DFS_CLOUD_AWS_S3_NUM_BUCKETS,
+            DFS_CLOUD_AWS_S3_NUM_BUCKETS_DEFAULT);
+    this.blockReportDelay = conf.getLong(
+            DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+            DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+    this.sleepInterval = conf.getLong(
+            DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY,
+            DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_DEFAULT);
+    this.markMissingBlockDelay = conf.getLong(
+            DFS_CLOUD_MARK_PARTIALLY_LISTED_BLOCKS_CORRUPT_AFTER_KEY,
+            DFS_CLOUD_MARK_PARTIALLY_LISTED_BLOCKS_CORRUPT_AFTER_DEFAULT);
+    this.maxSubTasks = conf.getLong(DFS_CLOUD_MAX_BR_SUB_TASKS_KEY,
+            DFS_CLOUD_MAX_BR_SUB_TASKS_DEFAULT);
+    this.maxProvidedBRThreads = conf.getInt(DFS_CLOUD_MAX_BR_THREADS_KEY,
+            DFS_CLOUD_MAX_BR_THREADS_DEFAULT);
+
+    this.cloudConnector = CloudPersistenceProviderFactory.getCloudClient(conf);
+
+  }
+
+  /*
+   * If Leader
+   * ---------
+   *    If it is time to block report then add all the sub tasks in the queue
+   *
+   * All nodes
+   *    Poll the BR tasks queue and find work if any
+   */
+  @Override
+  public void run() {
+    while (run) {
+      try {
+
+        if (ns.isLeader()) {
+          long startTime = getProvidedBlocksScanStartTime();
+          long existingTasks = countBRPendingTasks();
+          long timeElapsed = System.currentTimeMillis() - startTime;
+
+          if (timeElapsed > blockReportDelay && existingTasks == 0) {
+            final long END_ID = HdfsVariables.getMaxBlockID();
+            List<ProvidedBlockReportTask> tasks = generateTasks(END_ID);
+            addNewBlockReportTasks(tasks);
+          }
+        }
+
+        // poll for work
+        if (countBRPendingTasks() != 0) {
+          startWork();
+        }
+
+      } catch (IOException e) {
+        LOG.warn(e, e);
+      } finally {
+        if (run) {
+          try {
+            Thread.sleep(sleepInterval);
+          } catch (InterruptedException e) {
+            currentThread().interrupt();
+          }
+        }
+      }
+    }
+  }
+
+  private void startWork() throws IOException {
+    //start workers and wait for them to finish their work
+    Collection workers = new ArrayList<>();
+    for (int i = 0; i < maxProvidedBRThreads; i++) {
+      workers.add(new BRTasksPullers(i));
+    }
+
+    try {
+      isBRInProgress = true;
+      List<Future<Object>> futures =
+              ((FSNamesystem) ns).getFSOperationsExecutor().invokeAll(workers);
+      //Check for exceptions
+      for (Future<Object> maybeException : futures) {
+        maybeException.get();
+      }
+
+    } catch (InterruptedException e) {
+      LOG.error(e.getMessage(), e);
+      throw new IOException(e);
+    } catch (ExecutionException e) {
+      if (e.getCause() instanceof IOException) {
+        throw (IOException) e.getCause();
+      } else {
+        throw new IOException(e.getCause());
+      }
+    } finally {
+      isBRInProgress = false;
+    }
+  }
+
+  private boolean processTask(ProvidedBlockReportTask task) throws IOException {
+    boolean successful = false;
+    try {
+      for (long start = task.getStartIndex(); start < task.getEndIndex(); ) {
+        long end = start + prefixSize;
+        String prefix = CloudHelper.getPrefix(prefixSize, start);
+        LOG.debug("HopsFS-Cloud. BR Checking prefix: " + prefix);
+        Map<Long, CloudBlock> cloudBlocksMap = cloudConnector.getAll(prefix);
+        Map<Long, BlockInfoContiguous> dbBlocksMap = findAllBlocksRange(start, end);
+        LOG.debug("HopsFS-Cloud. BR DB view size: " + dbBlocksMap.size() +
+                " Cloud view size: " + cloudBlocksMap.size());
+
+        List<BlockInfoContiguous> toMissing = new ArrayList<>();
+        List<BlockToMarkCorrupt> toCorrupt = new ArrayList<>();
+        List<CloudBlock> toDelete = new ArrayList<>();
+        reportDiff(dbBlocksMap, cloudBlocksMap, toMissing, toCorrupt, toDelete);
+        LOG.debug("HopsFS-Cloud. BR toMissing: " + toMissing.size() +
+                " toCorrupt: " + toCorrupt.size() +
+                " toDelete: " + toDelete.size()+ " Prefix: "+prefix);
+        handleMissingBlocks(toMissing);
+        handleCorruptBlocks(toCorrupt);
+        handleToDeleteBlocks(toDelete);
+
+        start = end;
+      }
+      successful = true;
+    } catch (Exception e) {
+      LOG.warn(e, e);
+    } finally {
+      return successful;
+    }
+  }
+
+  public List<ProvidedBlockReportTask> generateTasks(long maxBlkID) {
+    List<ProvidedBlockReportTask> tasks = new ArrayList<>();
+    long prefixesToScan = (long) Math.ceil((double) maxBlkID / (double) prefixSize);
+    long taskSize = (long) Math.ceil((double) prefixesToScan / (double) maxSubTasks);
+    long startIndex = 0;
+    while (startIndex < maxBlkID) {
+      long endIndex = (startIndex) + taskSize * prefixSize;
+      ProvidedBlockReportTask task = new ProvidedBlockReportTask(
+              startIndex,
+              endIndex,
+              0, LeaderElection.LEADER_INITIALIZATION_ID);
+      tasks.add(task);
+      startIndex = endIndex;
+    }
+    return tasks;
+  }
+
+  /**
+   * This class is used to pull work from the queue and execute the operation
+   */
+  class BRTasksPullers implements Callable {
+    private int id;
+    private  int count;
+    BRTasksPullers(int id){
+      this.id  = id;
+    }
+    @Override
+    public Object call() throws Exception {
+      ProvidedBlockReportTask task = null;
+      do {
+        task = popPendingBRTask();
+        if (task != null) {
+          processTask(task);
+          count++;
+        } else {
+          LOG.info("HopsFS-Cloud. BR Worker ID: "+id+" processed "+count+" tasks");
+          return null;
+        }
+      } while (true);
+    }
+  }
+
+
+  /*
+   * A block is marked corrupt when
+   *    1. If the block's metadata exists in the database, but the block is absent form the
+   *       bucket listing.
+   *    2. Partial block information was obtained using cloud block listing. In S3 ls operation
+   *       is eventually consistent. That is, it is possible that for a block the ls operation
+   *       might return the meta block information but miss the actual block, or vice versa.
+   *       Such blocks are marked corrupt for the time being
+   *    3. The generations stamp, block size or bucket id does not match
+   *
+   *  A block is marked for deletion if it is in the cloud bucket listing but no corresponding
+   *    metadata is present in the database.
+   */
+  @VisibleForTesting
+  public void reportDiff(Map<Long, BlockInfoContiguous> dbView, Map<Long, CloudBlock> cView,
+                         List<BlockInfoContiguous> toMissing, List<BlockToMarkCorrupt> toCorrupt,
+                         List<CloudBlock> toDelete) throws IOException {
+    final Set<Long> aggregatedSafeBlocks = new HashSet<>();
+    aggregatedSafeBlocks.addAll(dbView.keySet());
+
+    for (BlockInfoContiguous dbBlock : dbView.values()) {
+
+      CloudBlock cblock = cView.get(dbBlock.getBlockId());
+      BlockToMarkCorrupt cb = null;
+
+      //under construction blocks should be handled using normal block reports.
+      if(dbBlock instanceof  BlockInfoContiguousUnderConstruction){
+        aggregatedSafeBlocks.remove(dbBlock.getBlockId());
+        continue;
+      }
+
+      if (cblock == null  || cblock.isPartiallyListed()) {
+        // if the block is partially listed  or totally missing form the listing
+        // then only mark the block corrupt after t, because s3 is eventually consistent
+        if ((System.currentTimeMillis() - dbBlock.getTimestamp()) > markMissingBlockDelay) {
+          toMissing.add(dbBlock);
+          aggregatedSafeBlocks.remove(dbBlock.getBlockId()); //this block is not safe now
+        }
+        cView.remove(dbBlock.getBlockId());
+        continue;
+      } else if (cblock.getBlock().getGenerationStamp() != dbBlock.getGenerationStamp()) {
+        cb = new BlockToMarkCorrupt(cblock, dbBlock, "Generation stamp mismatch",
+                CorruptReplicasMap.Reason.GENSTAMP_MISMATCH);
+      } else if (cblock.getBlock().getNumBytes() != dbBlock.getNumBytes()) {
+        cb = new BlockToMarkCorrupt(cblock, dbBlock, "Block size mismatch",
+                CorruptReplicasMap.Reason.SIZE_MISMATCH);
+      } else if (cblock.getBlock().getCloudBucketID() != dbBlock.getCloudBucketID()) {
+        cb = new BlockToMarkCorrupt(cblock, dbBlock, "Cloud bucket mismatch",
+                CorruptReplicasMap.Reason.INVALID_STATE);
+      } else {
+        //not corrupt
+        cView.remove(dbBlock.getBlockId());
+      }
+
+      if (cb != null) {
+        toCorrupt.add(cb);
+        aggregatedSafeBlocks.remove(dbBlock.getBlockId()); //this block is not safe now
+        cView.remove(dbBlock.getBlockId());
+      }
+    }
+
+    for (CloudBlock cloudBlock : cView.values()) {
+      toDelete.add(cloudBlock);
+    }
+
+    //safe blocks
+    if (ns.isInStartupSafeMode()) {
+      LOG.debug("HopsFS-Cloud. BR Aggregated safe block #: " + aggregatedSafeBlocks.size());
+      ns.adjustSafeModeBlocks(aggregatedSafeBlocks);
+    }
+  }
+
+  private void handleMissingBlocks(final List<BlockInfoContiguous> missingBlocks)
+          throws IOException {
+    //highest level of corruption --> all replicas are corrupt or missing
+    for (BlockInfoContiguous blk : missingBlocks) {
+      addCorrptUnderReplicatedBlock(blk);
+    }
+  }
+
+  private void handleCorruptBlocks(List<BlockToMarkCorrupt> corruptBlocks)
+          throws IOException {
+    for (BlockToMarkCorrupt b : corruptBlocks) {
+      addCorrptUnderReplicatedBlock(b.stored);
+    }
+  }
+
+  private void addCorrptUnderReplicatedBlock(final BlockInfoContiguous block) throws IOException {
+    new HopsTransactionalRequestHandler(HDFSOperationType.CLOUD_ADD_CORRUPT_BLOCKS) {
+      INodeIdentifier inodeIdentifier;
+
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        LockFactory lf = LockFactory.getInstance();
+        locks.add(lf.getIndividualBlockLock(block.getBlockId(), inodeIdentifier))
+                .add(lf.getBlockRelated(LockFactory.BLK.UR));
+      }
+
+      @Override
+      public Object performTask() throws StorageException, IOException {
+        bm.neededReplications.add(block, 0, 0, 1);
+        return null;
+      }
+    }.handle();
+
+  }
+
+
+  private void handleToDeleteBlocks(List<CloudBlock> toDelete) throws IOException {
+    List<InvalidatedBlock> invblks = new ArrayList<>();
+    for (CloudBlock cblock : toDelete) {
+      Block block = cblock.getBlock();
+      InvalidatedBlock invBlk = new InvalidatedBlock(
+              StorageId.CLOUD_STORAGE_ID,
+              block.getBlockId(),
+              block.getGenerationStamp(),
+              block.getCloudBucketID(),
+              block.getNumBytes(),
+              INode.NON_EXISTING_INODE_ID);
+      invblks.add(invBlk);
+    }
+    bm.getInvalidateBlocks().addAll(invblks);
+  }
+
+  public void shutDown() {
+    run = false;
+    interrupt();
+  }
+
+  public Map<Long, BlockInfoContiguous> findAllBlocksRange(final long startID, final long endID)
+          throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.BR_GET_RANGE_OF_BLOCKS) {
+              @Override
+              public Object performTask() throws IOException {
+                Map<Long, BlockInfoContiguous> blkMap = new HashMap<>();
+                BlockInfoDataAccess da = (BlockInfoDataAccess) HdfsStorageFactory
+                        .getDataAccess(BlockInfoDataAccess.class);
+
+                List<BlockInfoContiguous> blocks = da.findAllBlocks(startID, endID);
+                for (BlockInfoContiguous blk : blocks) {
+                  blkMap.put(blk.getBlockId(), blk);
+                }
+                return blkMap;
+              }
+            };
+    return (Map<Long, BlockInfoContiguous>) handler.handle();
+  }
+
+  public class BlockToMarkCorrupt {
+    /**
+     * The corrupted block in a datanode.
+     */
+    final CloudBlock corrupted;
+    /**
+     * The corresponding block stored in the BlockManager.
+     */
+    final BlockInfoContiguous stored;
+    /**
+     * The reason to mark corrupt.
+     */
+    final String reason;
+    /**
+     * The reason code to be stored
+     */
+    final CorruptReplicasMap.Reason reasonCode;
+
+    BlockToMarkCorrupt(CloudBlock corrupted,
+                       BlockInfoContiguous stored, String reason,
+                       CorruptReplicasMap.Reason reasonCode) {
+      Preconditions.checkNotNull(stored, "stored is null");
+
+      this.corrupted = corrupted;
+      this.stored = stored;
+      this.reason = reason;
+      this.reasonCode = reasonCode;
+    }
+
+    @Override
+    public String toString() {
+      return stored + " Reason: " + reason;
+    }
+  }
+
+  public static void scheduleBlockReportNow() throws IOException {
+    LOG.debug("HopsFS-Cloud. BR Scheduling a block report now");
+    setProvidedBlocksScanStartTime(0L);
+  }
+
+  public ProvidedBlockReportTask popPendingBRTask()
+          throws IOException {
+    HopsTransactionalRequestHandler handler = new HopsTransactionalRequestHandler(
+            HDFSOperationType.BR_POP_TASK) {
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        //take a lock on HdfsVariables.providedBlocksCheckStartTime to sync
+        HdfsStorageFactory.getConnector().writeLock();
+        LongVariable var = (LongVariable) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime);
+      }
+
+      @Override
+      public Object performTask() throws IOException {
+        EntityManager.preventStorageCall(false);
+        ProvidedBlockReportTasksDataAccess da = (ProvidedBlockReportTasksDataAccess) HdfsStorageFactory
+                .getDataAccess(ProvidedBlockReportTasksDataAccess.class);
+        ProvidedBlockReportTask task = (ProvidedBlockReportTask) da.popTask();
+        LOG.debug("HopsFS-Cloud. BR pulled a task from queue Task: " + task);
+        return task;
+      }
+    };
+    return (ProvidedBlockReportTask) handler.handle();
+  }
+
+  public long countBRPendingTasks()
+          throws IOException {
+    HopsTransactionalRequestHandler handler = new HopsTransactionalRequestHandler(
+            HDFSOperationType.BR_COUNT_TASKS) {
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        //take a lock on HdfsVariables.providedBlocksCheckStartTime to sync
+        HdfsStorageFactory.getConnector().writeLock();
+        LongVariable var = (LongVariable) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime);
+      }
+
+      @Override
+      public Object performTask() throws IOException {
+        EntityManager.preventStorageCall(false);
+        ProvidedBlockReportTasksDataAccess da = (ProvidedBlockReportTasksDataAccess) HdfsStorageFactory
+                .getDataAccess(ProvidedBlockReportTasksDataAccess.class);
+        return da.count();
+      }
+    };
+    return (long) handler.handle();
+  }
+
+  public List<ProvidedBlockReportTask> getAllTasks()
+          throws IOException {
+    HopsTransactionalRequestHandler handler = new HopsTransactionalRequestHandler(
+            HDFSOperationType.BR_GET_ALL_TASKS) {
+
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        //take a lock on HdfsVariables.providedBlocksCheckStartTime to sync
+        HdfsStorageFactory.getConnector().writeLock();
+        LongVariable var = (LongVariable) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime);
+      }
+
+      @Override
+      public Object performTask() throws IOException {
+        EntityManager.preventStorageCall(false);
+        ProvidedBlockReportTasksDataAccess da = (ProvidedBlockReportTasksDataAccess) HdfsStorageFactory
+                .getDataAccess(ProvidedBlockReportTasksDataAccess.class);
+        return da.getAllTasks();
+      }
+    };
+    return (List<ProvidedBlockReportTask>) handler.handle();
+  }
+
+  public List<ProvidedBlockReportTask> addNewBlockReportTasks(
+          final List<ProvidedBlockReportTask> tasks)
+          throws IOException {
+    HopsTransactionalRequestHandler handler = new HopsTransactionalRequestHandler(
+            HDFSOperationType.BR_ADD_TASKS) {
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        //take a lock on HdfsVariables.providedBlocksCheckStartTime to sync
+        HdfsStorageFactory.getConnector().writeLock();
+        LongVariable var = (LongVariable) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime);
+      }
+
+      @Override
+      public Object performTask() throws IOException {
+        EntityManager.preventStorageCall(false);
+        ProvidedBlockReportTasksDataAccess da = (ProvidedBlockReportTasksDataAccess) HdfsStorageFactory
+                .getDataAccess(ProvidedBlockReportTasksDataAccess.class);
+        da.addTasks(tasks);
+
+        if( LOG.isDebugEnabled()) {
+          for (ProvidedBlockReportTask task : tasks) {
+            LOG.debug("HopsFS-Cloud. BR Added new block report tasks " + task);
+          }
+        }
+
+        long startTime = System.currentTimeMillis();
+        Variables.updateVariable(
+                new LongVariable(
+                        Variable.Finder.providedBlocksCheckStartTime,
+                        startTime
+                ));
+        LOG.debug("HopsFS-Cloud. BR set start time to : " + startTime);
+
+        long counter = (long) Variables.getVariable(
+                Variable.Finder.providedBlockReportsCount).getValue();
+        Variables.updateVariable(
+                new LongVariable(
+                        Variable.Finder.providedBlockReportsCount,
+                        counter + 1
+                ));
+        LOG.debug("HopsFS-Cloud. BR set counter to : " + (counter + 1));
+
+        return null;
+      }
+    };
+    return (List<ProvidedBlockReportTask>) handler.handle();
+  }
+
+  public long getProvidedBlockReportsCount()
+          throws IOException {
+    HopsTransactionalRequestHandler handler = new HopsTransactionalRequestHandler(
+            HDFSOperationType.BR_COUNT_TASKS) {
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        //take a lock on HdfsVariables.providedBlocksCheckStartTime to sync
+        HdfsStorageFactory.getConnector().writeLock();
+        LongVariable var = (LongVariable) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime);
+      }
+
+      @Override
+      public Object performTask() throws IOException {
+        EntityManager.preventStorageCall(false);
+        long counter = (long) Variables.getVariable(
+                Variable.Finder.providedBlockReportsCount).getValue();
+        LOG.debug("HopsFS-Cloud. BR get counter : " + counter);
+
+        return counter;
+      }
+    };
+    return (Long) handler.handle();
+  }
+
+  public long getProvidedBlocksScanStartTime()
+          throws IOException {
+    HopsTransactionalRequestHandler handler = new HopsTransactionalRequestHandler(
+            HDFSOperationType.GET_PROVIDED_BLOCK_CHECK_START_TIME) {
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        //take a lock on HdfsVariables.providedBlocksCheckStartTime to sync
+        HdfsStorageFactory.getConnector().writeLock();
+        LongVariable var = (LongVariable) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime);
+      }
+
+      @Override
+      public Object performTask() throws IOException {
+        EntityManager.preventStorageCall(false);
+        long time = (long) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime).getValue();
+        LOG.debug("HopsFS-Cloud. BR get start time : " + time);
+        return time;
+      }
+    };
+    return (Long) handler.handle();
+  }
+
+  private static void setProvidedBlocksScanStartTime(final long newTime)
+          throws IOException {
+    HopsTransactionalRequestHandler handler = new HopsTransactionalRequestHandler(
+            HDFSOperationType.UPDATE_PROVIDED_BLOCK_CHECK_START_TIME) {
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        //take a lock on HdfsVariables.providedBlocksCheckStartTime to sync
+        HdfsStorageFactory.getConnector().writeLock();
+        LongVariable var = (LongVariable) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime);
+      }
+
+      @Override
+      public Object performTask() throws IOException {
+        EntityManager.preventStorageCall(false);
+        Variables.updateVariable(
+                new LongVariable(
+                        Variable.Finder.providedBlocksCheckStartTime,
+                        newTime
+                ));
+        LOG.debug("HopsFS-Cloud. BR set start time to: " + newTime);
+        return null;
+      }
+    };
+    handler.handle();
+  }
+
+  public static void deleteAllTask()
+          throws IOException {
+    HopsTransactionalRequestHandler handler = new HopsTransactionalRequestHandler(
+            HDFSOperationType.BR_DELETE_ALL_TASKS) {
+      @Override
+      public void acquireLock(TransactionLocks locks) throws IOException {
+        //take a lock on HdfsVariables.providedBlocksCheckStartTime to sync
+        HdfsStorageFactory.getConnector().writeLock();
+        LongVariable var = (LongVariable) Variables.getVariable(
+                Variable.Finder.providedBlocksCheckStartTime);
+      }
+
+      @Override
+      public Object performTask() throws IOException {
+        EntityManager.preventStorageCall(false);
+        ProvidedBlockReportTasksDataAccess da = (ProvidedBlockReportTasksDataAccess) HdfsStorageFactory
+                .getDataAccess(ProvidedBlockReportTasksDataAccess.class);
+        da.deleteAll();
+        LOG.debug("HopsFS-Cloud. BR Deleted all tasks");
+        return null;
+      }
+    };
+    handler.handle();
+  }
+  public boolean isBRInProgress() {
+    return isBRInProgress;
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/CloudHelper.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/CloudHelper.java
new file mode 100644
index 00000000000..192961600ad
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/CloudHelper.java
@@ -0,0 +1,98 @@
+/*
+ * Copyright (C) 2019 Logical Clocks AB.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs.server.common;
+
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.server.datanode.DatanodeUtil;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.regex.Pattern;
+
+public class CloudHelper {
+
+  public final static String BLOCKFILE_EXTENSION = ".data";
+  public final static String PREFIX_STR = "hopsfs-blocks-set-";
+
+  public static final Pattern blockFilePattern = Pattern.compile(
+          PREFIX_STR + "(-??\\d++)" + "/" + Block.BLOCK_FILE_PREFIX +
+                  "(-??\\d++)_(\\d++)\\" + BLOCKFILE_EXTENSION + "$");
+  public static final Pattern metaFilePattern = Pattern.compile(
+          PREFIX_STR + "(-??\\d++)" + "/" + Block.BLOCK_FILE_PREFIX +
+                  "(-??\\d++)_(\\d++)\\" + Block.METADATA_EXTENSION + "$");
+
+  //Block key prefix/blk_id_gs.data
+  public static String getBlockKey(final int prefixSize, Block b) {
+    return getPrefix(prefixSize, b.getBlockId()) + b.getBlockName() + "_" + b.getGenerationStamp()
+            + BLOCKFILE_EXTENSION;
+  }
+
+  //Meta key prefix/blk_id_gs.meta
+  public static String getMetaFileKey(final int prefixSize, Block b) {
+    String metaFileID = DatanodeUtil.getMetaName(b.getBlockName(), b.getGenerationStamp());
+    return getPrefix(prefixSize, b.getBlockId()) + metaFileID;
+  }
+
+  public static String getPrefix(final int prefixSize, long blockID) {
+    long prefixNo = blockID / prefixSize;
+    return PREFIX_STR + prefixNo + "/";
+  }
+
+  public static boolean isBlockFilename(String name) {
+    return blockFilePattern.matcher(name).matches();
+  }
+
+  public static boolean isMetaFilename(String name) {
+    return metaFilePattern.matcher(name).matches();
+  }
+
+  public static long extractBlockIDFromBlockName(String name){
+    List<Long> numbers = extractNumbers(name);
+    return numbers.get(numbers.size()-2);
+  }
+
+  public static long extractGSFromBlockName(String name){
+    List<Long> numbers = extractNumbers(name);
+    return numbers.get(numbers.size()-1);
+  }
+
+  public static long extractBlockIDFromMetaName(String name){
+    List<Long> numbers = extractNumbers(name);
+    return numbers.get(numbers.size()-2);
+  }
+
+  public static long extractGSFromMetaName(String name){
+    List<Long> numbers = extractNumbers(name);
+    return numbers.get(numbers.size()-1);
+  }
+  public static short extractBucketID(String name){
+    List<Long> bucketNums = CloudHelper.extractNumbers(name);
+    long bucketID = bucketNums.get(bucketNums.size() - 1);
+    assert bucketID <= Short.MAX_VALUE;
+    return (short)bucketID;
+  }
+
+  public static List<Long> extractNumbers(String key) {
+    String str = key.replaceAll("[^?0-9]+", " ");
+    String[] numbersStr = str.trim().split(" ");
+    List<Long> numbers = new ArrayList();
+    for (String num : numbersStr) {
+      numbers.add(Long.parseLong(num));
+    }
+    return numbers;
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
index ae0aded524a..39bd084b2a8 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
@@ -809,7 +809,9 @@ void receiveBlock(
           } else {
             // for isDatnode or TRANSFER_FINALIZED
             // Finalize the block.
+            datanode.data.preFinalize(block);
             datanode.data.finalizeBlock(block);
+            datanode.data.postFinalize(block);
           }
         }
         datanode.metrics.incrBlocksWritten();
@@ -1307,7 +1309,10 @@ private void finalizeBlock(long startTime) throws IOException {
         BlockReceiver.this.close();
         endTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;
         block.setNumBytes(replicaInfo.getNumBytes());
+
+        datanode.data.preFinalize(block);
         datanode.data.finalizeBlock(block);
+        datanode.data.postFinalize(block);
       }
 
       if (pinning) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
index a58a3c630bd..1e9e77fb62e 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hdfs.protocol.datatransfer.PacketHeader;
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.LengthInputStream;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudFsDatasetImpl;
 import org.apache.hadoop.hdfs.util.DataTransferThrottler;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.LongWritable;
@@ -254,18 +255,29 @@
         Preconditions.checkArgument(sendChecksum,
             "If verifying checksum, currently must also send it.");
       }
-      
-      final Replica replica;
+
+      Replica replica = null;
       final long replicaVisibleLength;
-      synchronized (datanode.data) {
-        if(block.getBlockId()<0){
-          LOG.debug("Suffed Inode: Reading Phantom data block.");
-          replica = new FinalizedReplica(block.getBlockId(), block.getNumBytes(), block.getGenerationStamp(), null, null);
-        } else {
+      if (block.getBlockId() < 0) {
+        // No need to take a lock as the block is not stored on this datanode
+        replica = new FinalizedReplica(block.getBlockId(), block.getNumBytes(),
+                block.getGenerationStamp(), block.getCloudBucketID(), null, null);
+      } else if (block.isProvidedBlock()) {
+        //finalized provided blocks are stored in the Cloud. Datanodes only cache these blocks.
+        //However, non finalized provided blocks are stored on datanodes during modification.
+        if (((CloudFsDatasetImpl) datanode.data).isProvideBlockFinalized(block)) {
+          //this is get the information from the cloud
+          replica = getReplica(block, datanode);
+        } // else read with lock
+      }
+
+      if (replica == null) {
+        synchronized (datanode.data) {
           replica = getReplica(block, datanode);
         }
-        replicaVisibleLength = replica.getVisibleLength();
       }
+      replicaVisibleLength = replica.getVisibleLength();
+
       // if there is a write in progress
       ChunkChecksum chunkChecksum = null;
       if (replica instanceof ReplicaBeingWritten) {
@@ -477,7 +489,7 @@ public void close() throws IOException {
   private static Replica getReplica(ExtendedBlock block, DataNode datanode)
       throws ReplicaNotFoundException {
     Replica replica =
-        datanode.data.getReplica(block.getBlockPoolId(), block.getBlockId());
+        datanode.data.getReplica(block);
     if (replica == null) {
       throw new ReplicaNotFoundException(block);
     }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
index 4408bde4deb..51777fcec73 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -75,15 +75,7 @@
 import org.apache.hadoop.hdfs.HdfsConfiguration;
 import org.apache.hadoop.hdfs.client.BlockReportOptions;
 import org.apache.hadoop.hdfs.client.HdfsClientConfigKeys;
-import org.apache.hadoop.hdfs.protocol.Block;
-import org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo;
-import org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol;
-import org.apache.hadoop.hdfs.protocol.DatanodeID;
-import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
-import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
-import org.apache.hadoop.hdfs.protocol.HdfsBlocksMetadata;
-import org.apache.hadoop.hdfs.protocol.HdfsConstants;
-import org.apache.hadoop.hdfs.protocol.RecoveryInProgressException;
+import org.apache.hadoop.hdfs.protocol.*;
 import org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage;
 import org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol;
 import org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair;
@@ -117,6 +109,7 @@
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi;
 import org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics;
+import org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException;
 import org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer;
 import org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand.RecoveringBlock;
 import org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol;
@@ -194,7 +187,6 @@
 
 import org.apache.hadoop.hdfs.net.DomainPeerServer;
 import org.apache.hadoop.hdfs.net.TcpPeerServer;
-import org.apache.hadoop.hdfs.protocol.DatanodeLocalInfo;
 import org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory;
 import org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient;
 import org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer;
@@ -205,6 +197,8 @@
 import org.apache.hadoop.net.unix.DomainSocket;
 import org.apache.hadoop.tracing.TraceUtils;
 import org.apache.hadoop.tracing.TracerConfigurationManager;
+
+import static org.apache.hadoop.hdfs.DFSConfigKeys.*;
 import static org.apache.hadoop.util.ExitUtil.terminate;
 import org.apache.hadoop.util.JvmPauseMonitor;
 import org.apache.htrace.core.Tracer;
@@ -2952,6 +2946,7 @@ void syncBlock(RecoveringBlock rBlock, List<BlockRecord> syncList)
     boolean isTruncateRecovery = rBlock.getNewBlock() != null;
     long blockId = (isTruncateRecovery) ?
         rBlock.getNewBlock().getBlockId() : block.getBlockId();
+    short cloudBucketID = block.getCloudBucketID();
 
     if (LOG.isDebugEnabled()) {
       LOG.debug("block=" + block + ", (length=" + block.getNumBytes() +
@@ -2962,7 +2957,7 @@ void syncBlock(RecoveringBlock rBlock, List<BlockRecord> syncList)
     // or their replicas have 0 length.
     // The block can be deleted.
     if (syncList.isEmpty()) {
-      nn.commitBlockSynchronization(block, recoveryId, 0, true, true,
+      commitBlockSyncWithRetry(nn, block, recoveryId, 0, true, true,
           DatanodeID.EMPTY_ARRAY, null);
       return;
     }
@@ -2989,7 +2984,7 @@ void syncBlock(RecoveringBlock rBlock, List<BlockRecord> syncList)
     // and the new block size
     List<BlockRecord> participatingList = new ArrayList<>();
     final ExtendedBlock newBlock = new ExtendedBlock(bpid, blockId,
-        -1, recoveryId);
+        -1, recoveryId, cloudBucketID );
     switch (bestState) {
       case FINALIZED:
         assert finalizedLength > 0 : "finalizedLength is not positive";
@@ -3056,10 +3051,66 @@ void syncBlock(RecoveringBlock rBlock, List<BlockRecord> syncList)
       datanodes[i] = r.id;
       storages[i] = r.storageID;
     }
-    nn.commitBlockSynchronization(block, newBlock.getGenerationStamp(),
+    commitBlockSyncWithRetry(nn, block, newBlock.getGenerationStamp(),
         newBlock.getNumBytes(), true, false, datanodes, storages);
   }
-  
+
+  protected void commitBlockSyncWithRetry(DatanodeProtocolClientSideTranslatorPB nn,
+          ExtendedBlock block, long newgenerationstamp, long newlength, boolean closeFile,
+      boolean deleteblock, DatanodeID[] newtargets, String[] newtargetstorages)
+          throws IOException {
+
+    int retries = conf.getInt(
+            DFS_CLIENT_BLOCK_WRITE_LOCATEFOLLOWINGBLOCK_RETRIES_KEY,
+            DFS_CLIENT_BLOCK_WRITE_LOCATEFOLLOWINGBLOCK_RETRIES_DEFAULT);
+
+    long sleeptime = conf.getInt(
+            DFS_CLIENT_BLOCK_WRITE_LOCATEFOLLOWINGBLOCK_INITIAL_DELAY_KEY,
+            DFS_CLIENT_BLOCK_WRITE_LOCATEFOLLOWINGBLOCK_INITIAL_DELAY_DEFAULT);
+
+      long localstart = Time.monotonicNow();
+      while (true) {
+        try {
+          nn.commitBlockSynchronization(block, newgenerationstamp,
+                  newlength, closeFile, deleteblock, newtargets, newtargetstorages);
+          return;
+        } catch (RemoteException e) {
+          IOException ue =
+                  e.unwrapRemoteException(FileNotFoundException.class,
+                          AccessControlException.class,
+                          NSQuotaExceededException.class,
+                          DSQuotaExceededException.class,
+                          UnresolvedPathException.class);
+          if (ue != e) {
+            throw ue; // no need to retry these exceptions
+          }
+
+          if (NotReplicatedYetException.class.getName().
+                  equals(e.getClassName())) {
+            if (retries == 0) {
+              throw e;
+            } else {
+              --retries;
+              LOG.info("Exception while syncing a block", e);
+              long elapsed = Time.monotonicNow() - localstart;
+              if (elapsed > 5000) {
+                LOG.info("Waiting for block sycn for " + (elapsed / 1000) + " seconds");
+              }
+              try {
+                LOG.warn("NotReplicatedYetException sleeping " + block + " retries left " + retries);
+                Thread.sleep(sleeptime);
+                sleeptime *= 2;
+              } catch (InterruptedException ie) {
+                LOG.warn("Caught exception ", ie);
+              }
+            }
+          } else {
+            throw e;
+          }
+        }
+      }
+  }
+
   private static void logRecoverBlock(String who, RecoveringBlock rb) {
     ExtendedBlock block = rb.getBlock();
     DatanodeInfo[] targets = rb.getLocations();
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java
index a837cf259ac..862131f9f40 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java
@@ -89,6 +89,7 @@
   public final static String STORAGE_DIR_RBW = "rbw";
   public final static String STORAGE_DIR_FINALIZED = "finalized";
   public final static String STORAGE_DIR_TMP = "tmp";
+  public final static String STORAGE_DIR_CACHE = "cache";
 
   /**
    * Set of bpids for which 'trash' is currently enabled.
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java
index 15996d97a8a..13a4b561940 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java
@@ -43,9 +43,9 @@
    * @param dir
    *     directory path where block and meta files are located
    */
-  public FinalizedReplica(long blockId, long len, long genStamp,
+  public FinalizedReplica(long blockId, long len, long genStamp, short cloudBucketID,
       FsVolumeSpi vol, File dir) {
-    super(blockId, len, genStamp, vol, dir);
+    super(blockId, len, genStamp, cloudBucketID, vol, dir);
   }
   
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaBeingWritten.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaBeingWritten.java
index a81293ca531..610d3112c5e 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaBeingWritten.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaBeingWritten.java
@@ -38,9 +38,9 @@
    * @param bytesToReserve disk space to reserve for this replica, based on
    *                       the estimated maximum block length.
    */
-  public ReplicaBeingWritten(long blockId, long genStamp, 
-        FsVolumeSpi vol, File dir, long bytesToReserve) {
-    super(blockId, genStamp, vol, dir, bytesToReserve);
+  public ReplicaBeingWritten(long blockId, long genStamp,
+        short cloudBucketID, FsVolumeSpi vol, File dir, long bytesToReserve) {
+    super(blockId, genStamp, cloudBucketID, vol, dir, bytesToReserve);
   }
   
   /**
@@ -72,8 +72,9 @@ public ReplicaBeingWritten(Block block, FsVolumeSpi vol, File dir,
    *                       the estimated maximum block length.
    */
   public ReplicaBeingWritten(long blockId, long len, long genStamp,
-      FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve) {
-    super(blockId, len, genStamp, vol, dir, writer, bytesToReserve);
+                             short cloudBucketID, FsVolumeSpi vol, File dir,
+                             Thread writer, long bytesToReserve) {
+    super(blockId, len, genStamp, cloudBucketID, vol, dir, writer, bytesToReserve);
   }
 
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java
index c9585034b4c..819f9799410 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java
@@ -61,9 +61,9 @@
    * @param bytesToReserve disk space to reserve for this replica, based on
    *                       the estimated maximum block length.
    */
-  public ReplicaInPipeline(long blockId, long genStamp, 
+  public ReplicaInPipeline(long blockId, long genStamp, short cloudBucketID,
         FsVolumeSpi vol, File dir, long bytesToReserve) {
-    this(blockId, 0L, genStamp, vol, dir, Thread.currentThread(), bytesToReserve);
+    this(blockId, 0L, genStamp, cloudBucketID, vol, dir, Thread.currentThread(), bytesToReserve);
   }
 
   /**
@@ -81,7 +81,7 @@ public ReplicaInPipeline(long blockId, long genStamp,
   ReplicaInPipeline(Block block, 
       FsVolumeSpi vol, File dir, Thread writer) {
     this( block.getBlockId(), block.getNumBytes(), block.getGenerationStamp(),
-        vol, dir, writer, 0L);
+        block.getCloudBucketID(), vol, dir, writer, 0L);
   }
 
   /**
@@ -95,9 +95,9 @@ public ReplicaInPipeline(long blockId, long genStamp,
    * @param bytesToReserve disk space to reserve for this replica, based on
    *                       the estimated maximum block length.
    */
-  ReplicaInPipeline(long blockId, long len, long genStamp,
+  ReplicaInPipeline(long blockId, long len, long genStamp, short cloudBucketID,
       FsVolumeSpi vol, File dir, Thread writer, long bytesToReserve) {
-    super( blockId, len, genStamp, vol, dir);
+    super( blockId, len, genStamp, cloudBucketID, vol, dir);
     this.bytesAcked = len;
     this.bytesOnDisk = len;
     this.writer = writer;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInfo.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInfo.java
index 74de410fcac..d17c5b35c46 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInfo.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInfo.java
@@ -74,8 +74,8 @@
    * @param dir
    *     directory path where block and meta files are located
    */
-  ReplicaInfo(long blockId, long genStamp, FsVolumeSpi vol, File dir) {
-    this(blockId, 0L, genStamp, vol, dir);
+  ReplicaInfo(long blockId, long genStamp, short cloudBucketID, FsVolumeSpi vol, File dir) {
+    this(blockId, 0L, genStamp, cloudBucketID, vol, dir);
   }
   
   /**
@@ -90,7 +90,7 @@
    */
   ReplicaInfo(Block block, FsVolumeSpi vol, File dir) {
     this(block.getBlockId(), block.getNumBytes(), block.getGenerationStamp(),
-        vol, dir);
+        block.getCloudBucketID(), vol, dir);
   }
   
   /**
@@ -107,9 +107,9 @@
    * @param dir
    *     directory path where block and meta files are located
    */
-  ReplicaInfo(long blockId, long len, long genStamp, FsVolumeSpi vol,
-      File dir) {
-    super(blockId, len, genStamp);
+  ReplicaInfo(long blockId, long len, long genStamp, short cloudBucketID,
+              FsVolumeSpi vol, File dir) {
+    super(blockId, len, genStamp, cloudBucketID);
     this.volume = vol;
     setDirInternal(dir);
   }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java
index 4d3b818d1b8..783e394195b 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaUnderRecovery.java
@@ -38,7 +38,8 @@
 
   public ReplicaUnderRecovery(ReplicaInfo replica, long recoveryId) {
     super(replica.getBlockId(), replica.getNumBytes(),
-        replica.getGenerationStamp(), replica.getVolume(), replica.getDir());
+        replica.getGenerationStamp(), replica.getCloudBucketID(),
+            replica.getVolume(), replica.getDir());
     if (replica.getState() != ReplicaState.FINALIZED &&
         replica.getState() != ReplicaState.RBW &&
         replica.getState() != ReplicaState.RWR) {
@@ -167,6 +168,6 @@ public String toString() {
   public ReplicaRecoveryInfo createInfo() {
     return new ReplicaRecoveryInfo(original.getBlockId(),
         original.getBytesOnDisk(), original.getGenerationStamp(),
-        original.getState());
+        original.getCloudBucketID(), original.getState());
   }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaWaitingToBeRecovered.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaWaitingToBeRecovered.java
index 23432d4b46c..92cfd8fc379 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaWaitingToBeRecovered.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ReplicaWaitingToBeRecovered.java
@@ -50,8 +50,8 @@
    *     directory path where block and meta files are located
    */
   public ReplicaWaitingToBeRecovered(long blockId, long len, long genStamp,
-      FsVolumeSpi vol, File dir) {
-    super(blockId, len, genStamp, vol, dir);
+      short cloudBucketID, FsVolumeSpi vol, File dir) {
+    super(blockId, len, genStamp, cloudBucketID, vol, dir);
   }
   
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/CloudPersistenceProvider.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/CloudPersistenceProvider.java
new file mode 100644
index 00000000000..dcc0610a6a1
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/CloudPersistenceProvider.java
@@ -0,0 +1,56 @@
+package org.apache.hadoop.hdfs.server.datanode.fsdataset;
+
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.CloudBlock;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.List;
+import java.util.Map;
+
+public interface CloudPersistenceProvider {
+  /*
+  deletes all the bucket belonging to the user.
+  This is only used for testing.
+   */
+  public void deleteAllBuckets(String prefix);
+
+  /*
+  Deletes all the buckets that are used by HopsFS
+   */
+  public void format();
+
+  /*
+  Check that all the buckets needed exist
+  throws runtime exception if the buckets dont exists or not writable.
+   */
+  public void checkAllBuckets();
+
+  public String getBucketDNSID(int ID);
+
+  public int getPrefixSize();
+
+  public void uploadObject(short bucketID, String objectID, File object,
+                           Map<String, String> metadata) throws IOException;
+
+  public boolean objectExists(short bucketID, String objectID)
+          throws IOException;
+
+  public Map<String, String> getUserMetaData(short bucketID, String objectID)
+          throws IOException;
+
+  public long getObjectSize(short bucketID, String objectID)
+          throws IOException;
+
+  public void downloadObject(short bucketID, String objectID, File path)
+          throws IOException;
+
+  public Map<Long, CloudBlock> getAll(String prefix) throws IOException;
+
+  public void deleteObject(short bucketID, String objectID) throws IOException;
+
+  public void renameObject(short srcBucket, short dstBucket, String srcKey,
+                           String dstKey) throws IOException ;
+  public void shutdown();
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
index 2c3312d904f..921266c8c7f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java
@@ -42,6 +42,7 @@
 import org.apache.hadoop.hdfs.server.datanode.StorageLocation;
 import org.apache.hadoop.hdfs.server.datanode.UnexpectedReplicaStateException;
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.HopsFsDatasetFactory;
 import org.apache.hadoop.hdfs.server.datanode.metrics.FSDatasetMBean;
 import org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand.RecoveringBlock;
 import org.apache.hadoop.hdfs.server.protocol.BlockReport;
@@ -75,10 +76,17 @@
      */
     public static Factory<?> getFactory(Configuration conf) {
       @SuppressWarnings("rawtypes")
-      final Class<? extends Factory> clazz =
-          conf.getClass(DFSConfigKeys.DFS_DATANODE_FSDATASET_FACTORY_KEY,
-              FsDatasetFactory.class, Factory.class);
-      return ReflectionUtils.newInstance(clazz, conf);
+
+      boolean cloud = conf.getBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE,
+              DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE_DEFAULT);
+      if (cloud) {
+        return ReflectionUtils.newInstance(HopsFsDatasetFactory.class, conf);
+      } else {
+        final Class<? extends Factory> clazz =
+                conf.getClass(DFSConfigKeys.DFS_DATANODE_FSDATASET_FACTORY_KEY,
+                        FsDatasetFactory.class, Factory.class);
+        return ReflectionUtils.newInstance(clazz, conf);
+      }
     }
 
     /**
@@ -181,11 +189,11 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)
    * Get reference to the replica meta info in the replicasMap.
    * To be called from methods that are synchronized on {@link FSDataset}
    *
-   * @param blockId
+   * @param block
    * @return replica from the replicas map
    */
   @Deprecated
-  public Replica getReplica(String bpid, long blockId);
+  public Replica getReplica(ExtendedBlock block);
 
   /**
    * @return replica meta information
@@ -320,6 +328,14 @@ public ReplicaHandler recoverAppend(
   public String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen)
       throws IOException;
   
+  /**
+   * Pre-Finalize stage where the provided block is uploaded to the cloud
+   *
+   * @param b
+   * @throws IOException
+   */
+  public void preFinalize(ExtendedBlock b) throws IOException;
+
   /**
    * Finalizes the block previously opened for writing using writeToBlock.
    * The block size is what is in the parameter b and it must match the amount
@@ -333,6 +349,14 @@ public String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen)
    */
   public void finalizeBlock(ExtendedBlock b) throws IOException;
 
+  /**
+   * Post-Finalize stage
+   *
+   * @param b
+   * @throws IOException
+   */
+  public void postFinalize(ExtendedBlock b) throws IOException;
+
   /**
    * Unfinalizes the block previously opened for writing using writeToBlock.
    * The temporary file associated with this block is deleted.
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
index aa485a0ad61..a9c8c2985ca 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.io.Files;
 import java.io.FileOutputStream;
 import java.io.OutputStreamWriter;
@@ -24,6 +25,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.DU;
 import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.StorageType;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
 import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;
@@ -64,7 +66,8 @@
  * <p/>
  * This class is synchronized by {@link FsVolumeImpl}.
  */
-class BlockPoolSlice {
+@VisibleForTesting
+public class BlockPoolSlice {
   static final Log LOG = LogFactory.getLog(BlockPoolSlice.class);
   
   private final String bpid;
@@ -75,6 +78,7 @@
   private final File finalizedDir;
   private final File rbwDir; // directory store RBW replica
   private final File tmpDir; // directory store Temporary replica
+  private final File cacheDir; // directory store Temporary replica
   private static String DU_CACHE_FILE = "dfsUsed";
   private volatile boolean dfsUsedSaved = false;
   private static final int SHUTDOWN_HOOK_PRIORITY = 30;
@@ -84,6 +88,7 @@
   // TODO:FEDERATION scalability issue - a thread per DU is needed
   private final DU dfsUsage;
 
+  private ProvidedBlocksCacheCleaner providedBlocksCache = null;
   /**
    * Create a blook pool slice
    *
@@ -118,6 +123,7 @@
     if (tmpDir.exists()) {
       FileUtil.fullyDelete(tmpDir);
     }
+
     this.rbwDir = new File(currentDir, DataStorage.STORAGE_DIR_RBW);
     final boolean supportAppends =
         conf.getBoolean(DFSConfigKeys.DFS_SUPPORT_APPEND_KEY,
@@ -135,6 +141,15 @@
         throw new IOException("Mkdirs failed to create " + tmpDir.toString());
       }
     }
+
+    // Setting up cache Dir
+    this.cacheDir = new File(bpDir, DataStorage.STORAGE_DIR_CACHE);
+    if (!cacheDir.mkdirs()) {
+      if (!cacheDir.isDirectory()) {
+        throw new IOException("Mkdirs failed to create " + cacheDir.toString());
+      }
+    }
+
     // Use cached value initially if available. Or the following call will
     // block until the initial du command completes.
     this.dfsUsage = new DU(bpDir, conf, loadDfsUsed());
@@ -150,6 +165,23 @@ public void run() {
           }
         }
       }, SHUTDOWN_HOOK_PRIORITY);
+
+    //Start the cache monitor for CLOUD storages
+    if (volume.getStorageType() == StorageType.CLOUD) {
+      int threshold = conf.getInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_ACTIVATION_PRECENTAGE_KEY,
+                      DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_ACTIVATION_PRECENTAGE_DEFAULT);
+      long interval = conf.getLong(DFSConfigKeys.DFS_DN_CLOUD_CACHE_CHECK_INTERVAL_KEY,
+                      DFSConfigKeys.DFS_DN_CLOUD_CACHE_CHECK_INTERVAL_DEFAULT);
+      int deleteBatchSize = conf.getInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_BATCH_SIZE_KEY,
+                      DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_BATCH_SIZE_DEFAULT);
+      int waitBeforeDelete = conf.getInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_WAIT_KEY,
+                      DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_WAIT_DEFAULT);
+      providedBlocksCache = new ProvidedBlocksCacheCleaner(getCacheDir(), interval,
+              threshold, deleteBatchSize, waitBeforeDelete);
+      providedBlocksCache.start();
+    } else {
+      providedBlocksCache = null;
+    }
   }
 
   File getDirectory() {
@@ -168,6 +200,10 @@ File getTmpDir() {
     return tmpDir;
   }
 
+  File getCacheDir() {
+    return cacheDir;
+  }
+
   /** Run DU on local drives.  It must be synchronized from caller. */
   void decDfsUsed(long value) {
     dfsUsage.decDfsUsed(value);
@@ -322,10 +358,11 @@ private void addReplicaToReplicasMap(Block block, ReplicaMap volumeMap,
     ReplicaInfo newReplica = null;
     long blockId = block.getBlockId();
     long genStamp = block.getGenerationStamp();
+    short cloudBucketID = block.getCloudBucketID();
     if (isFinalized) {
       newReplica = new FinalizedReplica(blockId, 
-          block.getNumBytes(), genStamp, volume, DatanodeUtil
-          .idToBlockDir(finalizedDir, blockId));
+          block.getNumBytes(), genStamp, block.getCloudBucketID(), volume,
+          DatanodeUtil.idToBlockDir(finalizedDir, blockId));
     } else {
       File file = new File(rbwDir, block.getBlockName());
       boolean loadRwr = true;
@@ -341,7 +378,7 @@ private void addReplicaToReplicasMap(Block block, ReplicaMap volumeMap,
           // and don't reserve any more space for writes.
           newReplica = new ReplicaBeingWritten(blockId,
               validateIntegrityAndSetLength(file, genStamp), 
-              genStamp, volume, file.getParentFile(), null, 0);
+              genStamp, cloudBucketID, volume, file.getParentFile(), null, 0);
           loadRwr = false;
         }
         sc.close();
@@ -360,7 +397,7 @@ private void addReplicaToReplicasMap(Block block, ReplicaMap volumeMap,
       if (loadRwr) {
         newReplica = new ReplicaWaitingToBeRecovered(blockId,
             validateIntegrityAndSetLength(file, genStamp),
-            genStamp, volume, file.getParentFile());
+            genStamp, cloudBucketID, volume, file.getParentFile());
       }
     }
 
@@ -373,7 +410,6 @@ private void addReplicaToReplicasMap(Block block, ReplicaMap volumeMap,
     }
   }
   
-
   /**
    * Add replicas under the given directory to the volume map
    *
@@ -407,7 +443,7 @@ void addToReplicasMap(ReplicaMap volumeMap, File dir, boolean isFinalized
       long genStamp = FsDatasetUtil.getGenerationStampFromFile(
           files, file);
       long blockId = Block.filename2id(file.getName());
-      Block block = new Block(blockId, file.length(), genStamp); 
+      Block block = new Block(blockId, file.length(), genStamp, Block.NON_EXISTING_BUCKET_ID );
       addReplicaToReplicasMap(block, volumeMap, 
           isFinalized);
     }
@@ -508,6 +544,9 @@ void shutdown(BlockReport blocksListToPersist) {
     saveDfsUsed();
     dfsUsedSaved = true;
     dfsUsage.shutdown();
+    if (providedBlocksCache != null) {
+      providedBlocksCache.shutdown();
+    }
   }
 
   private boolean readReplicasFromCache(ReplicaMap volumeMap) {
@@ -619,4 +658,21 @@ private void saveReplicas(BlockReport blocksListToPersist) {
       }
     }
   }
+
+  public void fileAccessed(File f){
+    if(providedBlocksCache != null) {
+      providedBlocksCache.fileAccessed(f.getAbsolutePath());
+    }
+  }
+
+  public void fileDeleted(File f){
+    if(providedBlocksCache != null) {
+      providedBlocksCache.fileDeleted(f.getAbsolutePath());
+    }
+  }
+
+  public ProvidedBlocksCacheCleaner getProvidedBlocksCacheCleaner(){
+    return providedBlocksCache;
+  }
+
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockingThreadPoolExecutorService.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockingThreadPoolExecutorService.java
new file mode 100644
index 00000000000..d12641e6cb0
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockingThreadPoolExecutorService.java
@@ -0,0 +1,171 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.RejectedExecutionHandler;
+import java.util.concurrent.ThreadFactory;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.util.concurrent.MoreExecutors;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+
+/**
+ * This ExecutorService blocks the submission of new tasks when its queue is
+ * already full by using a semaphore. Task submissions require permits, task
+ * completions release permits.
+ * <p>
+ * This is inspired by <a href="https://github.com/apache/incubator-s4/blob/master/subprojects/s4-comm/src/main/java/org/apache/s4/comm/staging/BlockingThreadPoolExecutorService.java">
+ * this s4 threadpool</a>
+ */
+@InterfaceAudience.Private
+public final class BlockingThreadPoolExecutorService
+        extends SemaphoredDelegatingExecutor {
+
+  private static final Logger LOG = LoggerFactory
+          .getLogger(BlockingThreadPoolExecutorService.class);
+
+  private static final AtomicInteger POOLNUMBER = new AtomicInteger(1);
+
+  private final ThreadPoolExecutor eventProcessingExecutor;
+
+  /**
+   * Returns a {@link java.util.concurrent.ThreadFactory} that names each
+   * created thread uniquely,
+   * with a common prefix.
+   *
+   * @param prefix The prefix of every created Thread's name
+   * @return a {@link java.util.concurrent.ThreadFactory} that names threads
+   */
+  static ThreadFactory getNamedThreadFactory(final String prefix) {
+    SecurityManager s = System.getSecurityManager();
+    final ThreadGroup threadGroup = (s != null) ? s.getThreadGroup() :
+            Thread.currentThread().getThreadGroup();
+
+    return new ThreadFactory() {
+      private final AtomicInteger threadNumber = new AtomicInteger(1);
+      private final int poolNum = POOLNUMBER.getAndIncrement();
+      private final ThreadGroup group = threadGroup;
+
+      @Override
+      public Thread newThread(Runnable r) {
+        final String name =
+                prefix + "-pool" + poolNum + "-t" + threadNumber.getAndIncrement();
+        return new Thread(group, r, name);
+      }
+    };
+  }
+
+  /**
+   * Get a named {@link ThreadFactory} that just builds daemon threads.
+   *
+   * @param prefix name prefix for all threads created from the factory
+   * @return a thread factory that creates named, daemon threads with
+   * the supplied exception handler and normal priority
+   */
+  public static ThreadFactory newDaemonThreadFactory(final String prefix) {
+    final ThreadFactory namedFactory = getNamedThreadFactory(prefix);
+    return new ThreadFactory() {
+      @Override
+      public Thread newThread(Runnable r) {
+        Thread t = namedFactory.newThread(r);
+        if (!t.isDaemon()) {
+          t.setDaemon(true);
+        }
+        if (t.getPriority() != Thread.NORM_PRIORITY) {
+          t.setPriority(Thread.NORM_PRIORITY);
+        }
+        return t;
+      }
+
+    };
+  }
+
+  private BlockingThreadPoolExecutorService(int permitCount,
+                                            ThreadPoolExecutor eventProcessingExecutor) {
+    super(MoreExecutors.listeningDecorator(eventProcessingExecutor),
+            permitCount, false);
+    this.eventProcessingExecutor = eventProcessingExecutor;
+  }
+
+  /**
+   * A thread pool that that blocks clients submitting additional tasks if
+   * there are already {@code activeTasks} running threads and {@code
+   * waitingTasks} tasks waiting in its queue.
+   *
+   * @param activeTasks maximum number of active tasks
+   * @param waitingTasks maximum number of waiting tasks
+   * @param keepAliveTime time until threads are cleaned up in {@code unit}
+   * @param unit time unit
+   * @param prefixName prefix of name for threads
+   */
+  public static BlockingThreadPoolExecutorService newInstance(
+          int activeTasks,
+          int waitingTasks,
+          long keepAliveTime, TimeUnit unit,
+          String prefixName) {
+
+    /* Although we generally only expect up to waitingTasks tasks in the
+    queue, we need to be able to buffer all tasks in case dequeueing is
+    slower than enqueueing. */
+    final BlockingQueue<Runnable> workQueue =
+            new LinkedBlockingQueue<>(waitingTasks + activeTasks);
+    ThreadPoolExecutor eventProcessingExecutor =
+            new ThreadPoolExecutor(activeTasks, activeTasks, keepAliveTime, unit,
+                    workQueue, newDaemonThreadFactory(prefixName),
+                    new RejectedExecutionHandler() {
+                      @Override
+                      public void rejectedExecution(Runnable r,
+                                                    ThreadPoolExecutor executor) {
+                        // This is not expected to happen.
+                        LOG.error("Could not submit task to executor {}",
+                                executor.toString());
+                      }
+                    });
+    eventProcessingExecutor.allowCoreThreadTimeOut(true);
+    return new BlockingThreadPoolExecutorService(waitingTasks + activeTasks,
+            eventProcessingExecutor);
+  }
+
+  /**
+   * Get the actual number of active threads.
+   * @return the active thread count
+   */
+  int getActiveCount() {
+    return eventProcessingExecutor.getActiveCount();
+  }
+
+  @Override
+  public String toString() {
+    final StringBuilder sb = new StringBuilder(
+            "BlockingThreadPoolExecutorService{");
+    sb.append(super.toString());
+    sb.append(", activeCount=").append(getActiveCount());
+    sb.append('}');
+    return sb.toString();
+  }
+}
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudFsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudFsDatasetImpl.java
new file mode 100644
index 00000000000..3d07bdeafb9
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudFsDatasetImpl.java
@@ -0,0 +1,644 @@
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import com.google.common.annotations.VisibleForTesting;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CloudProvider;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.StorageType;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
+import org.apache.hadoop.hdfs.server.common.CloudHelper;
+import org.apache.hadoop.hdfs.server.datanode.*;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+
+import java.io.*;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.LengthInputStream;
+import org.apache.hadoop.hdfs.server.protocol.BlockReport;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeStorage;
+import org.apache.hadoop.util.DataChecksum;
+
+public class CloudFsDatasetImpl extends FsDatasetImpl {
+  /**
+   * An FSDataset has a directory where it loads its data files.
+   *
+   * @param datanode
+   * @param storage
+   * @param conf
+   */
+  public static final String GEN_STAMP = "GEN_STAMP";
+  public static final String OBJECT_SIZE = "OBJECT_SIZE";
+
+  static final Log LOG = LogFactory.getLog(CloudFsDatasetImpl.class);
+  private CloudPersistenceProvider cloud;
+  private final boolean bypassCache;
+  private final int prefixSize;
+
+  CloudFsDatasetImpl(DataNode datanode, DataStorage storage,
+                     Configuration conf) throws IOException {
+    super(datanode, storage, conf);
+    bypassCache = conf.getBoolean(DFSConfigKeys.DFS_DN_CLOUD_BYPASS_CACHE_KEY,
+            DFSConfigKeys.DFS_DN_CLOUD_BYPASS_CACHE_DEFAULT);
+    prefixSize = conf.getInt(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY,
+            DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_DEFAULT);
+    cloud = CloudPersistenceProviderFactory.getCloudClient(conf);
+    cloud.checkAllBuckets();
+  }
+
+  @Override
+  public void preFinalize(ExtendedBlock b) throws IOException {
+    if (!b.isProvidedBlock()) {
+      super.preFinalize(b);
+    } else {
+      // upload to cloud
+      preFinalizeInternal(b);
+    }
+  }
+
+  public void preFinalizeInternal(ExtendedBlock b) throws IOException {
+    LOG.debug("HopsFS-Cloud. Prefinalize Stage. Uploading... Block: " + b.getLocalBlock());
+
+    ReplicaInfo replicaInfo = getReplicaInfo(b);
+    File blockFile = replicaInfo.getBlockFile();
+    File metaFile = replicaInfo.getMetaFile();
+    String blockFileKey = CloudHelper.getBlockKey(prefixSize, b.getLocalBlock());
+    String metaFileKey = CloudHelper.getMetaFileKey(prefixSize, b.getLocalBlock());
+
+    if (!cloud.objectExists(b.getCloudBucketID(), blockFileKey)
+            && !cloud.objectExists(b.getCloudBucketID(), metaFileKey)) {
+      cloud.uploadObject(b.getCloudBucketID(), blockFileKey, blockFile,
+              getBlockFileMetadata(b.getLocalBlock()));
+      cloud.uploadObject(b.getCloudBucketID(), metaFileKey, metaFile,
+              getMetaMetadata(b.getLocalBlock()));
+    } else {
+      LOG.error("HopsFS-Cloud. Block: " + b + " alreay exists.");
+      throw new IOException("Block: " + b + " alreay exists.");
+    }
+  }
+
+  @Override
+  public synchronized void finalizeBlock(ExtendedBlock b) throws IOException {
+    if (!b.isProvidedBlock()) {
+      super.finalizeBlock(b);
+    } else {
+      finalizeBlockInternal(b);
+    }
+  }
+
+  private synchronized void finalizeBlockInternal(ExtendedBlock b) throws IOException {
+    LOG.debug("HopsFS-Cloud. Finalizing bloclk. Block: " + b.getLocalBlock());
+    if (Thread.interrupted()) {
+      // Don't allow data modifications from interrupted threads
+      throw new IOException("Cannot finalize block from Interrupted Thread");
+    }
+
+    ReplicaInfo replicaInfo = getReplicaInfo(b);
+    File blockFile = replicaInfo.getBlockFile();
+    File metaFile = replicaInfo.getMetaFile();
+    long dfsBytes = blockFile.length() + metaFile.length();
+
+    // release rbw space
+    FsVolumeImpl v = (FsVolumeImpl) replicaInfo.getVolume();
+    v.releaseReservedSpace(replicaInfo.getBytesReserved());
+    v.decDfsUsed(b.getBlockPoolId(), dfsBytes);
+
+    // remove from volumeMap, so we can get it from s3 instead
+    volumeMap.remove(b.getBlockPoolId(), replicaInfo.getBlockId());
+
+    if (bypassCache) {
+      blockFile.delete();
+      metaFile.delete();
+    } else {
+      //move the blocks to the cache
+      FsVolumeImpl cloudVol = getCloudVolume();
+      File cDir = cloudVol.getCacheDir(b.getBlockPoolId());
+
+      File movedBlock = new File(cDir, CloudHelper.getBlockKey(prefixSize, b.getLocalBlock()));
+      File movedBlockParent = new File(movedBlock.getParent());
+      if (!movedBlockParent.exists()) {
+        movedBlockParent.mkdir();
+      }
+
+      if (!blockFile.renameTo(movedBlock)) {
+        LOG.warn("HopsFS-Cloud. Unable to move finalized block to cache. src: "
+                + blockFile.getAbsolutePath() + " dst: " + movedBlock.getAbsolutePath());
+        blockFile.delete();
+      } else {
+        providedBlocksCacheUpdateTS(b.getBlockPoolId(), movedBlock);
+        LOG.debug("HopsFS-Cloud. Moved " + movedBlock.getName() + " to cache. path " + movedBlock);
+      }
+
+      File movedMetaFile = new File(cDir, CloudHelper.getMetaFileKey(prefixSize,
+              b.getLocalBlock()));
+      File movedMetaFileParent = new File(movedMetaFile.getParent());
+      if (!movedMetaFileParent.exists()) {
+        movedMetaFileParent.mkdir();
+      }
+
+      if (!metaFile.renameTo(movedMetaFile)) {
+        LOG.warn("HopsFS-Cloud. Unable to move finalized block meta file to cache. src: "
+                + blockFile.getAbsolutePath() + " dst: " + movedBlock.getAbsolutePath());
+        metaFile.delete();
+      } else {
+        providedBlocksCacheUpdateTS(b.getBlockPoolId(), movedMetaFile);
+        LOG.debug("HopsFS-Cloud. Moved " + movedMetaFile.getName() + " to cache. path : " + movedMetaFile);
+      }
+    }
+  }
+
+  @Override
+  public void postFinalize(ExtendedBlock b) throws IOException {
+  }
+
+  @Override // FsDatasetSpi
+  public InputStream getBlockInputStream(ExtendedBlock b, long seekOffset)
+          throws IOException {
+
+    if (!b.isProvidedBlock()) {
+      return super.getBlockInputStream(b, seekOffset);
+    } else {
+
+      LOG.debug("HopsFS-Cloud. Get block inputstream " + b.getLocalBlock());
+      if (b.isProvidedBlock() && volumeMap.get(b.getBlockPoolId(), b.getBlockId()) != null) {
+        return super.getBlockInputStream(b, seekOffset);
+      } else {
+        FsVolumeImpl cloudVolume = getCloudVolume();
+        File localBlkCopy = new File(cloudVolume.getCacheDir(b.getBlockPoolId()),
+                CloudHelper.getBlockKey(prefixSize, b.getLocalBlock()));
+        String blockFileKey = CloudHelper.getBlockKey(prefixSize, b.getLocalBlock());
+
+        return getInputStreamInternal(b.getCloudBucketID(), blockFileKey,
+                localBlkCopy, b.getBlockPoolId(), seekOffset);
+      }
+    }
+  }
+
+  @Override // FsDatasetSpi
+  public LengthInputStream getMetaDataInputStream(ExtendedBlock b)
+          throws IOException {
+    if (!b.isProvidedBlock()) {
+      return super.getMetaDataInputStream(b);
+    } else {
+      FsVolumeImpl cloudVolume = getCloudVolume();
+      String metaFileKey = CloudHelper.getMetaFileKey(prefixSize, b.getLocalBlock());
+      File localMetaFileCopy = new File(cloudVolume.getCacheDir(b.getBlockPoolId()),
+              CloudHelper.getMetaFileKey(prefixSize, b.getLocalBlock()));
+
+      InputStream is = getInputStreamInternal(b.getCloudBucketID(), metaFileKey,
+              localMetaFileCopy, b.getBlockPoolId(), 0);
+      LengthInputStream lis = new LengthInputStream(is, localMetaFileCopy.length());
+
+      return lis;
+    }
+  }
+
+  private InputStream getInputStreamInternal(short cloudBucketID, String objectKey,
+                                             File localCopy, String bpid,
+                                             long seekOffset) throws IOException {
+    try {
+      long startTime = System.currentTimeMillis();
+
+      boolean download = bypassCache;
+      if (!bypassCache) {
+        if (!(localCopy.exists() && localCopy.length() > 0)) {
+          localCopy.delete();
+          download = true;
+        } else {
+          LOG.debug("HopsFS-Cloud. Reading provided block from cache. Block: " + objectKey);
+        }
+      }
+      if (download) {
+        cloud.downloadObject(cloudBucketID, objectKey, localCopy);
+      }
+
+      InputStream ioStream = new FileInputStream(localCopy);
+      ioStream.skip(seekOffset);
+
+      providedBlocksCacheUpdateTS(bpid, localCopy);  //after opening the file put it in the cache
+
+      LOG.debug("HopsFS-Cloud. " + objectKey + " GetInputStream Fn took :" + (System.currentTimeMillis() - startTime));
+      return ioStream;
+    } catch (IOException e) {
+      throw new IOException("Could not read " + objectKey + ". ", e);
+    }
+  }
+
+  @Override // FsDatasetSpi
+  public ReplicaInfo getReplica(ExtendedBlock b) {
+    if (!b.isProvidedBlock()) {
+      return super.getReplica(b);
+    } else if (b.isProvidedBlock() && volumeMap.get(b.getBlockPoolId(), b.getBlockId()) != null) {
+      return super.getReplica(b);
+    } else {
+      return getReplicaInternal(b);
+    }
+  }
+
+  public ReplicaInfo getReplicaInternal(ExtendedBlock b) {
+    ReplicaInfo replicaInfo = super.getReplica(b);
+    if (replicaInfo != null) {
+      return replicaInfo;
+    }
+
+    try {
+      String blockFileKey = CloudHelper.getBlockKey(prefixSize, b.getLocalBlock());
+      if (cloud.objectExists(b.getCloudBucketID(), blockFileKey)) {
+        Map<String, String> metadata = cloud.getUserMetaData(b.getCloudBucketID(), blockFileKey);
+
+        long genStamp = Long.parseLong((String) metadata.get(GEN_STAMP));
+        long size = Long.parseLong((String) metadata.get(OBJECT_SIZE));
+
+        FinalizedReplica info = new FinalizedReplica(b.getBlockId(), size, genStamp,
+                b.getCloudBucketID(), null, null);
+        return info;
+      }
+
+    } catch (IOException up) {
+      LOG.info(up, up);
+    }
+    return null;
+  }
+
+  // Finalized provided blocks are removed from the replica map
+  public boolean isProvideBlockFinalized(ExtendedBlock b) {
+    assert b.isProvidedBlock();
+    return super.getReplica(b) == null ? true : false;
+  }
+
+
+  private String getCloudProviderName() {
+    return conf.get(DFSConfigKeys.DFS_CLOUD_PROVIDER,
+            DFSConfigKeys.DFS_CLOUD_PROVIDER_DEFAULT);
+  }
+
+  @Override
+  FsVolumeImpl getNewFsVolumeImpl(FsDatasetImpl dataset, String storageID,
+                                  File currentDir, Configuration conf,
+                                  StorageType storageType) throws IOException {
+    if (storageType == StorageType.CLOUD) {
+      if (getCloudProviderName().compareToIgnoreCase(CloudProvider.AWS.name()) == 0) {
+        return new CloudFsVolumeImpl(this, storageID, currentDir, conf, storageType);
+      } else {
+        throw new UnsupportedOperationException("Cloud provider '" +
+                getCloudProviderName() + "' is not supported");
+      }
+    } else {
+      return new FsVolumeImpl(this, storageID, currentDir, conf, storageType);
+    }
+  }
+
+  /**
+   * We're informed that a block is no longer valid.  We
+   * could lazily garbage-collect the block, but why bother?
+   * just get rid of it.
+   */
+  @Override // FsDatasetSpi
+  public void invalidate(String bpid, Block invalidBlks[]) throws IOException {
+
+    final List<String> errors = new ArrayList<String>();
+
+    for (Block b : invalidBlks) {
+      if (b.isProvidedBlock() && volumeMap.get(bpid, b.getBlockId()) != null) {
+        super.invalidateBlock(bpid, b, errors);
+      } else {
+        invalidateProvidedBlock(bpid, b, errors);
+      }
+    }
+
+    printInvalidationErrors(errors, invalidBlks.length);
+  }
+
+  private void invalidateProvidedBlock(String bpid, Block invalidBlk, List<String> errors)
+          throws IOException {
+    final File f;
+    final FsVolumeImpl v;
+    ReplicaInfo info;
+    synchronized (this) {
+      info = volumeMap.get(bpid, invalidBlk);
+    }
+
+    // case when the block is not yet uploaded to the cloud
+    if (info != null) {
+      super.invalidateBlock(bpid, invalidBlk, errors);
+    } else {
+      // block is in the cloud.
+      // Edge cases such as deletion of be blocks in flight
+      // should be taekn care of by the block reporting system
+
+      FsVolumeImpl cloudVolume = getCloudVolume();
+
+      if (cloudVolume == null) {
+        errors.add("HopsFS-Cloud. Failed to delete replica " + invalidBlk);
+      }
+
+      File localBlkCopy = new File(cloudVolume.getCacheDir(bpid),
+              CloudHelper.getBlockKey(prefixSize, invalidBlk));
+      File localMetaFileCopy = new File(cloudVolume.getCacheDir(bpid),
+              CloudHelper.getMetaFileKey(prefixSize, invalidBlk));
+
+      LOG.info("HopsFS-Cloud. Scheduling async deletion of block: " + invalidBlk);
+      File volumeDir = cloudVolume.getCurrentDir();
+      asyncDiskService.deleteAsyncProvidedBlock(new ExtendedBlock(bpid, invalidBlk),
+              cloud, localBlkCopy, localMetaFileCopy, volumeDir);
+    }
+  }
+
+  @Override
+  FinalizedReplica updateReplicaUnderRecovery(
+          String bpid,
+          ReplicaUnderRecovery rur,
+          long recoveryId,
+          long newBlockId,
+          long newlength,
+          short cloudBlockID) throws IOException {
+
+    if(!rur.isProvidedBlock()){
+      return super.updateReplicaUnderRecovery(bpid, rur, recoveryId, newBlockId, newlength,
+              cloudBlockID);
+    }
+
+    boolean uploadedToTheCloud = true;
+    ReplicaInfo ri = volumeMap.get(bpid, rur.getBlockId());
+    if (ri != null) {   //the block is open
+      try {
+        checkReplicaFilesInternal(ri);
+        uploadedToTheCloud = true;
+      } catch (IOException e) {
+        super.checkReplicaFiles(ri);
+        uploadedToTheCloud = false;
+      }
+    }
+
+    if (!uploadedToTheCloud) {
+      FinalizedReplica fr = super.updateReplicaUnderRecovery(bpid, rur, recoveryId,
+              newBlockId, newlength, cloudBlockID);
+      uploadFinalizedBlockToCloud(bpid, fr);
+      return fr;
+    } else {
+      return updateReplicaUnderRecoveryInternal(bpid, rur, recoveryId,
+              newBlockId, newlength, cloudBlockID);
+    }
+  }
+
+  private void uploadFinalizedBlockToCloud(String bpid, FinalizedReplica fr) throws IOException {
+    ExtendedBlock eb = new ExtendedBlock(bpid, new Block(fr.getBlockId(), fr.getVisibleLength(),
+            fr.getGenerationStamp(), fr.getCloudBucketID()));
+    preFinalizeInternal(eb);
+    finalizeBlockInternal(eb);
+  }
+
+  FinalizedReplica updateReplicaUnderRecoveryInternal(
+          String bpid,
+          ReplicaUnderRecovery rur,
+          long recoveryId,
+          long newBlockId,
+          long newlength,
+          short cloudBlockID) throws IOException {
+    //check recovery id
+    if (rur.getRecoveryID() != recoveryId) {
+      throw new IOException("rur.getRecoveryID() != recoveryId = " +
+              recoveryId + ", rur=" + rur);
+    }
+
+    boolean copyOnTruncate = newBlockId > 0L && rur.getBlockId() != newBlockId;
+    if (copyOnTruncate == true) {
+      throw new UnsupportedOperationException("Truncate using copy is not supported");
+    }
+
+    // Create new truncated block with truncated data and bump up GS
+    //update length
+    if (rur.getNumBytes() < newlength) {
+      throw new IOException(
+              "rur.getNumBytes() < newlength = " + newlength + ", rur=" + rur);
+    }
+
+    if (rur.getNumBytes() >= newlength) { // Create a new block even if zero bytes are truncated,
+      // because GS needs to be increased.
+      truncateProvidedBlock(bpid, rur, rur.getNumBytes(), newlength, recoveryId);
+      // update RUR with the new length
+      rur.setNumBytesNoPersistance(newlength);
+      rur.setGenerationStampNoPersistance(recoveryId);
+    }
+
+    return new FinalizedReplica(rur, null, null);
+  }
+
+  private void truncateProvidedBlock(String bpid, ReplicaInfo rur, long oldlen,
+                                     long newlen, long newGS) throws IOException {
+    LOG.info("HopsFS-Cloud. Truncating a block: " + rur.getBlockId() + "_" + rur.getGenerationStamp());
+
+    Block bOld = new Block(rur.getBlockId(), rur.getNumBytes(), rur.getGenerationStamp(),
+            rur.getCloudBucketID());
+    String oldBlkKey = CloudHelper.getBlockKey(prefixSize, bOld);
+    String oldBlkMetaKey = CloudHelper.getMetaFileKey(prefixSize, bOld);
+
+    if (newlen > oldlen) {
+      throw new IOException("Cannot truncate block to from oldlen (=" + oldlen +
+              ") to newlen (=" + newlen + ")");
+    }
+
+    //download the block
+    FsVolumeImpl vol = getCloudVolume();
+    File blockFile = new File(vol.getCacheDir(bpid), oldBlkKey);
+    File metaFile = new File(vol.getCacheDir(bpid), oldBlkMetaKey);
+
+    if (!(blockFile.exists() && blockFile.length() == bOld.getNumBytes())) {
+      blockFile.delete(); //delete old files if any
+      cloud.downloadObject(rur.getCloudBucketID(), oldBlkKey, blockFile);
+      providedBlocksCacheUpdateTS(bpid, blockFile);
+    }
+
+    if (!(metaFile.exists() && metaFile.length() > 0)) {
+      metaFile.delete(); //delete old files if any
+      cloud.downloadObject(rur.getCloudBucketID(), oldBlkMetaKey, metaFile);
+      providedBlocksCacheUpdateTS(bpid, metaFile);
+    }
+
+    //truncate the disk block and update the metafile
+    DataChecksum dcs = BlockMetadataHeader.readHeader(metaFile).getChecksum();
+    int checksumsize = dcs.getChecksumSize();
+    int bpc = dcs.getBytesPerChecksum();
+    long n = (newlen - 1) / bpc + 1;
+    long newmetalen = BlockMetadataHeader.getHeaderSize() + n * checksumsize;
+    long lastchunkoffset = (n - 1) * bpc;
+    int lastchunksize = (int) (newlen - lastchunkoffset);
+    byte[] b = new byte[Math.max(lastchunksize, checksumsize)];
+
+    RandomAccessFile blockRAF = new RandomAccessFile(blockFile, "rw");
+    try {
+      //truncate blockFile
+      blockRAF.setLength(newlen);
+
+      //read last chunk
+      blockRAF.seek(lastchunkoffset);
+      blockRAF.readFully(b, 0, lastchunksize);
+    } finally {
+      blockRAF.close();
+    }
+
+    //compute checksum
+    dcs.update(b, 0, lastchunksize);
+    dcs.writeValue(b, 0, false);
+
+    //update metaFile
+    RandomAccessFile metaRAF = new RandomAccessFile(metaFile, "rw");
+    try {
+      metaRAF.setLength(newmetalen);
+      metaRAF.seek(newmetalen - checksumsize);
+      metaRAF.write(b, 0, checksumsize);
+    } finally {
+      metaRAF.close();
+    }
+
+    //update the blocks
+    LOG.info("HopsFS-Cloud. Truncated on disk copy of the block: " + bOld);
+
+    Block bNew = new Block(rur.getBlockId(), newlen, newGS, rur.getCloudBucketID());
+    String newBlkKey = CloudHelper.getBlockKey(prefixSize, bNew);
+    String newBlkMetaKey = CloudHelper.getMetaFileKey(prefixSize, bNew);
+
+    if (!cloud.objectExists(rur.getCloudBucketID(), newBlkKey)
+            && !cloud.objectExists(rur.getCloudBucketID(), newBlkMetaKey)) {
+      LOG.info("HopsFS-Cloud. Uploading Truncated Block: " + bNew);
+      cloud.uploadObject(rur.getCloudBucketID(), newBlkKey, blockFile,
+              getBlockFileMetadata(bNew));
+      cloud.uploadObject(rur.getCloudBucketID(), newBlkMetaKey, metaFile,
+              getMetaMetadata(bNew));
+    } else {
+      LOG.error("HopsFS-Cloud. Block: " + b + " alreay exists.");
+      throw new IOException("Block: " + b + " alreay exists.");
+    }
+
+    LOG.info("HopsFS-Cloud. Deleting old block from cloud. Block: " + bOld);
+    cloud.deleteObject(rur.getCloudBucketID(), oldBlkKey);
+    cloud.deleteObject(rur.getCloudBucketID(), oldBlkMetaKey);
+
+    LOG.info("HopsFS-Cloud. Deleting disk tmp copy: " + bOld);
+    blockFile.delete();
+    metaFile.delete();
+
+    //remove the entry from replica map
+    volumeMap.remove(bpid, bNew.getBlockId());
+  }
+
+  @Override
+  public void checkReplicaFiles(final ReplicaInfo r) throws IOException {
+    //the block files has to be somewhere, either in the cloud on disk and case of non finalized
+    // blocks
+
+    try {
+      checkReplicaFilesInternal(r);
+    } catch (IOException e) {
+      super.checkReplicaFiles(r);
+    }
+  }
+
+  public void checkReplicaFilesInternal(final ReplicaInfo r) throws IOException {
+    //check replica's file
+    // make sure that the block and the meta objects exist in S3.
+    Block b = new Block(r.getBlockId(), r.getNumBytes(),
+            r.getGenerationStamp(), r.getCloudBucketID());
+    String blockKey = CloudHelper.getBlockKey(prefixSize, b);
+    String metaKey = CloudHelper.getMetaFileKey(prefixSize, b);
+
+    if (!cloud.objectExists(r.getCloudBucketID(), blockKey)) {
+      throw new FileNotFoundException("Block: " + b + " not found in the cloud storage");
+    }
+
+    long blockSize = cloud.getObjectSize(r.getCloudBucketID(), blockKey);
+    if (blockSize != r.getNumBytes()) {
+      throw new IOException(
+              "File length mismatched. Expected: " + r.getNumBytes() + " Got: " + blockSize);
+    }
+
+    if (!cloud.objectExists(r.getCloudBucketID(), metaKey)) {
+      throw new FileNotFoundException("Meta Object for Block: " + b + " not found in the cloud " +
+              "storage");
+    }
+
+    long metaFileSize = cloud.getObjectSize(r.getCloudBucketID(), metaKey);
+    if (metaFileSize == 0) {
+      throw new IOException("Metafile is empty. Block: " + b);
+    }
+  }
+
+  @Override
+  public synchronized FsVolumeImpl getVolume(final ExtendedBlock b) {
+
+    if (!b.isProvidedBlock()) {
+      return super.getVolume(b);
+    } else {
+      return getVolumeInternal(b);
+    }
+  }
+
+  @Override // FsDatasetSpi
+  public Map<DatanodeStorage, BlockReport> getBlockReports(String bpid) {
+    return super.getBlockReports(bpid);
+  }
+
+  public synchronized FsVolumeImpl getVolumeInternal(final ExtendedBlock b) {
+    if (!b.isProvidedBlock()) {
+      return super.getVolume(b);
+
+    } else {
+      return getCloudVolume();
+    }
+  }
+
+  @Override
+  public void shutdown() {
+    super.shutdown();
+    cloud.shutdown();
+  }
+
+  @VisibleForTesting
+  public FsVolumeImpl getCloudVolume() {
+    for (FsVolumeImpl vol : getVolumes()) {
+      if (vol.getStorageType() == StorageType.CLOUD) {
+        return vol;
+      }
+    }
+    return null;
+  }
+
+  private Map<String, String> getBlockFileMetadata(Block b) {
+    Map<String, String> metadata = new HashMap<>();
+    metadata.put(GEN_STAMP, Long.toString(b.getGenerationStamp()));
+    metadata.put(OBJECT_SIZE, Long.toString(b.getNumBytes()));
+    return metadata;
+  }
+
+  private Map<String, String> getMetaMetadata(Block b) {
+    Map<String, String> metadata = new HashMap<>();
+    return metadata;
+  }
+
+  public void providedBlocksCacheUpdateTS(String bpid, File f) throws IOException {
+    FsVolumeImpl cloudVolume = getCloudVolume();
+    cloudVolume.getBlockPoolSlice(bpid).fileAccessed(f);
+  }
+
+  public void providedBlocksCacheDelete(String bpid, File f) throws IOException {
+    FsVolumeImpl cloudVolume = getCloudVolume();
+    cloudVolume.getBlockPoolSlice(bpid).fileDeleted(f);
+  }
+
+  @VisibleForTesting
+  public CloudPersistenceProvider getCloudConnector() {
+    return cloud;
+  }
+
+  @VisibleForTesting
+  public void installMockCloudConnector(CloudPersistenceProvider mock) {
+    cloud = mock;
+  }
+}
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudFsVolumeImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudFsVolumeImpl.java
new file mode 100644
index 00000000000..8522228a086
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudFsVolumeImpl.java
@@ -0,0 +1,117 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import com.google.common.annotations.VisibleForTesting;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.StorageType;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.io.IOException;
+
+/**
+ * This empty class is there to distinguish between disk and cloud backed
+ * volumes
+ * <p/>
+ * It uses the {@link FsDatasetImpl} object for synchronization.
+ */
+@InterfaceAudience.Private
+@VisibleForTesting
+public class CloudFsVolumeImpl extends FsVolumeImpl {
+  public static final Logger LOG =
+          LoggerFactory.getLogger(CloudFsVolumeImpl.class);
+  CloudFsVolumeImpl(FsDatasetImpl dataset, String storageID, File currentDir,
+                    Configuration conf, StorageType storageType) throws IOException {
+    super(dataset, storageID, currentDir, conf, storageType);
+    LOG.info("HopsFS-Cloud. Initializing CloudFsVolumeImpl.  ");
+  }
+
+  @Override
+  File createRbwFile(String bpid, Block b) throws IOException {
+    LOG.info("HopsFS-Cloud. Creating Rbw File. BlockID: "+b.getBlockId()+
+            " GenStamp: "+b.getGenerationStamp());
+    return super.createRbwFile(bpid, b);
+  }
+
+  @Override
+  public BlockIterator newBlockIterator(String bpid, String name) {
+    return new BlockIteratorImpl(bpid, name);
+  }
+
+
+  /**
+   * A BlockIterator implementation for FsVolumeImpl.
+   */
+  private class BlockIteratorImpl implements FsVolumeSpi.BlockIterator {
+
+    private final String name;
+    private final String bpid;
+
+    BlockIteratorImpl(String bpid, String name) {
+      this.name = name;
+      this.bpid = bpid;
+    }
+
+    @Override
+    public ExtendedBlock nextBlock() throws IOException {
+      return null;
+    }
+
+    @Override
+    public boolean atEnd() {
+      return true;
+    }
+
+    @Override
+    public void rewind() {
+    }
+
+    @Override
+    public void save() throws IOException {
+    }
+
+    @Override
+    public void setMaxStalenessMs(long maxStalenessMs) {
+    }
+
+    @Override
+    public void close() throws IOException {
+    }
+
+    @Override
+    public long getIterStartMs() {
+      return System.currentTimeMillis();
+    }
+
+    @Override
+    public long getLastSavedMs() {
+      return System.currentTimeMillis();
+    }
+
+    @Override
+    public String getBlockPoolId() {
+      return bpid;
+    }
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudPersistenceProviderFactory.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudPersistenceProviderFactory.java
new file mode 100644
index 00000000000..cbcf4077a70
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudPersistenceProviderFactory.java
@@ -0,0 +1,20 @@
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CloudProvider;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+
+public class CloudPersistenceProviderFactory {
+
+  public static CloudPersistenceProvider getCloudClient(Configuration conf) {
+    String cloudProvider = conf.get(DFSConfigKeys.DFS_CLOUD_PROVIDER,
+            DFSConfigKeys.DFS_CLOUD_PROVIDER_DEFAULT);
+    if (cloudProvider.compareToIgnoreCase(CloudProvider.AWS.name()) == 0) {
+      return new CloudPersistenceProviderS3Impl(conf);
+    } else {
+      throw new UnsupportedOperationException("Cloud provider '" + cloudProvider +
+              "' is not supported");
+    }
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudPersistenceProviderS3Impl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudPersistenceProviderS3Impl.java
new file mode 100644
index 00000000000..6a4439dd3e7
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/CloudPersistenceProviderS3Impl.java
@@ -0,0 +1,576 @@
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import com.amazonaws.AmazonServiceException;
+import com.amazonaws.SdkClientException;
+import com.amazonaws.regions.Regions;
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.AmazonS3ClientBuilder;
+import com.amazonaws.services.s3.model.*;
+import com.amazonaws.services.s3.transfer.Download;
+import com.amazonaws.services.s3.transfer.TransferManager;
+import com.amazonaws.services.s3.transfer.TransferManagerConfiguration;
+import com.amazonaws.services.s3.transfer.Upload;
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Sets;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.CloudBlock;
+import org.apache.hadoop.hdfs.server.common.CloudHelper;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.util.*;
+import java.util.concurrent.*;
+import java.util.concurrent.atomic.AtomicInteger;
+
+public class CloudPersistenceProviderS3Impl implements CloudPersistenceProvider {
+
+  static final Log LOG = LogFactory.getLog(CloudPersistenceProviderS3Impl.class);
+
+  private final Configuration conf;
+  private final AmazonS3 s3Client;
+  private final String bucketPrefix;
+  private final String bucketIDSeparator = ".";
+  private final Regions region;
+  private final int numBuckets;
+  private final int prefixSize;
+  private TransferManager transfers;
+  private ExecutorService threadPoolExecutor;
+  private final int bucketDeletionThreads;
+
+  CloudPersistenceProviderS3Impl(Configuration conf) {
+    this.conf = conf;
+    this.bucketPrefix = conf.get(DFSConfigKeys.S3_BUCKET_PREFIX,
+            DFSConfigKeys.S3_BUCKET_PREFIX_DEFAULT);
+    this.region = Regions.fromName(conf.get(DFSConfigKeys.DFS_CLOUD_AWS_S3_REGION,
+            DFSConfigKeys.DFS_CLOUD_AWS_S3_REGION_DEFAULT));
+    this.numBuckets = conf.getInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS,
+            DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS_DEFAULT);
+    this.bucketDeletionThreads =
+            conf.getInt(DFSConfigKeys.DFS_NN_MAX_THREADS_FOR_FORMATTING_CLOUD_BUCKETS_KEY,
+                    DFSConfigKeys.DFS_NN_MAX_THREADS_FOR_FORMATTING_CLOUD_BUCKETS_DEFAULT);
+    this.prefixSize = conf.getInt(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY,
+            DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_DEFAULT);
+
+    this.s3Client = connect();
+
+    initTransferManager();
+  }
+
+  private AmazonS3 connect() {
+    LOG.info("HopsFS-Cloud. Connecting to S3. Region " + region);
+    AmazonS3 s3client = AmazonS3ClientBuilder.standard()
+            .withRegion(region)
+            .build();
+
+    return s3client;
+  }
+
+  private void initTransferManager() {
+    int maxThreads = conf.getInt(DFSConfigKeys.DFS_DN_CLOUD_MAX_TRANSFER_THREADS,
+            DFSConfigKeys.DFS_DN_CLOUD_MAX_TRANSFER_THREADS_DEFAULT);
+    if (maxThreads < 2) {
+      LOG.warn(DFSConfigKeys.DFS_DN_CLOUD_MAX_TRANSFER_THREADS +
+              " must be at least 2: forcing to 2.");
+      maxThreads = 2;
+    }
+
+    long keepAliveTime = conf.getLong(DFSConfigKeys.DFS_CLOUD_KEEPALIVE_TIME,
+            DFSConfigKeys.DFS_CLOUD_KEEPALIVE_TIME_DEFAULT);
+
+    threadPoolExecutor = new ThreadPoolExecutor(
+            maxThreads, Integer.MAX_VALUE,
+            keepAliveTime, TimeUnit.SECONDS,
+            new LinkedBlockingQueue<Runnable>(),
+            BlockingThreadPoolExecutorService.newDaemonThreadFactory(
+                    "hopsfs-cloud-transfers-unbounded"));
+
+    long partSize = conf.getLong(DFSConfigKeys.DFS_CLOUD_MULTIPART_SIZE,
+            DFSConfigKeys.DFS_CLOUD_MULTIPART_SIZE_DEFAULT);
+
+    if (partSize < 5 * 1024 * 1024) {
+      LOG.error(DFSConfigKeys.DFS_CLOUD_MULTIPART_SIZE + " must be at least 5 MB");
+      partSize = 5 * 1024 * 1024;
+    }
+
+    long multiPartThreshold = conf.getLong(DFSConfigKeys.DFS_CLOUD_MIN_MULTIPART_THRESHOLD,
+            DFSConfigKeys.DFS_CLOUD_MIN_MULTIPART_THRESHOLD_DEFAULT);
+    if (multiPartThreshold < 5 * 1024 * 1024) {
+      LOG.error(DFSConfigKeys.DFS_CLOUD_MIN_MULTIPART_THRESHOLD + " must be at least 5 MB");
+      multiPartThreshold = 5 * 1024 * 1024;
+    }
+
+    TransferManagerConfiguration transferConfiguration = new TransferManagerConfiguration();
+    transferConfiguration.setMinimumUploadPartSize(partSize);
+    transferConfiguration.setMultipartUploadThreshold(multiPartThreshold);
+    transferConfiguration.setMultipartCopyPartSize(partSize);
+    transferConfiguration.setMultipartCopyThreshold(multiPartThreshold);
+
+    transfers = new TransferManager(s3Client, threadPoolExecutor);
+    transfers.setConfiguration(transferConfiguration);
+  }
+
+  static long longOption(Configuration conf,
+                         String key,
+                         long defVal,
+                         long min) {
+    long v = conf.getLong(key, defVal);
+    Preconditions.checkArgument(v >= min,
+            String.format("Value of %s: %d is below the minimum value %d",
+                    key, v, min));
+    return v;
+  }
+
+  private void createS3Bucket(String bucketName) {
+    if (!s3Client.doesBucketExist(bucketName)) {
+      s3Client.createBucket(bucketName);
+      // Verify that the bucket was created by retrieving it and checking its location.
+      String bucketLocation = s3Client.getBucketLocation(new GetBucketLocationRequest(bucketName));
+      LOG.info("HopsFS-Cloud. New bucket created. Name: " +
+              bucketName + " Location: " + bucketLocation);
+    } else {
+      LOG.info("HopsFS-Cloud. Bucket already exists. Bucket Name: " + bucketName);
+    }
+  }
+
+  /*
+  deletes all the bucket belonging to this user.
+  This is only used for testing.
+   */
+  public void deleteAllBuckets(String prefix) {
+    ExecutorService tPool = Executors.newFixedThreadPool(bucketDeletionThreads);
+    try {
+      List<Bucket> buckets = s3Client.listBuckets();
+      LOG.info("HopsFS-Cloud. Deleting all of the buckets for this user. Number of deletion " +
+              "threads " + bucketDeletionThreads);
+      for (Bucket b : buckets) {
+        if (b.getName().startsWith(prefix)) {
+          emptyAndDeleteS3Bucket(b.getName(), tPool);
+        }
+      }
+    } finally {
+      tPool.shutdown();
+    }
+  }
+
+  /*
+  Deletes all the buckets that are used by HopsFS
+   */
+  @Override
+  public void format() {
+    ExecutorService tPool = Executors.newFixedThreadPool(bucketDeletionThreads);
+    try {
+      System.out.println("HopsFS-Cloud. Deleting all of the buckets used by HopsFS. Number of " +
+              "deletion " +
+              "threads " + bucketDeletionThreads);
+      for (int i = 0; i < numBuckets; i++) {
+        emptyAndDeleteS3Bucket(getBucketDNSID(i), tPool);
+      }
+
+      createBuckets();
+    } finally {
+      tPool.shutdown();
+    }
+  }
+
+  @Override
+  public void checkAllBuckets() {
+
+    final int retry = 300;  // keep trying until the newly created bucket is available
+    for (int i = 0; i < numBuckets; i++) {
+      String bucketID = getBucketDNSID(i);
+      boolean exists = false;
+      for (int j = 0; j < retry; j++) {
+        if (!s3Client.doesBucketExistV2(bucketID)) {
+          //wait for a sec and retry
+          try {
+            Thread.sleep(1000);
+          } catch (InterruptedException e) {
+          }
+          continue;
+        } else {
+          exists = true;
+          break;
+        }
+      }
+
+      if (!exists) {
+        throw new IllegalStateException("S3 Bucket " + bucketID + " needed for the file system " +
+                "does not exists");
+      } else {
+        //check the bucket is writable
+        UUID uuid = UUID.randomUUID();
+        try {
+          s3Client.putObject(bucketID, uuid.toString()/*key*/, "test");
+          s3Client.deleteObject(bucketID, uuid.toString()/*key*/);
+        } catch (Exception e) {
+          throw new IllegalStateException("Write test for S3 bucket: " + bucketID + " failed. " + e);
+        }
+      }
+    }
+  }
+
+  private void createBuckets() {
+    for (int i = 0; i < numBuckets; i++) {
+      createS3Bucket(getBucketDNSID(i));
+    }
+  }
+
+
+  private void emptyAndDeleteS3Bucket(final String bucketName, ExecutorService tPool) {
+    final AtomicInteger deletedBlocks = new AtomicInteger(0);
+    try {
+      if (!s3Client.doesBucketExistV2(bucketName)) {
+        return;
+      }
+
+      System.out.println("HopsFS-Cloud. Deleting bucket: " + bucketName);
+
+      ObjectListing objectListing = s3Client.listObjects(bucketName);
+      while (true) {
+        Iterator<S3ObjectSummary> objIter = objectListing.getObjectSummaries().iterator();
+
+        final List<Callable<Object>> addTasks = new ArrayList<>();
+        while (objIter.hasNext()) {
+          final String objectKey = objIter.next().getKey();
+
+          Callable task = new Callable<Object>() {
+            @Override
+            public Object call() throws Exception {
+              s3Client.deleteObject(bucketName, objectKey);
+              String msg = "\rDeleted Blocks: " + (deletedBlocks.incrementAndGet());
+              System.out.print(msg);
+              return null;
+            }
+          };
+          tPool.submit(task);
+        }
+
+        // If the bucket contains many objects, the listObjects() call
+        // might not return all of the objects in the first listing. Check to
+        // see whether the listing was truncated. If so, retrieve the next page of objects
+        // and delete them.
+        if (objectListing.isTruncated()) {
+          objectListing = s3Client.listNextBatchOfObjects(objectListing);
+        } else {
+          break;
+        }
+      }
+
+      System.out.println("");
+
+      // Delete all object versions (required for versioned buckets).
+      VersionListing versionList = s3Client.listVersions(
+              new ListVersionsRequest().withBucketName(bucketName));
+      while (true) {
+        Iterator<S3VersionSummary> versionIter = versionList.getVersionSummaries().iterator();
+        while (versionIter.hasNext()) {
+          S3VersionSummary vs = versionIter.next();
+          s3Client.deleteVersion(bucketName, vs.getKey(), vs.getVersionId());
+        }
+
+        if (versionList.isTruncated()) {
+          versionList = s3Client.listNextBatchOfVersions(versionList);
+        } else {
+          break;
+        }
+      }
+
+      // After all objects and object versions are deleted, delete the bucket.
+      s3Client.deleteBucket(bucketName);
+    } catch (AmazonServiceException up) {
+      // The call was transmitted successfully, but Amazon S3 couldn't process
+      // it, so it returned an error response.
+      up.printStackTrace();
+      throw up;
+    } catch (SdkClientException up) {
+      // Amazon S3 couldn't be contacted for a response, or the client couldn't
+      // parse the response from Amazon S3.
+      up.printStackTrace();
+      throw up;
+    }
+  }
+
+
+  @Override
+  public void uploadObject(short bucketID, String objectID, File object,
+                           Map<String, String> metadata) throws IOException {
+    try {
+      LOG.debug("HopsFS-Cloud. Put Object. Bucket ID: " + bucketID + " Object ID: " + objectID);
+
+      long startTime = System.currentTimeMillis();
+      String bucket = getBucketDNSID(bucketID);
+      PutObjectRequest putReq = new PutObjectRequest(bucket,
+              objectID, object);
+
+      // Upload a file as a new object with ContentType and title specified.
+      ObjectMetadata objMetadata = new ObjectMetadata();
+      objMetadata.setContentType("plain/text");
+      //objMetadata.addUserMetadata(entry.getKey(), entry.getValue());
+      objMetadata.setUserMetadata(metadata);
+      putReq.setMetadata(objMetadata);
+
+      Upload upload = transfers.upload(putReq);
+
+      upload.waitForUploadResult();
+      LOG.info("HopsFS-Cloud. Put Object. Bucket ID: " + bucketID + " Object ID: " + objectID
+              + " Time (ms): " + (System.currentTimeMillis() - startTime));
+    } catch (InterruptedException e) {
+      throw new InterruptedIOException(e.toString());
+    } catch (AmazonServiceException e) {
+      throw new IOException(e);
+    } catch (SdkClientException e) {
+      throw new IOException(e);
+    }
+  }
+
+  public String getBucketDNSID(int ID) {
+    return bucketPrefix + bucketIDSeparator + ID;
+  }
+
+  @Override
+  public int getPrefixSize() {
+    return prefixSize;
+  }
+
+  @Override
+  public boolean objectExists(short bucketID, String objectID) throws IOException {
+    try {
+      long startTime = System.currentTimeMillis();
+      boolean exists = s3Client.doesObjectExist(getBucketDNSID(bucketID), objectID);
+      LOG.debug("HopsFS-Cloud. Object Exists?. Bucket ID: " + bucketID + " Object ID: " + objectID
+              + " Time (ms): " + (System.currentTimeMillis() - startTime));
+      return exists;
+    } catch (AmazonServiceException e) {
+      throw new IOException(e); // throwing runtime exception will kill DN
+    } catch (SdkClientException e) {
+      throw new IOException(e);
+    }
+  }
+
+  private ObjectMetadata getS3ObjectMetadata(short bucketID, String objectID)
+          throws IOException {
+    try {
+      GetObjectMetadataRequest req = new GetObjectMetadataRequest(getBucketDNSID(bucketID),
+              objectID);
+      ObjectMetadata s3metadata = s3Client.getObjectMetadata(req);
+      return s3metadata;
+    } catch (AmazonServiceException e) {
+      throw new IOException(e); // throwing runtime exception will kill DN
+    } catch (SdkClientException e) {
+      throw new IOException(e);
+    }
+  }
+
+
+  @Override
+  public Map<String, String> getUserMetaData(short bucketID, String objectID)
+          throws IOException {
+    long startTime = System.currentTimeMillis();
+    ObjectMetadata s3metadata = getS3ObjectMetadata(bucketID, objectID);
+    Map<String, String> metadata = s3metadata.getUserMetadata();
+    LOG.info("HopsFS-Cloud. Get Object Metadata. Bucket ID: " + bucketID + " Object ID: " + objectID
+            + " Time (ms): " + (System.currentTimeMillis() - startTime));
+    return metadata;
+  }
+
+  @Override
+  public long getObjectSize(short bucketID, String objectID) throws IOException {
+    long startTime = System.currentTimeMillis();
+    ObjectMetadata s3metadata = getS3ObjectMetadata(bucketID, objectID);
+    long size = s3metadata.getContentLength();
+    LOG.debug("HopsFS-Cloud. Get Object Size. Bucket ID: " + bucketID + " Object ID: " + objectID
+            + " Time (ms): " + (System.currentTimeMillis() - startTime));
+    return size;
+  }
+
+  @Override
+  public void downloadObject(short bucketID, String objectID, File path) throws IOException {
+    try {
+      long startTime = System.currentTimeMillis();
+      Download down = transfers.download(getBucketDNSID(bucketID), objectID, path);
+      down.waitForCompletion();
+      LOG.info("HopsFS-Cloud. Download Object. Bucket ID: " + bucketID + " Object ID: " + objectID
+              + " Download Path: " + path
+              + " Time (ms): " + (System.currentTimeMillis() - startTime));
+    } catch (AmazonServiceException e) {
+      throw new IOException(e); // throwing runtime exception will kill DN
+    } catch (SdkClientException e) {
+      throw new IOException(e);
+    } catch (InterruptedException e) {
+      throw new InterruptedIOException(e.toString());
+    }
+  }
+
+  @Override
+  public Map<Long, CloudBlock> getAll(String prefix) throws IOException {
+    Map<Long, CloudBlock> blocks = new HashMap<>();
+    for (int i = 0; i < numBuckets; i++) {
+      listBucket(getBucketDNSID(i), prefix, blocks);
+    }
+    return blocks;
+  }
+
+  @Override
+  public void deleteObject(short bucketID, String objectID) throws IOException {
+    try {
+      s3Client.deleteObject(getBucketDNSID(bucketID), objectID);
+    } catch (AmazonServiceException up) {
+      throw new IOException(up);
+    } catch (SdkClientException up) {
+      throw new IOException(up);
+    }
+  }
+
+  @Override
+  public void shutdown() {
+    s3Client.shutdown();
+    if (transfers != null) {
+      transfers.shutdownNow(true);
+      transfers = null;
+    }
+  }
+
+  private void listBucket(String bucketName, String prefix, Map<Long, CloudBlock> result)
+          throws IOException {
+    Map<Long, S3ObjectSummary> blockObjs = new HashMap<>();
+    Map<Long, S3ObjectSummary> metaObjs = new HashMap<>();
+
+    try {
+      if (!s3Client.doesBucketExist(bucketName)) {
+        return;
+      }
+
+      assert prefix != null;
+
+      ObjectListing objectListing = s3Client.listObjects(bucketName, prefix);
+      while (true) {
+        Iterator<S3ObjectSummary> objIter = objectListing.getObjectSummaries().iterator();
+        while (objIter.hasNext()) {
+          S3ObjectSummary s3Object = objIter.next();
+          String key = s3Object.getKey();
+
+          if (CloudHelper.isBlockFilename(key)) {
+            long blockID = CloudHelper.extractBlockIDFromBlockName(key);
+            blockObjs.put(blockID, s3Object);
+          } else if (CloudHelper.isMetaFilename(key)) {
+            long blockID = CloudHelper.extractBlockIDFromMetaName(key);
+            metaObjs.put(blockID, s3Object);
+          } else {
+            LOG.warn("HopsFS-Cloud. File system objects are tampered. The " + key + " is not HopsFS object.");
+          }
+        }
+
+        if (objectListing.isTruncated()) {
+          objectListing = s3Client.listNextBatchOfObjects(objectListing);
+        } else {
+          break;
+        }
+      }
+    } catch (AmazonServiceException up) {
+      throw new IOException(up);
+    } catch (SdkClientException up) {
+      throw new IOException(up);
+    }
+
+    mergeMetaAndBlockObjects(metaObjs, blockObjs, result);
+
+    return;
+  }
+
+  private void mergeMetaAndBlockObjects(Map<Long, S3ObjectSummary> metaObjs,
+                                        Map<Long, S3ObjectSummary> blockObjs,
+                                        Map<Long, CloudBlock> res) {
+
+    Set blockKeySet = blockObjs.keySet();
+    Set metaKeySet = metaObjs.keySet();
+    Sets.SetView<Long> symDiff = Sets.symmetricDifference(blockKeySet, metaKeySet);
+    Sets.SetView<Long> intersection = Sets.intersection(blockKeySet, metaKeySet);
+
+    for (Long blockID : intersection) {
+      S3ObjectSummary blockObj = blockObjs.get(blockID);
+      S3ObjectSummary metaObj = metaObjs.get(blockID);
+
+      long blockSize = blockObj.getSize();
+      short bucketID = CloudHelper.extractBucketID(blockObj.getBucketName());
+
+      //Generation stamps of the meta file and block much match
+      assert CloudHelper.extractGSFromBlockName(blockObj.getKey()) ==
+              CloudHelper.extractGSFromMetaName(metaObj.getKey());
+      long genStamp = CloudHelper.extractGSFromMetaName(metaObj.getKey());
+
+      Block block = new Block(blockID, blockSize, genStamp, bucketID);
+
+      CloudBlock cb = new CloudBlock(block, blockObj.getLastModified().getTime());
+      res.put(blockID, cb);
+    }
+
+    for (Long id : symDiff) {
+      String keyFound = "";
+      String bucket = "";
+      CloudBlock cb = new CloudBlock();
+
+      S3ObjectSummary blockObj = blockObjs.get(id);
+      S3ObjectSummary metaObj = metaObjs.get(id);
+
+      if (blockObj != null) {
+        cb.setBlockObjectFound(true);
+        cb.setLastModified(blockObj.getLastModified().getTime());
+        keyFound = blockObj.getKey();
+        bucket = blockObj.getBucketName();
+      } else if (metaObj != null) {
+        cb.setMetaObjectFound(true);
+        cb.setLastModified(metaObj.getLastModified().getTime());
+        keyFound = metaObj.getKey();
+        bucket = metaObj.getBucketName();
+      }
+
+      long blockID;
+      long gs;
+
+      if (CloudHelper.isMetaFilename(keyFound)) {
+        blockID = CloudHelper.extractBlockIDFromMetaName(keyFound);
+        gs = CloudHelper.extractGSFromMetaName(keyFound);
+      } else if (CloudHelper.isBlockFilename(keyFound)) {
+        blockID = CloudHelper.extractBlockIDFromBlockName(keyFound);
+        gs = CloudHelper.extractGSFromBlockName(keyFound);
+      } else {
+        LOG.warn("HopsFS-Cloud. File system objects are tampered. The " + keyFound + " is not HopsFS " +
+                "object.");
+        continue;
+      }
+
+      Block block = new Block();
+      block.setBlockIdNoPersistance(blockID);
+      block.setGenerationStampNoPersistance(gs);
+      block.setCloudBucketIDNoPersistance(CloudHelper.extractBucketID(bucket));
+      cb.setBlock(block);
+      res.put(id, cb);
+    }
+  }
+
+  public void renameObject(short srcBucket, short dstBucket, String srcKey,
+                           String dstKey) throws IOException {
+    try {
+      CopyObjectRequest req = new CopyObjectRequest(getBucketDNSID(srcBucket), srcKey,
+              getBucketDNSID(dstBucket), dstKey);
+      CopyObjectResult res = s3Client.copyObject(req);
+
+      //delete the src
+      deleteObject(srcBucket, srcKey);
+    } catch (AmazonServiceException up) {
+      throw new IOException(up);
+    } catch (SdkClientException up) {
+      throw new IOException(up);
+    }
+
+
+  }
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
index c8163c751ce..de59002cc09 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java
@@ -21,6 +21,7 @@
 
 import java.io.File;
 import java.io.FileDescriptor;
+import java.io.IOException;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
@@ -33,8 +34,12 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.CloudProvider;
 import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
+import org.apache.hadoop.hdfs.server.common.CloudHelper;
 import org.apache.hadoop.hdfs.server.datanode.DataNode;
+import org.apache.hadoop.hdfs.server.datanode.DatanodeUtil;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference;
 import org.apache.hadoop.hdfs.server.protocol.BlockCommand;
 import org.apache.hadoop.io.IOUtils;
@@ -221,7 +226,14 @@ void deleteAsync(FsVolumeReference volumeRef, File blockFile, File metaFile,
         volumeRef, blockFile, metaFile, block, trashDirectory);
     execute(((FsVolumeImpl) volumeRef.getVolume()).getCurrentDir(), deletionTask);
   }
-  
+
+  void deleteAsyncProvidedBlock(ExtendedBlock block, CloudPersistenceProvider cloud,
+                                File blockFile, File metaFile, File volumeDir) {
+    FsDatasetAsyncDiskService.CloudReplicaObjectDeleteTask deletionTask =
+            new FsDatasetAsyncDiskService.CloudReplicaObjectDeleteTask(
+                    block, cloud, blockFile, metaFile);
+    execute(volumeDir, deletionTask);
+  }
   /**
    * A task for deleting a block file and its associated meta file, as well
    * as decrement the dfs usage of the volume.
@@ -321,4 +333,61 @@ private synchronized void updateDeletedBlockId(ExtendedBlock block) {
       numDeletedBlocks = 0;
     }
   }
+
+  public class CloudReplicaObjectDeleteTask implements Runnable {
+    final ExtendedBlock block;
+    final CloudPersistenceProvider cloud;
+    final File localBlockFile;
+    final File localMetaFile;
+
+
+    public CloudReplicaObjectDeleteTask(ExtendedBlock block, CloudPersistenceProvider cloud,
+                                        final File localBlockFile, final File localMetaFile ) {
+      this.block = block;
+      this.cloud = cloud;
+      this.localBlockFile = localBlockFile;
+      this.localMetaFile = localMetaFile;
+    }
+
+    @Override
+    public String toString() {
+      // Called in AsyncDiskService.execute for displaying error messages.
+      return "HopsFS-Cloud. deletion of block " + block.getBlockPoolId() + " BlockID: " +
+              block.getBlockId() + " GenStamp: " + block.getGenerationStamp() +
+              " CloudBucketID " + block.getCloudBucketID();
+    }
+
+    @Override
+    public void run() {
+      LOG.info("HopsFS-Cloud. Deleting block from cloud. " + block);
+      short bucketID = block.getCloudBucketID();
+      String blockKey = CloudHelper.getBlockKey(cloud.getPrefixSize(), block.getLocalBlock());
+      String metaFileKey = CloudHelper.getMetaFileKey(cloud.getPrefixSize(), block.getLocalBlock());
+
+      try {
+        cloud.deleteObject(bucketID, blockKey);
+        if (localBlockFile.delete()) {
+          LOG.info("HopsFS-Cloud. Deleted cached block "+blockKey);
+          ((CloudFsDatasetImpl) fsdatasetImpl).providedBlocksCacheDelete(block.getBlockPoolId(),
+                  localBlockFile);
+        } else {
+          LOG.info("HopsFS-Cloud. No local cached copy found for block:"
+                  +blockKey);
+        }
+
+        cloud.deleteObject(bucketID, metaFileKey);
+        //delete these blocks from local cache
+        if (localMetaFile.delete()) {
+          LOG.info("HopsFS-Cloud. Deleted cached block meta file: "+metaFileKey);
+          ((CloudFsDatasetImpl) fsdatasetImpl).providedBlocksCacheDelete(block.getBlockPoolId(),
+                  localMetaFile);
+        } else {
+          LOG.info("HopsFS-Cloud. No local cached copy found for Meta file:"
+                  +metaFileKey);
+        }
+      } catch (IOException e) {
+        LOG.warn("HopsFS-Cloud. Unable to delete block "+block, e);
+      }
+    }
+  }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java
index 49913a5fd56..9d6a8f8caf5 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java
@@ -274,7 +274,7 @@ public FsDatasetCache(FsDatasetImpl dataset) {
    */
   synchronized void cacheBlock(long blockId, String bpid,
       String blockFileName, long length, long genstamp,
-      Executor volumeExecutor) {
+      short cloudBucketID, Executor volumeExecutor) {
     ExtendedBlockId key = new ExtendedBlockId(blockId, bpid);
     Value prevValue = mappableBlockMap.get(key);
     if (prevValue != null) {
@@ -285,7 +285,7 @@ synchronized void cacheBlock(long blockId, String bpid,
     }
     mappableBlockMap.put(key, new Value(null, State.CACHING));
     volumeExecutor.execute(
-        new CachingTask(key, blockFileName, length, genstamp));
+        new CachingTask(key, blockFileName, length, genstamp, cloudBucketID));
     LOG.debug("Initiating caching for Block with id {}, pool {}", blockId,
       bpid);
   }
@@ -346,12 +346,15 @@ synchronized void uncacheBlock(String bpid, long blockId) {
     private final String blockFileName;
     private final long length;
     private final long genstamp;
+    private final short cloudBucketID;
 
-    CachingTask(ExtendedBlockId key, String blockFileName, long length, long genstamp) {
+    CachingTask(ExtendedBlockId key, String blockFileName, long length,
+                long genstamp, short cloudBucketID) {
       this.key = key;
       this.blockFileName = blockFileName;
       this.length = length;
       this.genstamp = genstamp;
+      this.cloudBucketID = cloudBucketID;
     }
 
     @Override
@@ -360,7 +363,8 @@ public void run() {
       FileInputStream blockIn = null, metaIn = null;
       MappableBlock mappableBlock = null;
       ExtendedBlock extBlk =
-          new ExtendedBlock(key.getBlockPoolId(), key.getBlockId(), length, genstamp);
+          new ExtendedBlock(key.getBlockPoolId(), key.getBlockId(),
+                  length, genstamp, cloudBucketID);
       long newUsedBytes = usedBytesCount.reserve(length);
       boolean reservedBytes = false;
       try {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
index a57275f4894..679b24a12d8 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java
@@ -176,7 +176,7 @@ public synchronized Block getStoredBlock(String bpid, long blkid)
     }
     final File metafile = FsDatasetUtil.findMetaFile(blockfile);
     final long gs = FsDatasetUtil.parseGenerationStamp(blockfile, metafile);
-    return new Block(blkid, blockfile.length(), gs);
+    return new Block(blkid, blockfile.length(), gs, Block.NON_EXISTING_BUCKET_ID);
   }
 
   /**
@@ -223,7 +223,7 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)
   final Map<String, DatanodeStorage> storageMap;
   final FsDatasetAsyncDiskService asyncDiskService;
   final FsDatasetCache cacheManager;
-  private final Configuration conf;
+  final Configuration conf;
   private final int validVolsRequired;
   
   final ReplicaMap volumeMap;
@@ -340,15 +340,28 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)
 
   private void addVolume(Collection<StorageLocation> dataLocations,
       Storage.StorageDirectory sd) throws IOException {
+
     final File dir = sd.getCurrentDir();
     final StorageType storageType =
-        getStorageTypeFromLocations(dataLocations, sd.getRoot());
+            getStorageTypeFromLocations(dataLocations, sd.getRoot());
+
+    // currently only one CLOUD storage per datanode is supported
+    // Why? 
+    //   1. A block might be cached on more than one volume
+    //   2. We do not use replca Map in the CloudFSDatasetImpl. 
+    //      That is, we do not know which volume stores the block 
+    for(DatanodeStorage storage : storageMap.values()){
+      if (storage.getStorageType() == StorageType.CLOUD && storageType == StorageType.CLOUD){
+        throw new RuntimeException("Bad datanode configuration. Only one CLOUD storage is " +
+                "supported per datanode");
+      }
+    }
 
     // If IOException raises from FsVolumeImpl() or getVolumeMap(), there is
     // nothing needed to be rolled back to make various data structures, e.g.,
     // storageMap and asyncDiskService, consistent.
-    FsVolumeImpl fsVolume = new FsVolumeImpl(
-        this, sd.getStorageUuid(), dir, this.conf, storageType);
+    FsVolumeImpl fsVolume = getNewFsVolumeImpl(
+            this, sd.getStorageUuid(), dir, this.conf, storageType);
     FsVolumeReference ref = fsVolume.obtainReference();
     ReplicaMap tempVolumeMap = new ReplicaMap(this);
     fsVolume.getVolumeMap(tempVolumeMap);
@@ -390,8 +403,10 @@ public void addVolume(final StorageLocation location,
     final Storage.StorageDirectory sd = builder.getStorageDirectory();
 
     StorageType storageType = location.getStorageType();
-    final FsVolumeImpl fsVolume =
-        createFsVolume(sd.getStorageUuid(), sd.getCurrentDir(), storageType);
+
+    FsVolumeImpl fsVolume = getNewFsVolumeImpl(this, sd.getStorageUuid(), sd.getCurrentDir(),
+            conf, storageType);
+
     final ReplicaMap tempVolumeMap = new ReplicaMap(fsVolume);
     ArrayList<IOException> exceptions = Lists.newArrayList();
 
@@ -878,7 +893,8 @@ public ReplicaInfo moveBlockAcrossStorage(ExtendedBlock block,
 
       ReplicaInfo newReplicaInfo = new ReplicaInPipeline(
           replicaInfo.getBlockId(), replicaInfo.getGenerationStamp(),
-          targetVolume, blockFiles[0].getParentFile(), 0);
+          replicaInfo.getCloudBucketID(), targetVolume,
+          blockFiles[0].getParentFile(), 0);
       newReplicaInfo.setNumBytesNoPersistance(blockFiles[1].length());
       // Finalize the copied files
       newReplicaInfo = finalizeReplica(block.getBlockPoolId(), newReplicaInfo);
@@ -1017,7 +1033,8 @@ private synchronized ReplicaBeingWritten append(String bpid,
     File oldmeta = replicaInfo.getMetaFile();
     ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(
         replicaInfo.getBlockId(), replicaInfo.getNumBytes(), newGS,
-        v, newBlkFile.getParentFile(), Thread.currentThread(), estimateBlockLen);
+        replicaInfo.getCloudBucketID(), v, newBlkFile.getParentFile(),
+        Thread.currentThread(), estimateBlockLen);
     File newmeta = newReplicaInfo.getMetaFile();
 
     // rename meta file to rbw directory
@@ -1147,7 +1164,29 @@ public synchronized String recoverClose(ExtendedBlock b, long newGS,
     }
     return replicaInfo.getStorageUuid();
   }
-  
+
+  /**
+   * Pre-Finalize stage where the provided block is uploaded to the cloud
+   *
+   * @param b
+   * @throws IOException
+   */
+  @Override
+  public void preFinalize(ExtendedBlock b) throws IOException {
+    // Do nothing.
+  }
+
+  /**
+   * Post-Finalize stage
+   *
+   * @param b
+   * @throws IOException
+   */
+  @Override
+  public void postFinalize(ExtendedBlock b) throws IOException {
+    // Do nothing.
+  }
+
   /**
    * Bump a replica's generation stamp to a new one.
    * Its on-disk meta file name is renamed to be the new one too.
@@ -1204,7 +1243,7 @@ public synchronized ReplicaHandler createRbw(
     }
 
     ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(b.getBlockId(), 
-        b.getGenerationStamp(), v, f.getParentFile(), b.getNumBytes());
+        b.getGenerationStamp(), b.getCloudBucketID(), v, f.getParentFile(), b.getNumBytes());
     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);
     return new ReplicaHandler(newReplicaInfo, ref);
   }
@@ -1277,6 +1316,7 @@ public synchronized ReplicaInPipeline convertTemporaryToRbw(
     final long blockId = b.getBlockId();
     final long expectedGs = b.getGenerationStamp();
     final long visible = b.getNumBytes();
+    final short cloudBucketID = b.getCloudBucketID();
     LOG.info(
         "Convert " + b + " from Temporary to RBW, visible length=" + visible);
 
@@ -1324,7 +1364,7 @@ public synchronized ReplicaInPipeline convertTemporaryToRbw(
         bpslice.getRbwDir());
     // create RBW
     final ReplicaBeingWritten rbw = new ReplicaBeingWritten(
-        blockId, numBytes, expectedGs,
+        blockId, numBytes, expectedGs, cloudBucketID,
         v, dest.getParentFile(), Thread.currentThread(), 0);
     rbw.setBytesAcked(visible);
     // overwrite the RBW in the volume map
@@ -1357,9 +1397,8 @@ public ReplicaHandler createTemporary(
             IOUtils.cleanup(null, ref);
             throw e;
           }
-          ReplicaInPipeline newReplicaInfo =
-              new ReplicaInPipeline(b.getBlockId(), b.getGenerationStamp(), v,
-                  f.getParentFile(), 0);
+          ReplicaInPipeline newReplicaInfo = new ReplicaInPipeline(b.getBlockId(),
+                  b.getGenerationStamp(), b.getCloudBucketID(), v, f.getParentFile(), 0);
           volumeMap.add(b.getBlockPoolId(), newReplicaInfo);
           return new ReplicaHandler(newReplicaInfo, ref);
         } else {
@@ -1670,7 +1709,7 @@ File validateBlockFile(String bpid, Block b) {
   /**
    * Check the files of a replica.
    */
-  static void checkReplicaFiles(final ReplicaInfo r) throws IOException {
+  public void checkReplicaFiles(final ReplicaInfo r) throws IOException {
     //check replica's file
     final File f = r.getBlockFile();
     if (!f.exists()) {
@@ -1678,8 +1717,8 @@ static void checkReplicaFiles(final ReplicaInfo r) throws IOException {
     }
     if (r.getBytesOnDisk() != f.length()) {
       throw new IOException(
-          "File length mismatched.  The length of " + f + " is " + f.length() +
-              " but r=" + r);
+              "File length mismatched.  The length of " + f + " is " + f.length() +
+                      " but r=" + r);
     }
 
     //check replica's meta file
@@ -1700,66 +1739,77 @@ static void checkReplicaFiles(final ReplicaInfo r) throws IOException {
   @Override // FsDatasetSpi
   public void invalidate(String bpid, Block invalidBlks[]) throws IOException {
     final List<String> errors = new ArrayList<String>();
+
     for (Block invalidBlk : invalidBlks) {
-      final File f;
-      final FsVolumeImpl v;
-      synchronized (this) {
-        final ReplicaInfo info = volumeMap.get(bpid, invalidBlk);
-        if (info == null) {
-          // It is okay if the block is not found -- it may be deleted earlier.
-          LOG.info("Failed to delete replica " + invalidBlk +
-              ": ReplicaInfo not found.");
-          continue;
-        }
-        if (info.getGenerationStamp() != invalidBlk.getGenerationStamp()) {
-          errors.add("Failed to delete replica " + invalidBlk +
-              ": GenerationStamp not matched, info=" + info);
-          continue;
-        }
-        f = info.getBlockFile();
-        v = (FsVolumeImpl) info.getVolume();
-        if (v == null) {
-          errors.add("Failed to delete replica " + invalidBlk +
-              ". No volume for this replica, file=" + f);
-          continue;
-        }
-        File parent = f.getParentFile();
-        if (parent == null) {
-          errors.add("Failed to delete replica " + invalidBlk +
-              ". Parent not found for file " + f);
-          continue;
-        }
-        ReplicaInfo removing = volumeMap.remove(bpid, invalidBlk);
-        addDeletingBlock(bpid, removing.getBlockId());
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Block file " + removing.getBlockFile().getName()
-              + " is to be deleted");
-        }
+      invalidateBlock(bpid, invalidBlk,errors);
+    }
+
+    printInvalidationErrors(errors, invalidBlks.length);
+  }
+
+  public void invalidateBlock(String bpid, Block invalidBlk, List<String> errors)
+          throws IOException {
+    final File f;
+    final FsVolumeImpl v;
+    synchronized (this) {
+      final ReplicaInfo info = volumeMap.get(bpid, invalidBlk);
+      if (info == null) {
+        // It is okay if the block is not found -- it may be deleted earlier.
+        LOG.info("Failed to delete replica " + invalidBlk +
+                ": ReplicaInfo not found.");
+        return;
+      }
+      if (info.getGenerationStamp() != invalidBlk.getGenerationStamp()) {
+        errors.add("Failed to delete replica " + invalidBlk +
+                ": GenerationStamp not matched, info=" + info);
+        return;
+      }
+      f = info.getBlockFile();
+      v = (FsVolumeImpl) info.getVolume();
+      if (v == null) {
+        errors.add("Failed to delete replica " + invalidBlk +
+                ". No volume for this replica, file=" + f);
+        return;
       }
-    
-      // If a DFSClient has the replica in its cache of short-circuit file
-      // descriptors (and the client is using ShortCircuitShm), invalidate it.
-      datanode.getShortCircuitRegistry().processBlockInvalidation(
-                new ExtendedBlockId(invalidBlk.getBlockId(), bpid));
-      
-      // If the block is cached, start uncaching it.
-      cacheManager.uncacheBlock(bpid, invalidBlk.getBlockId());
-      
-      // Delete the block asynchronously to make sure we can do it fast enough.
-      // It's ok to unlink the block file before the uncache operation
-      // finishes.
-      try {
-        asyncDiskService.deleteAsync(v.obtainReference(), f,
-            FsDatasetUtil.getMetaFile(f, invalidBlk.getGenerationStamp()),
-            new ExtendedBlock(bpid, invalidBlk), dataStorage.getTrashDirectoryForBlockFile(bpid, f));
-      } catch (ClosedChannelException e) {
-        LOG.warn("Volume " + v + " is closed, ignore the deletion task for " + "block " + invalidBlks);
+      File parent = f.getParentFile();
+      if (parent == null) {
+        errors.add("Failed to delete replica " + invalidBlk +
+                ". Parent not found for file " + f);
+        return;
       }
+      ReplicaInfo removing = volumeMap.remove(bpid, invalidBlk);
+      addDeletingBlock(bpid, removing.getBlockId());
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Block file " + removing.getBlockFile().getName()
+                + " is to be deleted");
+      }
+    }
+
+    // If a DFSClient has the replica in its cache of short-circuit file
+    // descriptors (and the client is using ShortCircuitShm), invalidate it.
+    datanode.getShortCircuitRegistry().processBlockInvalidation(
+            new ExtendedBlockId(invalidBlk.getBlockId(), bpid));
+
+    // If the block is cached, start uncaching it.
+    cacheManager.uncacheBlock(bpid, invalidBlk.getBlockId());
+
+    // Delete the block asynchronously to make sure we can do it fast enough.
+    // It's ok to unlink the block file before the uncache operation
+    // finishes.
+    try {
+      asyncDiskService.deleteAsync(v.obtainReference(), f,
+              FsDatasetUtil.getMetaFile(f, invalidBlk.getGenerationStamp()),
+              new ExtendedBlock(bpid, invalidBlk), dataStorage.getTrashDirectoryForBlockFile(bpid, f));
+    } catch (ClosedChannelException e) {
+      LOG.warn("Volume " + v + " is closed, ignore the deletion task for " + "block " + invalidBlk);
     }
+  }
+
+  void printInvalidationErrors(List<String> errors, int total) throws IOException {
     if (!errors.isEmpty()) {
       StringBuilder b = new StringBuilder("Failed to delete ")
-        .append(errors.size()).append(" (out of ").append(invalidBlks.length)
-        .append(") replica(s):");
+              .append(errors.size()).append(" (out of ").append(total)
+              .append(") replica(s):");
       for(int i = 0; i < errors.size(); i++) {
         b.append("\n").append(i).append(") ").append(errors.get(i));
       }
@@ -1799,6 +1849,7 @@ private void cacheBlock(String bpid, long blockId) {
     FsVolumeImpl volume;
     String blockFileName;
     long length, genstamp;
+    short cloudBucketID;
     Executor volumeExecutor;
 
     synchronized (this) {
@@ -1837,10 +1888,11 @@ private void cacheBlock(String bpid, long blockId) {
       blockFileName = info.getBlockFile().getAbsolutePath();
       length = info.getVisibleLength();
       genstamp = info.getGenerationStamp();
+      cloudBucketID = info.getCloudBucketID();
       volumeExecutor = volume.getCacheExecutor();
     }
     cacheManager.cacheBlock(blockId, bpid, 
-        blockFileName, length, genstamp, volumeExecutor);
+        blockFileName, length, genstamp, cloudBucketID, volumeExecutor);
   }
 
   @Override // FsDatasetSpi
@@ -2018,8 +2070,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,
       if (memBlockInfo == null) {
         // Block is missing in memory - add the block to volumeMap
         ReplicaInfo diskBlockInfo =
-            new FinalizedReplica(blockId, diskFile.length(), diskGS, vol,
-                diskFile.getParentFile());
+            new FinalizedReplica(blockId, diskFile.length(), diskGS,
+                Block.NON_EXISTING_BUCKET_ID, vol, diskFile.getParentFile());
         volumeMap.add(bpid, diskBlockInfo);
         LOG.warn("Added missing block to memory " + diskBlockInfo);
         return;
@@ -2106,8 +2158,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,
    */
   @Override // FsDatasetSpi
   @Deprecated
-  public ReplicaInfo getReplica(String bpid, long blockId) {
-    return volumeMap.get(bpid, blockId);
+  public ReplicaInfo getReplica(ExtendedBlock b) {
+    return volumeMap.get(b.getBlockPoolId(), b.getBlockId());
   }
 
   @Override
@@ -2124,11 +2176,17 @@ public synchronized ReplicaRecoveryInfo initReplicaRecovery(
   }
 
   /**
-   * static version of {@link #initReplicaRecovery(RecoveringBlock)}.
+   * version of {@link #initReplicaRecovery(RecoveringBlock)}.
    */
-  static ReplicaRecoveryInfo initReplicaRecovery(String bpid, ReplicaMap map,
+  public ReplicaRecoveryInfo initReplicaRecovery(String bpid, ReplicaMap map,
       Block block, long recoveryId, long xceiverStopTimeout) throws IOException {
-    final ReplicaInfo replica = map.get(bpid, block.getBlockId());
+    ExtendedBlock exBlock = new ExtendedBlock(bpid,
+            block.getBlockId(),
+            block.getNumBytes(),
+            block.getGenerationStamp(),
+            block.getCloudBucketID()) ;
+    //calls the overridden method in case of cloud
+    final ReplicaInfo replica = getReplica(exBlock) ;
     LOG.info("initReplicaRecovery: " + block + ", recoveryId=" + recoveryId +
         ", replica=" + replica);
 
@@ -2226,7 +2284,7 @@ public synchronized String updateReplicaUnderRecovery(
     //update replica
     final FinalizedReplica finalized = updateReplicaUnderRecovery(oldBlock
         .getBlockPoolId(), (ReplicaUnderRecovery) replica, recoveryId,
-        newBlockId, newlength);
+        newBlockId, newlength, oldBlock.getCloudBucketID());
 
     boolean copyTruncate = newBlockId != oldBlock.getBlockId();
     if(!copyTruncate) {
@@ -2253,12 +2311,13 @@ public synchronized String updateReplicaUnderRecovery(
     return getVolume(new ExtendedBlock(bpid, finalized)).getStorageID();
   }
 
-  private FinalizedReplica updateReplicaUnderRecovery(
+  FinalizedReplica updateReplicaUnderRecovery(
                                           String bpid,
                                           ReplicaUnderRecovery rur,
                                           long recoveryId,
                                           long newBlockId,
-                                          long newlength) throws IOException {
+                                          long newlength,
+                                          short cloudBlockID) throws IOException {
     //check recovery id
     if (rur.getRecoveryID() != recoveryId) {
       throw new IOException(
@@ -2295,8 +2354,8 @@ private FinalizedReplica updateReplicaUnderRecovery(
         // Copying block to a new block with new blockId.
         // Not truncating original block.
         ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(
-            newBlockId, recoveryId, rur.getVolume(), blockFile.getParentFile(),
-            newlength);
+            newBlockId, recoveryId, cloudBlockID, rur.getVolume(),
+            blockFile.getParentFile(), newlength);
         newReplicaInfo.setNumBytesNoPersistance(newlength);
         volumeMap.add(bpid, newReplicaInfo);
         finalizeReplica(bpid, newReplicaInfo);
@@ -2619,4 +2678,15 @@ private void addDeletingBlock(String bpid, Long blockId) {
       s.add(blockId);
     }
   }
+
+
+  FsVolumeImpl getNewFsVolumeImpl(FsDatasetImpl dataset, String storageID, File currentDir,
+               Configuration conf, StorageType storageType) throws IOException {
+    return new FsVolumeImpl( this, storageID, currentDir, conf, storageType);
+  }
+
+  //only for testing
+  public ReplicaMap getVolumeMap() {
+    return volumeMap;
+  }
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java
index 757ad649776..cc16951c1fb 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java
@@ -161,8 +161,8 @@ private void printReferenceTraceInfo(String op) {
     }
     FsDatasetImpl.LOG.trace("Reference count: " + op + " " + this + ": " +
         this.reference.getReferenceCount());
-    FsDatasetImpl.LOG.trace(
-        Joiner.on("\n").join(Thread.currentThread().getStackTrace()));
+//    FsDatasetImpl.LOG.trace(
+//        Joiner.on("\n").join(Thread.currentThread().getStackTrace()));
   }
 
   /**
@@ -265,6 +265,10 @@ File getTmpDir(String bpid) throws IOException {
     return getBlockPoolSlice(bpid).getTmpDir();
   }
 
+  public File getCacheDir(String bpid) throws IOException{
+    return getBlockPoolSlice(bpid).getCacheDir();
+  }
+
   void decDfsUsed(String bpid, long value) {
     synchronized (dataset) {
       BlockPoolSlice bp = bpSlices.get(bpid);
@@ -341,7 +345,7 @@ long getReserved(){
     return reserved;
   }
 
-  BlockPoolSlice getBlockPoolSlice(String bpid) throws IOException {
+  public BlockPoolSlice getBlockPoolSlice(String bpid) throws IOException {
     BlockPoolSlice bp = bpSlices.get(bpid);
     if (bp == null) {
       throw new IOException("block pool " + bpid + " is not found");
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/HopsFsDatasetFactory.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/HopsFsDatasetFactory.java
new file mode 100644
index 00000000000..f9933189e21
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/HopsFsDatasetFactory.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.server.datanode.DataNode;
+import org.apache.hadoop.hdfs.server.datanode.DataStorage;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;
+
+import java.io.IOException;
+
+/**
+ * A factory for creating {@link FsDatasetImpl} objects.
+ */
+public class HopsFsDatasetFactory extends FsDatasetSpi.Factory<FsDatasetImpl> {
+  @Override
+  public FsDatasetImpl newInstance(DataNode datanode, DataStorage storage,
+      Configuration conf) throws IOException {
+    return new CloudFsDatasetImpl(datanode, storage, conf);
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedBlocksCacheCleaner.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedBlocksCacheCleaner.java
new file mode 100644
index 00000000000..d388a4a230d
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedBlocksCacheCleaner.java
@@ -0,0 +1,182 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileUtil;
+
+public class ProvidedBlocksCacheCleaner extends Thread {
+
+  static final Log LOG = LogFactory.getLog(ProvidedBlocksCacheCleaner.class);
+
+  private final File baseDir;
+  private final long checkInterval;
+  private final int activationTheshold;
+  private final int deleteBatchSize;
+  private final int waitBeforeDelete;
+  private Map<String, CachedProvidedBlock> cachedFiles = new ConcurrentHashMap<>();
+  private ProvidedBlocksCacheDiskUtilization diskUtilization;
+  private boolean run = true;
+
+  public ProvidedBlocksCacheCleaner(File baseDir, long checkInterval, int activationTheshold,
+                                    int deleteBatchSize, int waitBeforeDelete) {
+    this.baseDir = baseDir;
+    this.activationTheshold = activationTheshold;
+    this.checkInterval = checkInterval;
+    this.deleteBatchSize = deleteBatchSize;
+    this.diskUtilization = new ProvidedBlocksCacheDiskUtilization(baseDir);
+    this.waitBeforeDelete = waitBeforeDelete;
+
+    File[] files = baseDir.listFiles(); //flat dir structure
+    for (File file : files) {
+      if (file.isFile()) {
+        cachedFiles.put(file.getAbsolutePath(), new CachedProvidedBlock(file.getAbsolutePath(),
+                System.currentTimeMillis()));
+      }
+    }
+  }
+
+  public void fileAccessed(String file) {
+    cachedFiles.put(file, new CachedProvidedBlock(file,
+            System.currentTimeMillis()));
+    LOG.debug("HopsFS-Cloud. Provided blocks Cache. Added/Updated file: " + file+ " Total cached " +
+            "files: "+cachedFiles.size());
+  }
+
+  public void fileDeleted(String file) {
+    cachedFiles.remove(file);
+    LOG.debug("HopsFS-Cloud. Provided blocks Cache. Removed file: " + file);
+  }
+
+  @Override
+  public void run() {
+    try {
+      while (run) {
+        double percentage = diskUtilization.getDiskUtilization();
+        if (percentage >= activationTheshold) {
+          LOG.info("HopsFS-Cloud. Disk utilization is " + (long) percentage + ". " +
+                  "Freeing up space to make room for new blocks");
+          freeUpSpace();
+          Thread.sleep(100);
+          continue;
+        } else {
+          LOG.debug("HopsFS-Cloud. Provided bocks cache. No need to free up disk space. " +
+                  "Disk utilization: " + percentage + "%");
+        }
+        Thread.sleep(checkInterval);
+      }
+    } catch (InterruptedException e) {
+
+    }
+  }
+
+
+  public synchronized void freeUpSpace() {
+    LOG.debug("HopsFS-Cloud. Provided blocks cache. Total Files in cache are " + cachedFiles.size());
+    List<CachedProvidedBlock> list = new ArrayList<>(cachedFiles.values());
+    Collections.sort(list);
+
+    int counter = 0;
+    for (CachedProvidedBlock cBlk : list) {
+      String filePath = cBlk.path;
+      File file = new File(filePath);
+
+      long ts = file.lastModified();
+      if ((System.currentTimeMillis() - ts) < waitBeforeDelete) {
+        continue;  // do not delete recently downloaded files. It is possible that a clients
+        // are currently reading these files
+      }
+
+      boolean isFileUnlocked = false;
+      boolean deleted = false;
+      try {
+        org.apache.commons.io.FileUtils.touch(file); //managed to update TS of the file
+        // --> file is not open
+        isFileUnlocked = true;
+      } catch (IOException e) {
+        isFileUnlocked = false;
+      }
+
+      if (isFileUnlocked) {
+        if (file.delete()) {
+          fileDeleted(filePath);
+          counter++;
+          deleted = true;
+        }
+      }
+
+      if (!deleted) {
+        LOG.warn("HopsFS-Cloud. Provided blocks disk cache. Could not delete " + file.getName());
+        continue;
+      }
+
+      if (counter >= deleteBatchSize) {
+        break;
+      }
+    }
+  }
+
+  public void shutdown() {
+    run = false;
+    interrupt();
+  }
+
+  class CachedProvidedBlock implements Comparable<CachedProvidedBlock> {
+    private final String path;
+    private final long time;
+
+    public CachedProvidedBlock(String path, long time) {
+      this.path = path;
+      this.time = time;
+    }
+
+    public long getTime() {
+      return time;
+    }
+
+    public String getPath() {
+      return path;
+    }
+
+    @Override
+    public int compareTo(CachedProvidedBlock o) {
+      return new Long(time).compareTo(o.time);
+    }
+  }
+
+  public int getCachedFilesCount() {
+    return cachedFiles.size();
+  }
+
+  public ProvidedBlocksCacheDiskUtilization getDiskUtilizationCalc() {
+    return diskUtilization;
+  }
+
+  public void setDiskUtilizationMock(ProvidedBlocksCacheDiskUtilization mock) {
+    diskUtilization = mock;
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedBlocksCacheDiskUtilization.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedBlocksCacheDiskUtilization.java
new file mode 100644
index 00000000000..20982934b3e
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ProvidedBlocksCacheDiskUtilization.java
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import java.io.File;
+
+public class ProvidedBlocksCacheDiskUtilization {
+  private final File baseDir;
+
+  public ProvidedBlocksCacheDiskUtilization(File baseDir) {
+    this.baseDir = baseDir;
+  }
+
+  public double getDiskUtilization() {
+    long freeSpace = baseDir.getUsableSpace();
+    long totalSpace = baseDir.getTotalSpace();
+    long consumed = (totalSpace - freeSpace);
+    double percentage = ((double) consumed / (double) totalSpace) * 100.0;
+    return percentage;
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/SemaphoredDelegatingExecutor.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/SemaphoredDelegatingExecutor.java
new file mode 100644
index 00000000000..cde7242d503
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/SemaphoredDelegatingExecutor.java
@@ -0,0 +1,228 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs.server.datanode.fsdataset.impl;
+
+import com.google.common.util.concurrent.ForwardingListeningExecutorService;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+import com.google.common.util.concurrent.ListeningExecutorService;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+import java.util.concurrent.Semaphore;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+/**
+ * This ExecutorService blocks the submission of new tasks when its queue is
+ * already full by using a semaphore. Task submissions require permits, task
+ * completions release permits.
+ * <p>
+ * This is a refactoring of {@link BlockingThreadPoolExecutorService}; that code
+ * contains the thread pool logic, whereas this isolates the semaphore
+ * and submit logic for use with other thread pools and delegation models.
+ * <p>
+ * This is inspired by <a href="https://github.com/apache/incubator-s4/blob/master/subprojects/s4-comm/src/main/java/org/apache/s4/comm/staging/BlockingThreadPoolExecutorService.java">
+ * this s4 threadpool</a>
+ */
+@SuppressWarnings("NullableProblems")
+@InterfaceAudience.Private
+public class SemaphoredDelegatingExecutor extends
+        ForwardingListeningExecutorService {
+
+  private final Semaphore queueingPermits;
+  private final ListeningExecutorService executorDelegatee;
+  private final int permitCount;
+
+  /**
+   * Instantiate.
+   * @param executorDelegatee Executor to delegate to
+   * @param permitCount number of permits into the queue permitted
+   * @param fair should the semaphore be "fair"
+   */
+  public SemaphoredDelegatingExecutor(
+          ListeningExecutorService executorDelegatee,
+          int permitCount,
+          boolean fair) {
+    this.permitCount = permitCount;
+    queueingPermits = new Semaphore(permitCount, fair);
+    this.executorDelegatee = executorDelegatee;
+  }
+
+  @Override
+  protected ListeningExecutorService delegate() {
+    return executorDelegatee;
+  }
+
+
+  @Override
+  public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)
+          throws InterruptedException {
+    throw new RuntimeException("Not implemented");
+  }
+
+  @Override
+  public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks,
+                                       long timeout, TimeUnit unit) throws InterruptedException {
+    throw new RuntimeException("Not implemented");
+  }
+
+  @Override
+  public <T> T invokeAny(Collection<? extends Callable<T>> tasks)
+          throws InterruptedException, ExecutionException {
+    throw new RuntimeException("Not implemented");
+  }
+
+  @Override
+  public <T> T invokeAny(Collection<? extends Callable<T>> tasks, long timeout,
+                         TimeUnit unit)
+          throws InterruptedException, ExecutionException, TimeoutException {
+    throw new RuntimeException("Not implemented");
+  }
+
+  @Override
+  public <T> ListenableFuture<T> submit(Callable<T> task) {
+    try {
+      queueingPermits.acquire();
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      return Futures.immediateFailedCheckedFuture(e);
+    }
+    return super.submit(new CallableWithPermitRelease<>(task));
+  }
+
+  @Override
+  public <T> ListenableFuture<T> submit(Runnable task, T result) {
+    try {
+      queueingPermits.acquire();
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      return Futures.immediateFailedCheckedFuture(e);
+    }
+    return super.submit(new RunnableWithPermitRelease(task), result);
+  }
+
+  @Override
+  public ListenableFuture<?> submit(Runnable task) {
+    try {
+      queueingPermits.acquire();
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      return Futures.immediateFailedCheckedFuture(e);
+    }
+    return super.submit(new RunnableWithPermitRelease(task));
+  }
+
+  @Override
+  public void execute(Runnable command) {
+    try {
+      queueingPermits.acquire();
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+    }
+    super.execute(new RunnableWithPermitRelease(command));
+  }
+
+  /**
+   * Get the number of permits available; guaranteed to be
+   * {@code 0 <= availablePermits <= size}.
+   * @return the number of permits available at the time of invocation.
+   */
+  public int getAvailablePermits() {
+    return queueingPermits.availablePermits();
+  }
+
+  /**
+   * Get the number of threads waiting to acquire a permit.
+   * @return snapshot of the length of the queue of blocked threads.
+   */
+  public int getWaitingCount() {
+    return queueingPermits.getQueueLength();
+  }
+
+  /**
+   * Total number of permits.
+   * @return the number of permits as set in the constructor
+   */
+  public int getPermitCount() {
+    return permitCount;
+  }
+
+  @Override
+  public String toString() {
+    final StringBuilder sb = new StringBuilder(
+            "SemaphoredDelegatingExecutor{");
+    sb.append("permitCount=").append(getPermitCount());
+    sb.append(", available=").append(getAvailablePermits());
+    sb.append(", waiting=").append(getWaitingCount());
+    sb.append('}');
+    return sb.toString();
+  }
+
+  /**
+   * Releases a permit after the task is executed.
+   */
+  class RunnableWithPermitRelease implements Runnable {
+
+    private Runnable delegatee;
+
+    RunnableWithPermitRelease(Runnable delegatee) {
+      this.delegatee = delegatee;
+    }
+
+    @Override
+    public void run() {
+      try {
+        delegatee.run();
+      } finally {
+        queueingPermits.release();
+      }
+
+    }
+  }
+
+  /**
+   * Releases a permit after the task is completed.
+   */
+  class CallableWithPermitRelease<T> implements Callable<T> {
+
+    private Callable<T> delegatee;
+
+    CallableWithPermitRelease(Callable<T> delegatee) {
+      this.delegatee = delegatee;
+    }
+
+    @Override
+    public T call() throws Exception {
+      try {
+        return delegatee.call();
+      } finally {
+        queueingPermits.release();
+      }
+    }
+
+  }
+
+}
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java
index 4a5195e7520..b6414655500 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java
@@ -277,7 +277,8 @@ public Object performTask() throws IOException {
         INode targetNode = iip.getLastINode();
         // [s] for the files stored in the database setting the replication level does not make
         // any sense. For now we will just set the replication level as requested by the user
-        if (isFile && !((INodeFile) targetNode).isFileStoredInDB()) {
+        if (isFile && (!((INodeFile) targetNode).isFileStoredInDB() &&
+                targetNode.getStoragePolicyID() != HdfsConstants.CLOUD_STORAGE_POLICY_ID)) {
           bm.setReplication(blockRepls[0], blockRepls[1], src, blocks);
         }
         return isFile;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
index 79a276aeaac..fe432820f00 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java
@@ -37,6 +37,7 @@
 import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.fs.StorageType;
 import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;
+import org.apache.hadoop.hdfs.protocol.HdfsConstants;
 import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;
 import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous;
 import org.apache.hadoop.hdfs.protocol.QuotaExceededException;
@@ -163,6 +164,8 @@ private static void verifyTargetFile(FSDirectory fsd, final String target,
     final INodeFile targetINode = targetIIP.getLastINode().asFile();
     final INodeDirectory targetParent = targetINode.getParent();
     // now check the srcs
+    int diskFiles = 0;
+    int cloudFiles = 0;
     for (String src : srcs) {
       final INodesInPath iip = fsd.getINodesInPath4Write(src);
       // permission check for srcs
@@ -184,6 +187,26 @@ private static void verifyTargetFile(FSDirectory fsd, final String target,
         throw new HadoopIllegalArgumentException("concat: the src file " + src
             + " is the same with the target file " + targetIIP.getPath());
       }
+
+      if(srcINodeFile.getStoragePolicyID() == HdfsConstants.DB_STORAGE_POLICY_ID) {
+        throw new HadoopIllegalArgumentException("concat: source file " + src
+                + " is stored in DB.");
+      }
+
+      if(srcINodeFile.getStoragePolicyID() == HdfsConstants.CLOUD_STORAGE_POLICY_ID){
+        cloudFiles++;
+      } else {
+        diskFiles++;
+      }
+
+
+      //for CLOUD storage policy all file should have same storage policy
+      //we can not mix different storage policies.
+      if(cloudFiles > 1 && diskFiles > 1) {
+        throw new HadoopIllegalArgumentException("concat: some src files are stored on" +
+                " DN disks and some are stored in the cloud");
+      }
+
       // source file cannot be under construction or empty
       if(srcINodeFile.isUnderConstruction() || srcINodeFile.numBlocks() == 0) {
         throw new HadoopIllegalArgumentException("concat: source file " + src
@@ -285,6 +308,7 @@ static void unprotectedConcat(FSDirectory fsd, INodesInPath targetIIP,
     }
 
     trgInode.setModificationTime(timestamp);
+    trgInode.setHasBlocks(true);
     trgParent.updateModificationTime(timestamp);
     // update quota on the parent directory ('count' files removed, 0 space)
     fsd.unprotectedUpdateCount(targetIIP, targetIIP.length() - 1, deltas);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
index 53e0ce6dc66..6df51e81803 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java
@@ -64,6 +64,8 @@
 import org.apache.hadoop.hdfs.protocol.HdfsConstantsClient;
 import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;
 import org.apache.hadoop.hdfs.protocol.QuotaExceededException;
+import org.apache.hadoop.hdfs.protocol.FSLimitException.MaxDirectoryItemsExceededException;
+import org.apache.hadoop.hdfs.protocol.FSLimitException.PathComponentTooLongException;
 import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous;
 import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguousUnderConstruction;
 import org.apache.hadoop.hdfs.protocol.proto.HdfsProtos;
@@ -178,8 +180,21 @@
 
     namesystem = ns;
 
+    BlockStoragePolicySuite storagePolicySuite = BlockStoragePolicySuite.createDefaultSuite();
+    byte storagePolicy;
+    boolean storagePolicyEnabled = conf.getBoolean(DFSConfigKeys.DFS_STORAGE_POLICY_ENABLED_KEY,
+            DFS_STORAGE_POLICY_ENABLED_DEFAULT);
+    boolean cloudStrageEnabled = conf.getBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE,
+            DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE_DEFAULT);
+    if (storagePolicyEnabled && cloudStrageEnabled) {
+      storagePolicy = storagePolicySuite.getPolicy(HdfsConstants.CLOUD_STORAGE_POLICY_NAME).getId();
+    } else {
+      storagePolicy = HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED;
+    }
+
     createRoot(ns.createFsOwnerPermissions(new FsPermission((short) 0755)),
-        false /*dont overwrite if root inode already existes*/);
+        false /*dont overwrite if root inode already existes*/,
+            storagePolicy);
     this.isPermissionEnabled = conf.getBoolean(
       DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY,
       DFSConfigKeys.DFS_PERMISSIONS_ENABLED_DEFAULT);
@@ -342,7 +357,7 @@ INodesInPath addFile(INodesInPath existing, String localName, PermissionStatus
     throws IOException {
 
     long modTime = now();
-    
+
     INodeFile newNode = new INodeFile(IDsGeneratorFactory.getInstance().getUniqueINodeID(), permissions,
         BlockInfoContiguous.EMPTY_ARRAY, replication, modTime, modTime, preferredBlockSize, (byte) 0);
     newNode.setLocalNameNoPersistance(localName.getBytes(Charsets.UTF_8));
@@ -360,6 +375,7 @@ INodesInPath addFile(INodesInPath existing, String localName, PermissionStatus
     if (NameNode.stateChangeLog.isDebugEnabled()) {
       NameNode.stateChangeLog.debug("DIR* addFile: " + localName + " is added");
     }
+
     return newiip;
   }
 
@@ -1167,7 +1183,7 @@ public Object performTask() throws StorageException, IOException {
   void reset() throws IOException {
     createRoot(
         namesystem.createFsOwnerPermissions(new FsPermission((short) 0755)),
-        true);    
+        true, HdfsConstantsClient.BLOCK_STORAGE_POLICY_ID_UNSPECIFIED);
     // addToInodeMap(rootDir) is only adding encryption zones and no zone is created at this point.
     nameCache.reset();
   }  
@@ -1666,7 +1682,7 @@ public boolean isQuotaEnabled() {
   
   //add root inode if its not there
   public INodeDirectory createRoot(
-      final PermissionStatus ps, final boolean overwrite) throws IOException {
+          final PermissionStatus ps, final boolean overwrite, final byte defaultStoragePolicy ) throws IOException {
     LightWeightRequestHandler addRootINode =
         new LightWeightRequestHandler(HDFSOperationType.SET_ROOT) {
           @Override
@@ -1679,6 +1695,7 @@ public Object performTask() throws IOException {
                     HdfsConstantsClient.GRANDFATHER_INODE_ID, INodeDirectory.getRootDirPartitionKey());
             if (rootInode == null || overwrite == true) {
               newRootINode = INodeDirectory.createRootDir(ps);
+              newRootINode.setBlockStoragePolicyIDNoPersistance(defaultStoragePolicy);
 
               DirectoryWithQuotaFeature quotaFeature = new DirectoryWithQuotaFeature.Builder(newRootINode.getId()).
                   nameSpaceQuota(DirectoryWithQuotaFeature.DEFAULT_NAMESPACE_QUOTA).
@@ -1690,7 +1707,6 @@ public Object performTask() throws IOException {
               
               newRootINode.addDirectoryWithQuotaFeature(quotaFeature);
               
-              // Set the block storage policy to DEFAULT
               List<INode> newINodes = new ArrayList();
               newINodes.add(newRootINode);
               da.prepare(INode.EMPTY_LIST, newINodes, INode.EMPTY_LIST);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
index 4ac673042cc..4c7482cb00b 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
@@ -165,20 +165,15 @@
 import org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier;
 import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;
 import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager;
-import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous;
-import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguousUnderConstruction;
-import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;
-import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;
-import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager;
-import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStatistics;
-import org.apache.hadoop.hdfs.server.blockmanagement.HashBuckets;
-import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo;
+import org.apache.hadoop.hdfs.server.blockmanagement.*;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.BlockUCState;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.RollingUpgradeStartupOption;
 import org.apache.hadoop.hdfs.server.common.Storage;
 import org.apache.hadoop.hdfs.server.common.StorageInfo;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudPersistenceProviderFactory;
 import org.apache.hadoop.hdfs.server.namenode.INode.BlocksMapUpdateInfo;
 import org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean;
 import org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics;
@@ -258,7 +253,6 @@
 import org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos;
 import org.apache.hadoop.hdfs.protocol.proto.HdfsProtos;
 import org.apache.hadoop.hdfs.protocolPB.PBHelper;
-import org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection;
 import org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase;
 import org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;
 import org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress.Counter;
@@ -495,6 +489,10 @@ private void logAuditEvent(boolean succeeded, UserGroupInformation ugi,
   private final TopConf topConf;
   private TopMetrics topMetrics;
 
+  // HopsFS Cloud Storage
+  // Max buckets
+  private final int MAX_CLOUD_BUCKETS;
+
   /**
    * Notify that loading of this FSDirectory is complete, and
    * it is imageLoaded for use
@@ -630,6 +628,9 @@ private FSNamesystem(Configuration conf, NameNode namenode, boolean ignoreRetryC
         throw new IllegalArgumentException("The size for the database files is not correctly set");
       }
 
+      MAX_CLOUD_BUCKETS = conf.getInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS,
+              DFS_CLOUD_AWS_S3_NUM_BUCKETS_DEFAULT);
+
       this.datanodeStatistics =
           blockManager.getDatanodeManager().getDatanodeStatistics();
 
@@ -857,6 +858,10 @@ void startCommonServices(Configuration conf) throws IOException {
     RootINodeCache.start();
     nnResourceChecker = new NameNodeResourceChecker(conf);
     checkAvailableResources();
+
+    boolean cloud = conf.getBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE,
+            DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE_DEFAULT);
+
     if (isLeader()) {
       // the node is starting and directly leader, this means that no NN was alive before
       clearSafeBlocks();
@@ -869,9 +874,22 @@ void startCommonServices(Configuration conf) throws IOException {
       prog.setTotal(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS,
           getCompleteBlocksTotal());
       setBlockTotal();
+
+      if (cloud) {
+        //delete all BR tasks
+        ProvidedBlocksChecker.deleteAllTask();
+        //schedule new BR
+        ProvidedBlocksChecker.scheduleBlockReportNow();
+      }
     }
+
     shouldPopulateReplicationQueue = true;
     blockManager.activate(conf);
+
+    if (cloud) {
+      blockManager.startProvidedBlocksChecker(conf);
+    }
+
     if (dir.isQuotaEnabled()) {
       quotaUpdateManager.activate();
     }
@@ -905,6 +923,15 @@ void startActiveServices() throws IOException {
     LOG.info("Starting services required for active state");
     LOG.info("Catching up to latest edits from old active before " + "taking over writer role in edits logs");
     try {
+      boolean cloud = conf.getBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE,
+              DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE_DEFAULT);
+      if(cloud){
+        CloudPersistenceProvider cloudConnector =
+                CloudPersistenceProviderFactory.getCloudClient(conf);
+        cloudConnector.checkAllBuckets();
+        cloudConnector.shutdown();
+      }
+
       blockManager.getDatanodeManager().markAllDatanodesStale();
 
       // Only need to re-process the queue, If not in SafeMode.
@@ -1764,7 +1791,8 @@ boolean truncateInternal(String src, long newLength,
       System.arraycopy(oldData, 0, newData, 0, newLengthInt);
       file.deleteFileDataStoredInDB();
       file.storeFileDataInDB(newData);
-      return onBlockBoundary;
+      file.setSize(newData.length);
+      return true; //truncate is ready
     }
     if(!onBlockBoundary) {
       // Open file for write, but don't log into edits
@@ -1804,10 +1832,11 @@ Block prepareFileForTruncate(INodesInPath iip,
         file.getFileUnderConstructionFeature().getClientName(), src);
     boolean shouldRecoverNow = (newBlock == null);
     BlockInfoContiguous oldBlock = file.getLastBlock();
-    boolean shouldCopyOnTruncate = shouldCopyOnTruncate(file, oldBlock);
+    boolean shouldCopyOnTruncate = shouldCopyOnTruncate(file, oldBlock); //always false for HopsFS
     if(newBlock == null) {
       newBlock = (shouldCopyOnTruncate) ? createNewBlock(file) :
-          new Block(oldBlock.getBlockId(), oldBlock.getNumBytes(), file.nextGenerationStamp());
+          new Block(oldBlock.getBlockId(), oldBlock.getNumBytes(), file.nextGenerationStamp(),
+                  oldBlock.getCloudBucketID());
     }
 
     BlockInfoContiguousUnderConstruction truncatedBlockUC;
@@ -2148,14 +2177,35 @@ private HdfsFileStatus startFileInt(String srcArg, String src, PermissionStatus
     final INodesInPath iip = dir.getINodesInPath4Write(src);
     startFileInternal(pc, iip, permissions, holder, clientMachine, create, overwrite,
         createParent, replication, blockSize, suite, version, edek);
-    final HdfsFileStatus stat = FSDirStatAndListingOp.
+     HdfsFileStatus stat = FSDirStatAndListingOp.
         getFileInfo(dir, src, false, FSDirectory.isReservedRawName(srcArg), true);
 
-    //Set HdfsFileStatus if the file shoudl be stored in DB
-    final INode inode = dir.getINodesInPath4Write(src).getLastINode();
-    final INodeFile myFile = INodeFile.valueOf(inode, src, true);
-    final BlockStoragePolicy storagePolicy =
-            getBlockManager().getStoragePolicySuite().getPolicy(myFile.getStoragePolicyID());
+    // HopsFS-Cloud
+    // ============
+    // For new files if the parent storage policy is CLOUD then
+    // we override the storage policy to DB. The client will first try
+    // to write the file to DB. If the file is large then the client will request
+    // for a block allocation and the file will be written accoring to the parent
+    // storage policy
+    if(stat.getStoragePolicy() ==  HdfsConstants.CLOUD_STORAGE_POLICY_ID){
+      LOG.debug("HopsFS-Cloud. Override Storage policy. CLOUD -> DB");
+      stat = new HdfsFileStatus(
+              stat.getLen(),
+              stat.isDir(),
+              stat.getReplication(),
+              stat.getBlockSize(),
+              stat.getModificationTime(),
+              stat.getAccessTime(),
+              stat.getPermission(),
+              stat.getOwner(),
+              stat.getGroup(),
+              stat.getSymlinkInBytes(),
+              stat.getLocalNameInBytes(),
+              stat.getFileId(),
+              stat.getChildrenNum(),
+              stat.getFileEncryptionInfo(),
+              HdfsConstants.DB_STORAGE_POLICY_ID);
+    }
 
     logAuditEvent(true, "create", src, null,
         (isAuditEnabled() && isExternalInvocation()) ? stat : null);
@@ -2321,9 +2371,17 @@ private LocatedBlock appendFileInternal(FSPermissionChecker pc,
       
       final BlockInfoContiguous lastBlock = myFile.getLastBlock();
       // Check that the block has at least minimum replication.
-      if (lastBlock != null && lastBlock.isComplete() && !getBlockManager().isSufficientlyReplicated(lastBlock)) {
-        throw new IOException("append: lastBlock=" + lastBlock + " of src=" + src
-            + " is not sufficiently replicated yet.");
+      if( lastBlock != null ) {
+        if (!lastBlock.isProvidedBlock() &&
+                lastBlock.isComplete() &&
+                !getBlockManager().isSufficientlyReplicated(lastBlock)) {
+          throw new IOException("append: lastBlock=" + lastBlock + " of src=" + src
+                  + " is not sufficiently replicated yet.");
+        } else if (lastBlock.isProvidedBlock() &&
+                !lastBlock.isComplete()) {
+          throw new IOException("append: lastBlock=" + lastBlock + " of src=" + src
+                  + " is not complete yet.");
+        }
       }
       return prepareFileForAppend(src, iip, holder, clientMachine, newBlock);
     } catch (IOException ie) {
@@ -2364,6 +2422,11 @@ LocatedBlock prepareFileForAppend(String src, INodesInPath iip,
       return blockManager.createPhantomLocatedBlocks(file, file.getFileDataInDB(), true, false, null).getLocatedBlocks().
           get(0);
     } else {
+
+      if ( file.getStoragePolicyID() == HdfsConstants.CLOUD_STORAGE_POLICY_ID ){
+        newBlock = true;  //force create new blocks on append for CLOUD storage policy
+      }
+
       LocatedBlock ret = null;
       if (!newBlock) {
         ret = blockManager.convertLastBlockToUnderConstruction(file, 0);
@@ -2867,10 +2930,19 @@ public Object performTask() throws IOException {
 
     FileState fileState = analyzeFileState(src, fileId, clientName, previous, onRetryBlock);
     INodeFile pendingFile = fileState.inode;
+    boolean storedInCloud =
+            pendingFile.getStoragePolicyID() == HdfsConstants.CLOUD_STORAGE_POLICY_ID;
+
     // Check if the penultimate block is minimally replicated
-    if (!checkFileProgress(src, pendingFile, false)) {
+    boolean checkFullFile = false;
+    if(storedInCloud){
+      checkFullFile = true;
+    }
+
+    if (!checkFileProgress(src, pendingFile, checkFullFile)) {
       throw new NotReplicatedYetException("Not replicated yet: " + src);
     }
+
     src = fileState.path;
 
     if (onRetryBlock[0] != null && onRetryBlock[0].getLocations().length > 0) {
@@ -2894,22 +2966,32 @@ public Object performTask() throws IOException {
     replication = pendingFile.getBlockReplication();
 
     // Get the storagePolicyID of this file
-    byte storagePolicyID = pendingFile.getStoragePolicyID();
-
-    if(getBlockManager().getStoragePolicySuite().getPolicy(storagePolicyID).getName()
-            .equals(HdfsConstants.DB_STORAGE_POLICY_NAME)){
+    if(pendingFile.getStoragePolicyID()== HdfsConstants.DB_STORAGE_POLICY_ID){
       //Policy is DB but the file is moved to DNs so change the policy for this file
-      pendingFile.setStoragePolicyID(getBlockManager().getStoragePolicySuite()
-              .getDefaultPolicy().getId());
+      if( pendingFile.getParent().getStoragePolicyID() == HdfsConstants.DB_STORAGE_POLICY_ID ){
+        pendingFile.setStoragePolicyID(getBlockManager().getStoragePolicySuite()
+                .getDefaultPolicy().getId());
+      } else {
+        pendingFile.setStoragePolicyID(pendingFile.getParent().getStoragePolicyID());
+        storedInCloud = pendingFile.getStoragePolicyID() ==
+                HdfsConstants.CLOUD_STORAGE_POLICY_ID;
+      }
     }
 
     if (clientNode == null) {
       clientNode = getClientNode(clientMachine);
     }
     // choose targets for the new block to be allocated.
+    if(storedInCloud){
+      //so that we choose only one target. The client writes to
+      //target datanode and then the datanode uploads the block to the cloud.
+      // no need to write to multiple datanodes
+      replication = 1;
+    }
+
     return getBlockManager().chooseTarget4NewBlock(
         src, replication, clientNode, excludedNodes, blockSize,
-        favoredNodes, storagePolicyID);
+        favoredNodes, pendingFile.getStoragePolicyID());
   }
       
               /**
@@ -2948,6 +3030,10 @@ LocatedBlock storeAllocatedBlock(String src, long fileId, String clientName,
 
     // allocate new block, record block locations in INode.
     newBlock = createNewBlock(pendingFile);
+    if(pendingFile.getStoragePolicyID() == HdfsConstants.CLOUD_STORAGE_POLICY_ID) {
+      ProvidedBlocksCacheHelper.updateProvidedBlockCacheLocation(newBlock, targets);
+    }
+
     INodesInPath inodesInPath = INodesInPath.fromINode(pendingFile);
     saveAllocatedBlock(src, inodesInPath, newBlock, targets);
 
@@ -3486,6 +3572,7 @@ private boolean completeFileStoredInDataBase(String src, String holder, long fil
     }
 
     pendingFile = checkLease(src, holder, inode, fileId, true);
+    pendingFile.setStoragePolicyID(HdfsConstants.DB_STORAGE_POLICY_ID);
 
     //in case of appending to small files. we might have to migrate the file from
     //in-memory to on disk
@@ -3510,7 +3597,7 @@ private boolean completeFileStoredInDataBase(String src, String holder, long fil
     }
 
 
-    finalizeINodeFileUnderConstructionStoredInDB(src, pendingFile);
+    finalizeINodeFileUnderConstruction(src, pendingFile);
 
     NameNode.stateChangeLog
         .info("DIR* completeFile: " + src + " is closed by " + holder);
@@ -3541,8 +3628,15 @@ private void saveAllocatedBlock(String src, INodesInPath inodesInPath,
    */
   private Block createNewBlock(INodeFile pendingFile)
       throws IOException {
-    Block b = new Block(nextBlockId()
-        , 0, 0);
+    short bucketID = Block.NON_EXISTING_BUCKET_ID;
+    long blockID = nextBlockId();
+
+    if(pendingFile.getStoragePolicyID() == HdfsConstants.CLOUD_STORAGE_POLICY_ID){
+      bucketID = Block.getCloudBucket(MAX_CLOUD_BUCKETS, blockID);
+      LOG.debug("HopsFS-Cloud. The new block ID: "+blockID+" for file: \""+pendingFile.getName()+
+                      "\" will be stored in cloud bucket: "+bucketID);
+    }
+    Block b = new Block(blockID, 0, 0, bucketID);
     // Increment the generation stamp for every new block.
     b.setGenerationStampNoPersistance(pendingFile.nextGenerationStamp());
     return b;
@@ -3853,9 +3947,26 @@ public Object performTask() throws IOException {
           }
         }
         final INodeFile pendingFile = checkLease(src2, clientName, inode, fileId, true);
+        
         if (lastBlockLength > 0) {
-          pendingFile.getFileUnderConstructionFeature().updateLengthOfLastBlock(pendingFile, lastBlockLength);
+          if (pendingFile.getStoragePolicyID() == HdfsConstants.CLOUD_STORAGE_POLICY_ID){
+            BlockInfoContiguous lastBlock = pendingFile.getLastBlock();
+            if(lastBlock.getBlockUCState() == BlockUCState.COMPLETE  ) {
+              //for provided blocks there is a race condition between
+              //FSYNC and BLOCK_RECEIVED_AND_DELETED.
+              //BLOCK_RECEIVED_AND_DELETED COMPLETS the blocks and updates the
+              //length making updating the block length in FSYNC redundant. Plus
+              //assert statements in  updateLengthOfLastBlock fail for completed blocks.
+              //for COMPLETED provided blocks there is no need to re-update the length.
+              //TestsS3FileCreation.TestDataRace
+              assert lastBlock.getNumBytes() == lastBlockLength;
+            }
+          } else {
+            pendingFile.getFileUnderConstructionFeature().
+                    updateLengthOfLastBlock(pendingFile, lastBlockLength);
+          }
         }
+
         persistBlocks(src2, pendingFile);
         pendingFile.recomputeFileSize();
         return null;
@@ -4027,22 +4138,46 @@ private Lease reassignLeaseInternal(Lease lease, String src, String newHolder,
   }
 
   private void commitOrCompleteLastBlock(
-      final INodeFile fileINode, final INodesInPath iip, final Block commitBlock)
-      throws IOException {
+          final INodeFile fileINode, final INodesInPath iip, final Block commitBlock)
+          throws IOException {
     Preconditions.checkArgument(fileINode.isUnderConstruction());
-    if (!blockManager.commitOrCompleteLastBlock(fileINode, commitBlock)) {
+
+    // For provided block the process of marking block COMITTED/COMPLETED
+    // is handled by the block manger when
+    // it receives the FINALIZED notification from the datanodes.
+    if (commitBlock != null && commitBlock.isProvidedBlock()) {
+      LOG.debug("HopsFS-Cloud. FSnamesystem CommitOrComplete for file: "+fileINode.getName()+
+              " BlockID: "+commitBlock.getBlockId()+" GenStamp: "+commitBlock.getGenerationStamp());
+      BlockInfoContiguous lastBlock = fileINode.getLastBlock();
+      if (lastBlock == null)
+        return;
+
+      //if we get here and blocks are not complete then the
+      //checkFileProgress fn should throw an exception to the client
+
+      updateQuotaUponBlockCompletion(fileINode, iip, commitBlock);
+
       return;
+    } else {
+      if (!blockManager.commitOrCompleteLastBlock(fileINode, commitBlock)) {
+        return;
+      }
+      updateQuotaUponBlockCompletion(fileINode, iip, commitBlock);
     }
 
+  }
+
+  public void updateQuotaUponBlockCompletion(final INodeFile fileINode, final INodesInPath iip,
+                                             final Block commitBlock) throws IOException {
     fileINode.recomputeFileSize();
 
     if (dir.isQuotaEnabled()) {
       final long diff = fileINode.getPreferredBlockSize()
-          - commitBlock.getNumBytes();
+              - commitBlock.getNumBytes();
       if (diff > 0) {
         // Adjust disk space consumption if required
         dir.updateSpaceConsumed(iip, 0,
-            -diff, fileINode.getBlockReplication());
+                -diff, fileINode.getBlockReplication());
       }
     }
   }
@@ -4050,13 +4185,13 @@ private void commitOrCompleteLastBlock(
   private void finalizeINodeFileUnderConstruction(String src,
       INodeFile pendingFile)
       throws IOException {
-    finalizeINodeFileUnderConstructionInternal(src, pendingFile, false);
-  }
+    boolean skipReplicationCheck = false;
+    if (pendingFile.getStoragePolicyID() == HdfsConstants.CLOUD_STORAGE_POLICY_ID ||
+         pendingFile.getStoragePolicyID() == HdfsConstants.DB_STORAGE_POLICY_ID   ){
+      skipReplicationCheck = true;
+    }
 
-  private void finalizeINodeFileUnderConstructionStoredInDB(String src,
-      INodeFile pendingFile)
-      throws IOException {
-    finalizeINodeFileUnderConstructionInternal(src, pendingFile, true);
+    finalizeINodeFileUnderConstructionInternal(src, pendingFile, skipReplicationCheck);
   }
 
   private void finalizeINodeFileUnderConstructionInternal(String src,
@@ -4065,7 +4200,7 @@ private void finalizeINodeFileUnderConstructionInternal(String src,
     FileUnderConstructionFeature uc = pendingFile.getFileUnderConstructionFeature();
     Preconditions.checkArgument(uc != null);
     leaseManager.removeLease(uc.getClientName(), src);
-    
+
     // close file and persist block allocations for this file
     pendingFile.toCompleteFile(now());
     closeFile(src, pendingFile);
@@ -4111,14 +4246,13 @@ public Object performTask() throws IOException {
         // If a DN tries to commit to the standby, the recovery will
         // fail, and the next retry will succeed on the new NN.
 
-        checkNameNodeSafeMode(
-          "Cannot commitBlockSynchronization while in safe mode");
-        LOG.info("commitBlockSynchronization(oldBlock=" + oldBlock +
-            ", newGenerationStamp=" + newGenerationStamp + ", newLength=" +
-            newLength + ", newTargets=" + Arrays.asList(newTargets) +
-            ", closeFile=" + closeFile + ", deleteBlock=" + deleteBlock + ")");
-        final BlockInfoContiguous storedBlock =
-            getStoredBlock(ExtendedBlock.getLocalBlock(oldBlock));
+        if (inodeIdentifier == null) {
+          throw new FileNotFoundException("File not found for block: " + oldBlock
+                  + ", likely due to delayed block"
+                  + " removal");
+        }
+
+        final BlockInfoContiguous storedBlock = getStoredBlock(ExtendedBlock.getLocalBlock(oldBlock));
         if (storedBlock == null) {
           if (deleteBlock) {
             // This may be a retry attempt so ignore the failure
@@ -4131,6 +4265,47 @@ public Object performTask() throws IOException {
             throw new IOException("Block (=" + oldBlock + ") not found");
           }
         }
+
+        BlockCollection blockCollection = storedBlock.getBlockCollection();
+        if (blockCollection == null) {
+          throw new IOException("The blockCollection of " + storedBlock
+                  + " is null, likely because the file owning this block was"
+                  + " deleted and the block removal is delayed");
+        }
+        INodeFile iFile = ((INode)blockCollection).asFile();
+
+        checkNameNodeSafeMode(
+                "Cannot commitBlockSynchronization while in safe mode");
+        LOG.info("commitBlockSynchronization(oldBlock=" + oldBlock +
+                ", newGenerationStamp=" + newGenerationStamp + ", newLength=" +
+                newLength + ", newTargets=" + Arrays.asList(newTargets) +
+                ", closeFile=" + closeFile + ", deleteBlock=" + deleteBlock + ")");
+
+        if(iFile.getStoragePolicyID() == HdfsConstants.CLOUD_STORAGE_POLICY_ID) {
+          commitBlockSynchronizationInternalProvidedBlks(oldBlock, newGenerationStamp, newLength,
+                  closeFile, deleteBlock, newTargets, newTargetStorages);
+        } else {
+          commitBlockSynchronizationInternal(oldBlock, newGenerationStamp, newLength,
+                  closeFile, deleteBlock, newTargets, newTargetStorages,
+                  src, copyTruncate, truncatedBlock);
+        }
+        return null;
+      }
+    }.handle(this);
+  }
+
+  void commitBlockSynchronizationInternal(final ExtendedBlock oldBlock,
+                                  final long newGenerationStamp, final long newLength,
+                                  final boolean closeFile, final boolean deleteBlock,
+                                  final DatanodeID[] newTargets, final String[] newTargetStorages,
+                                          final String[] src, final boolean[] copyTruncate,
+                                          final BlockInfoContiguousUnderConstruction[] truncatedBlock )
+          throws IOException {
+        // If a DN tries to commit to the standby, the recovery will
+        // fail, and the next retry will succeed on the new NN.
+
+        final BlockInfoContiguous storedBlock =
+            getStoredBlock(ExtendedBlock.getLocalBlock(oldBlock));
         final long oldGenerationStamp = storedBlock.getGenerationStamp();
         final long oldNumBytes = storedBlock.getNumBytes();
         //
@@ -4152,11 +4327,7 @@ public Object performTask() throws IOException {
               + " deleted and the block removal is delayed");
         }
         INodeFile iFile = ((INode)storedBlock.getBlockCollection()).asFile();
-        if (inodeIdentifier==null) {
-          throw new FileNotFoundException("File not found: "
-              + iFile.getFullPathName() + ", likely due to delayed block"
-              + " removal");
-        }
+
         if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) &&
           iFile.getLastBlock().isComplete()) {
           if (LOG.isDebugEnabled()) {
@@ -4164,7 +4335,7 @@ public Object performTask() throws IOException {
                 + ") since the file (=" + iFile.getLocalName()
                 + ") is not under construction");
           }
-          return null;
+          return;
         }
           
         truncatedBlock[0] = (BlockInfoContiguousUnderConstruction) iFile
@@ -4246,22 +4417,49 @@ public Object performTask() throws IOException {
           src[0] = iFile.getFullPathName();
           persistBlocks(src[0], iFile);
         }
-        return null;
+  }
+
+  void commitBlockSynchronizationInternalProvidedBlks(final ExtendedBlock oldBlock,
+          final long newGenerationStamp, final long newLength,
+          final boolean closeFile, final boolean deleteBlock,
+          final DatanodeID[] newTargets, final String[] newTargetStorages)
+          throws IOException {
+    String src;
+    LOG.debug("HopsFS-Cloud. Commit Block Synchronization for provided block");
+
+    Preconditions.checkArgument(deleteBlock == false);
+    Preconditions.checkArgument(closeFile == true);
+
+    final BlockInfoContiguous storedBlock =
+            getStoredBlock(ExtendedBlock.getLocalBlock(oldBlock));
+    BlockCollection blockCollection = storedBlock.getBlockCollection();
+    INodeFile iFile = ((INode) blockCollection).asFile();
+
+    //at this stage the block may have been marked completed by the
+    //incremental BR.
+    BlockUCState storedState = iFile.getLastBlock().getBlockUCState();
+    if (!iFile.isUnderConstruction()) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Unexpected block (=" + oldBlock
+                + ") since the file (=" + iFile.getLocalName()
+                + ") is not under construction");
       }
+      return;
+    }
+
+    if (storedState != BlockUCState.COMPLETE) {
+      throw new NotReplicatedYetException("Unable to syn block. Waiting for incremental blcok " +
+              "report");
+    }
 
-    }.handle(this);
     if (closeFile) {
+      src = closeFileCommitBlocks(iFile, storedBlock);
       LOG.info(
-          "commitBlockSynchronization(oldBlock=" + oldBlock
-          + ", file=" + src[0]
-          + (copyTruncate[0] ? ", newBlock=" + truncatedBlock
-              : ", newgenerationstamp=" + newGenerationStamp)
-          + ", newLength=" + newLength + ", newTargets=" + Arrays.asList(newTargets)
-          + ") successful"
-      
-      );
-        } else {
-          LOG.info("commitBlockSynchronization(" + oldBlock + ") successful");
+              "commitBlockSynchronization(oldBlock=" + oldBlock + ", file=" +
+                      src + ", newGenerationStamp=" + newGenerationStamp +
+                      ", newLength=" + newLength + ", newTargets=" +
+                      Arrays.asList(newTargets) +
+                      ") successful");
     }
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java
index 4542cec0791..38f02739a1e 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java
@@ -125,6 +125,7 @@ public int compare(INode o1, INode o2) {
   protected boolean inTree = false;
   protected long parentId = 0;
   public static int RANDOM_PARTITIONING_MAX_LEVEL=1;
+  public static int NON_EXISTING_INODE_ID=-1;
 
   protected boolean subtreeLocked;
   protected long subtreeLockOwner;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java
index 29441b42301..f2bcacce0dd 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java
@@ -307,7 +307,11 @@ public void deleteFileDataStoredInDB() throws StorageException {
   void addBlock(BlockInfoContiguous newblock)
       throws StorageException, TransactionContextException {
     BlockInfoContiguous maxBlk = findMaxBlk();
-    newblock.setBlockIndex(maxBlk.getBlockIndex() + 1);
+    int index = 0;
+    if (maxBlk != null) {
+      index = maxBlk.getBlockIndex() + 1;
+    }
+    newblock.setBlockIndex(index);
   }
 
   /**
@@ -379,7 +383,11 @@ public final long computeFileSizeNotIncludingLastUcBlock() throws StorageExcepti
   
   public long computeFileSize()
       throws StorageException, TransactionContextException {
-    return computeFileSize(true, false);
+    if(isFileStoredInDB){
+      return this.getSize();
+    } else {
+      return computeFileSize(true, false);
+    }
   }  
 
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
index a6bd5cb8ef1..70054bdce6e 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java
@@ -47,6 +47,8 @@
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.RollingUpgradeStartupOption;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;
 import org.apache.hadoop.hdfs.server.common.StorageInfo;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudPersistenceProviderFactory;
 import org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics;
 import org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;
 import org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressMetrics;
@@ -572,8 +574,8 @@ protected void initialize(Configuration conf) throws IOException {
             DFSConfigKeys.DFS_BR_LB_MAX_CONCURRENT_BR_PER_NN_DEFAULT);
     final long brMaxProcessingTime = conf.getLong(DFSConfigKeys.DFS_BR_LB_MAX_BR_PROCESSING_TIME,
             DFSConfigKeys.DFS_BR_LB_MAX_BR_PROCESSING_TIME_DEFAULT);
-     this.brTrackingService = new BRTrackingService(updateThreshold, maxConcurrentBRs,
-             brMaxProcessingTime);
+    this.brTrackingService = new BRTrackingService(updateThreshold, maxConcurrentBRs,
+            brMaxProcessingTime);
     this.mdCleaner = MDCleaner.getInstance();
     this.stoTableCleanDelay = conf.getLong(
             DFSConfigKeys.DFS_SUBTREE_CLEAN_FAILED_OPS_LOCKS_DELAY_KEY,
@@ -637,7 +639,7 @@ private void startCommonServices(Configuration conf) throws IOException {
     startLeaderElectionService();
 
     startMDCleanerService();
-    
+
     namesystem.startCommonServices(conf);
     registerNNSMXBean();
     rpcServer.start();
@@ -943,6 +945,8 @@ private static boolean formatHdfs(Configuration conf, boolean force,
       throw new RuntimeException(e.getMessage());
     }
 
+    formatCloud(conf);
+
     return false;
   }
 
@@ -974,9 +978,24 @@ public static boolean formatAll(Configuration conf) throws IOException {
       throw new RuntimeException(e.getMessage());
     }
 
+    formatCloud(conf);
+
     return false;
   }
 
+  private static void formatCloud(Configuration conf){
+    // Wipe cloud buckets
+    boolean cloud = conf.getBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE,
+            DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE_DEFAULT);
+    if(cloud){
+      System.out.println("Formatting Cloud Buckets");
+      CloudPersistenceProvider cloudConnector =
+              CloudPersistenceProviderFactory.getCloudClient(conf);
+      cloudConnector.format();
+      cloudConnector.shutdown();
+    }
+  }
+
   public static void checkAllowFormat(Configuration conf) throws IOException {
     if (!conf.getBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY,
         DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_DEFAULT)) {
@@ -1411,7 +1430,7 @@ private void startLeaderElectionService() throws IOException {
       LOG.warn("NN was interrupted");
     }
   }
-  
+
   private void createAndStartCRLFetcherService(Configuration conf) throws Exception {
     if (conf.getBoolean(CommonConfigurationKeysPublic.IPC_SERVER_SSL_ENABLED,
         CommonConfigurationKeysPublic.IPC_SERVER_SSL_ENABLED_DEFAULT)) {
@@ -1442,5 +1461,8 @@ public BRTrackingService getBRTrackingService(){
   NameNodeRpcServer getNameNodeRpcServer(){
     return rpcServer;
   }
+
+
 }
 
+
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockReport.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockReport.java
index 5d58806d79f..690a23bbb60 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockReport.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/BlockReport.java
@@ -74,8 +74,8 @@ public boolean hasNext() {
           @Override
           public Block next() {
             BlockReportReplica next = it.next();
-            return new Block(next.getBlockId(), next.getBytesOnDisk(), next
-                .getGenerationStamp());
+            return new Block(next.getBlockId(), next.getBytesOnDisk(),
+                    next.getGenerationStamp(), next.getCloudBucketID());
           }
         };
       }
@@ -155,13 +155,13 @@ private static int bucket(long blockId, int numBuckets){
   
   public static byte[] hashAsFinalized(BlockReportReplica block){
     Block toHash = new Block(block.getBlockId(), block.getBytesOnDisk(),
-        block.getGenerationStamp());
+        block.getGenerationStamp(), block.getCloudBucketID());
     return hashAsFinalized(toHash);
   }
 
   public static byte[] hashAsFinalized(BlockInfoContiguous block){
     Block toHash = new Block(block.getBlockId(), block.getNumBytes(),
-        block.getGenerationStamp());
+        block.getGenerationStamp(), block.getCloudBucketID());
     return hashAsFinalized(toHash);
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
index 4b99b2b4595..12d4cdccb10 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/protocol/ReplicaRecoveryInfo.java
@@ -32,8 +32,8 @@
   private ReplicaState originalState;
 
   public ReplicaRecoveryInfo(long blockId, long diskLen, long gs,
-      ReplicaState rState) {
-    setNoPersistance(blockId, diskLen, gs);
+                             short cloudBucketID, ReplicaState rState) {
+    setNoPersistance(blockId, diskLen, gs, cloudBucketID);
     originalState = rState;
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
index 88de2b28ad2..072b524e093 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java
@@ -158,6 +158,7 @@ public static String toJsonString(final HdfsFileStatus status,
     m.put("blockId", extendedblock.getBlockId());
     m.put("numBytes", extendedblock.getNumBytes());
     m.put("generationStamp", extendedblock.getGenerationStamp());
+    m.put("cloudBucketID", extendedblock.getCloudBucketID());
     return m;
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/hdfs.proto b/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/hdfs.proto
index 453c6b1046f..6b97ec0edd1 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/hdfs.proto
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/proto/hdfs.proto
@@ -38,6 +38,7 @@ message ExtendedBlockProto {
   required uint64 blockId = 2; // the local id within a pool
   required uint64 generationStamp = 3;
   optional uint64 numBytes = 4 [default = 0]; // len does not belong in ebid
+  optional int32 cloudBucketID = 5 [ default = -1 ];
   // here for historical reasons
 }
 
@@ -174,6 +175,7 @@ enum StorageTypeProto {
   ARCHIVE = 4;
   RAM_DISK = 5;
   DB = 6;
+  CLOUD = 7;
 }
 
 ///**
@@ -461,6 +463,7 @@ message BlockProto {
   required uint64 blockId = 1;
   required uint64 genStamp = 2;
   optional uint64 numBytes = 3 [default = 0];
+  optional int32 cloudBucketID = 4 [ default = -1 ];
 }
 
 /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml b/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
index 5788ad64248..7fb5de2c3ea 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml
@@ -2040,4 +2040,146 @@
   </description>
 </property>
 
+<property>
+  <name>dfs.enable.cloud.persistence</name>
+  <value>false</value>
+  <description>Enable/Disable cloud storage. Default: false</description>
+</property>
+
+<property>
+  <name>dfs.cloud.provider</name>
+  <value>AWS</value>
+  <description>Cloud provider. Default: AWS</description>
+</property>
+
+<property>
+  <name>dfs.cloud.aws.s3.bucket.prefix</name>
+  <value>hopsfs.bucket</value>
+  <description>Prefix for new s3 buckets</description>
+</property>
+
+<property>
+  <name>dfs.cloud.aws.s3.region</name>
+  <value>eu-north-1</value>
+  <description>S3 regions. Default: eu-north-1 </description>
+</property>
+
+<property>
+  <name>dfs.cloud.aws.s3.num.buckets</name>
+  <value>1</value>
+  <description>Number of bucket.</description>
+</property>
+
+<property>
+  <name>dfs.namenode.invalidate.cloud.blocks.per.iteration</name>
+  <value>10000</value>
+  <description>Number of cloud blocks to schedule for deletion per cycle</description>
+</property>
+
+<property>
+  <name>dfs.cloud.multipart.size</name>
+  <value>10485760</value>
+  <description>S3 multipart default size. Default 10 MB</description>
+</property>
+
+<property>
+  <name>dfs.cloud.multipart.threshold</name>
+  <value>2147483647</value>
+  <description>Currently multipart is disabled. Default: Integer.MAX_VALUE </description>
+</property>
+
+<property>
+  <name>dfs.dn.cloud.max.upload.threads</name>
+  <value>20</value>
+  <description>the maximum number of threads to allow in the pool used by TransferManager </description>
+</property>
+
+<property>
+  <name>dfs.cloud.threads.keepalivetime.sec</name>
+  <value>60</value>
+  <description>the time an idle thread waits before terminating </description>
+</property>
+
+<property>
+  <name>dfs.dn.cloud.bypass.cache</name>
+  <value>false</value>
+  <description></description>
+</property>
+
+<property>
+  <name>dfs.dnfcloud.cache.check.interval</name>
+  <value>3000</value>
+  <description>Interval between checking disk space for cache </description>
+</property>
+
+<property>
+  <name>dfs.dn.cloud.cache.delete.activation.percentage</name>
+  <value>80</value>
+  <description> </description>
+</property>
+
+<property>
+  <name>dfs.dn.cloud.cache.delete.batch.size</name>
+  <value>100</value>
+  <description> </description>
+</property>
+
+<property>
+  <name>dfs.dn.cloud.cache.delete.wait</name>
+  <value>1800000</value>
+  <description> 
+   block file must be an 30 mins old before it can be deleted.
+   This is to make sure that we do not delete newly downloaded
+   blocks in the cache that are being read by remote clients
+ </description>
+</property>
+
+<property>
+  <name>dfs.nn.max.threads.for.formatting.cloud.buckets</name>
+  <value>30</value>
+  <description>
+  </description>
+</property>
+
+<property>
+  <name>dfs.cloud.prefix.size</name>
+  <value>500</value>
+  <description>
+  </description>
+</property>
+
+<property>
+  <name>dfs.cloud.block.report.delay</name>
+  <value>3600000</value>
+  <description>
+  </description>
+</property>
+
+<property>
+  <name>dfs.cloud.block.report.thread.sleep.interval</name>
+  <value>10000</value>
+  <description>
+  </description>
+</property>
+
+<property>
+  <name>dfs.cloud.mark.partially.listed.blocks.corrupt.after</name>
+  <value>3600000</value>
+  <description>
+  </description>
+</property>
+
+<property>
+  <name>dfs.cloud.max.br.sub.tasks</name>
+  <value>1000</value>
+  <description>
+  </description>
+</property>
+
+<property>
+  <name>dfs.cloud.max.br.threads</name>
+  <value>10</value>
+  <description>
+  </description>
+</property>
 </configuration>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/CloudTestHelper.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/CloudTestHelper.java
new file mode 100644
index 00000000000..3c9d980845d
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/CloudTestHelper.java
@@ -0,0 +1,287 @@
+package org.apache.hadoop.hdfs;
+
+import io.hops.metadata.HdfsStorageFactory;
+import io.hops.metadata.hdfs.dal.*;
+import io.hops.metadata.hdfs.entity.*;
+import io.hops.transaction.handler.HDFSOperationType;
+import io.hops.transaction.handler.LightWeightRequestHandler;
+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguousUnderConstruction;
+import org.junit.rules.TestName;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CloudProvider;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.CloudBlock;
+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous;
+import org.apache.hadoop.hdfs.server.common.CloudHelper;
+import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudFsDatasetImpl;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudPersistenceProviderFactory;
+import org.apache.hadoop.hdfs.server.namenode.INode;
+import org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction;
+import org.apache.hadoop.hdfs.server.blockmanagement.PendingBlockInfo;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import java.io.IOException;
+import java.util.*;
+
+public class CloudTestHelper {
+  static final Log LOG = LogFactory.getLog(CloudTestHelper.class);
+
+  private static List<INode> findAllINodes() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                INodeDataAccess da = (INodeDataAccess) HdfsStorageFactory
+                        .getDataAccess(INodeDataAccess.class);
+                return da.allINodes();
+              }
+            };
+    return (List<INode>) handler.handle();
+  }
+
+
+  public static Map<Long, BlockInfoContiguous> findAllBlocks() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                Map<Long, BlockInfoContiguous> blkMap = new HashMap<>();
+                BlockInfoDataAccess da = (BlockInfoDataAccess) HdfsStorageFactory
+                        .getDataAccess(BlockInfoDataAccess.class);
+
+                List<BlockInfoContiguous> blocks = da.findAllBlocks();
+                for (BlockInfoContiguous blk : blocks) {
+                  blkMap.put(blk.getBlockId(), blk);
+                }
+                return blkMap;
+              }
+            };
+    return (Map<Long, BlockInfoContiguous>) handler.handle();
+  }
+
+  public static Map<Long, ProvidedBlockCacheLoc> findCacheLocations() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                ProvidedBlockCacheLocDataAccess da = (ProvidedBlockCacheLocDataAccess) HdfsStorageFactory
+                        .getDataAccess(ProvidedBlockCacheLocDataAccess.class);
+
+                Map<Long, ProvidedBlockCacheLoc> blkMap = da.findAll();
+                return blkMap;
+              }
+            };
+    return (Map<Long, ProvidedBlockCacheLoc>) handler.handle();
+  }
+
+
+  public static List<Replica> findAllReplicas() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                ReplicaDataAccess da = (ReplicaDataAccess) HdfsStorageFactory
+                        .getDataAccess(ReplicaDataAccess.class);
+                return da.findAll();
+              }
+            };
+    return (List<Replica>) handler.handle();
+  }
+
+
+  private static List<ReplicaUnderConstruction> findAllReplicasUC() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                ReplicaUnderConstructionDataAccess da =
+                        (ReplicaUnderConstructionDataAccess) HdfsStorageFactory
+                                .getDataAccess(ReplicaUnderConstructionDataAccess.class);
+                return da.findAll();
+              }
+            };
+    return (List<ReplicaUnderConstruction>) handler.handle();
+  }
+
+
+  private static List<PendingBlockInfo> findAllPendingBlocks() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                PendingBlockDataAccess da = (PendingBlockDataAccess) HdfsStorageFactory
+                        .getDataAccess(PendingBlockDataAccess.class);
+                return da.findAll();
+              }
+            };
+    return (List<PendingBlockInfo>) handler.handle();
+  }
+
+  private static List<CorruptReplica> findAllCorruptReplicas() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                CorruptReplicaDataAccess da = (CorruptReplicaDataAccess) HdfsStorageFactory
+                        .getDataAccess(CorruptReplicaDataAccess.class);
+                return da.findAll();
+              }
+            };
+    return (List<CorruptReplica>) handler.handle();
+  }
+
+  private static List<ExcessReplica> findAllExcessReplicas() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                ExcessReplicaDataAccess da = (ExcessReplicaDataAccess) HdfsStorageFactory
+                        .getDataAccess(ExcessReplicaDataAccess.class);
+                return da.findAll();
+              }
+            };
+    return (List<ExcessReplica>) handler.handle();
+  }
+
+  private static List<InvalidatedBlock> findAllInvalidatedBlocks() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                InvalidateBlockDataAccess da = (InvalidateBlockDataAccess) HdfsStorageFactory
+                        .getDataAccess(InvalidateBlockDataAccess.class);
+                return da.findAll();
+              }
+            };
+    return (List<InvalidatedBlock>) handler.handle();
+  }
+
+  private static List<UnderReplicatedBlock> findAllUnderReplicatedBlocks() throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                UnderReplicatedBlockDataAccess da = (UnderReplicatedBlockDataAccess) HdfsStorageFactory
+                        .getDataAccess(UnderReplicatedBlockDataAccess.class);
+                return da.findAll();
+              }
+            };
+    return (List<UnderReplicatedBlock>) handler.handle();
+  }
+
+  private static boolean match(Map<Long, CloudBlock> cloudView,
+                               Map<Long, BlockInfoContiguous> dbView) {
+    if (cloudView.size() != dbView.size()) {
+      List cv = new ArrayList(cloudView.values());
+      Collections.sort(cv);
+      List dbv = new ArrayList(dbView.values());
+      Collections.sort(cv);
+      LOG.info("HopsFS-Cloud Cloud Blocks " + Arrays.toString(cv.toArray()));
+      LOG.info("HopsFS-Cloud DB Blocks " + Arrays.toString(dbv.toArray()));
+    }
+
+    assert cloudView.size() == dbView.size();
+
+    for (long blkID : dbView.keySet()) {
+      CloudBlock cloudBlock = cloudView.get(blkID);
+
+      assert !cloudBlock.isPartiallyListed();
+
+      BlockInfoContiguous dbBlock = dbView.get(blkID);
+
+      assert cloudBlock != null && dbBlock != null;
+
+      assert cloudBlock.getBlock().getCloudBucketID() == dbBlock.getCloudBucketID();
+      assert cloudBlock.getBlock().getGenerationStamp() == dbBlock.getGenerationStamp();
+      assert cloudBlock.getBlock().getNumBytes() == dbBlock.getNumBytes();
+
+      assert dbBlock.getBlockUCState() == HdfsServerConstants.BlockUCState.COMPLETE;
+    }
+
+    return true;
+  }
+
+  public static boolean matchMetadata(Configuration conf) throws IOException {
+    return matchMetadata(conf, false);
+  }
+
+  public static boolean matchMetadata(Configuration conf, boolean expectingUCB) throws IOException {
+    int prefixSize = conf.getInt(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY,
+            DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_DEFAULT);
+    LOG.info("HopsFS-Cloud. CloudTestHelper. Checking Metadata");
+    CloudPersistenceProvider cloud = null;
+
+    try {
+      cloud = CloudPersistenceProviderFactory.getCloudClient(conf);
+      Map<Long, CloudBlock> cloudView = getAllCloudBlocks(cloud);
+      Map<Long, BlockInfoContiguous> dbView = findAllBlocks();
+
+      List sortDBBlocks = new ArrayList();
+      sortDBBlocks.add(dbView.values());
+      Collections.sort(sortDBBlocks);
+      List sortCloudBlocks = new ArrayList();
+      sortCloudBlocks.add(cloudView.values());
+      Collections.sort(sortCloudBlocks);
+
+      LOG.info("HopsFS-Cloud. DB View: " + sortDBBlocks);
+      LOG.info("HopsFS-Cloud. Cloud View: " + sortCloudBlocks);
+
+
+      String cloudProvider = conf.get(DFSConfigKeys.DFS_CLOUD_PROVIDER);
+      if (!cloudProvider.equals(CloudProvider.AWS.name())) {
+        match(cloudView, dbView); //fails becase of S3 eventual consistent LS, GCE is consistent
+      }
+
+      //block cache mapping
+      Map<Long, ProvidedBlockCacheLoc> cacheLoc = findCacheLocations();
+      assert cacheLoc.size() == dbView.size();
+
+      for (Block blk : dbView.values()) {
+        if (blk instanceof BlockInfoContiguousUnderConstruction && expectingUCB) {
+          continue;
+        }
+
+        LOG.info("HopsFS-Cloud. Checking Block: " + blk);
+        short bucketID = blk.getCloudBucketID();
+        String blockKey = CloudHelper.getBlockKey(prefixSize, blk);
+        String metaKey = CloudHelper.getMetaFileKey(prefixSize, blk);
+
+        assert cloud.objectExists(bucketID, blockKey) == true;
+        assert cloud.objectExists(bucketID, metaKey) == true;
+        assert cloud.getObjectSize(bucketID, blockKey) == blk.getNumBytes();
+
+        Map<String, String> metadata = cloud.getUserMetaData(bucketID, blockKey);
+        assert Long.parseLong(metadata.get(CloudFsDatasetImpl.OBJECT_SIZE)) == blk.getNumBytes();
+        assert Long.parseLong(metadata.get(CloudFsDatasetImpl.GEN_STAMP)) == blk.getGenerationStamp();
+
+        metadata = cloud.getUserMetaData(bucketID, metaKey);
+        assert metadata.size() == 0;
+        assert cacheLoc.get(blk.getBlockId()) != null;
+      }
+
+      assert findAllReplicas().size() == 0;
+      if (!expectingUCB)
+        assert findAllReplicasUC().size() == 0;
+      assert findAllExcessReplicas().size() == 0;
+      assert findAllPendingBlocks().size() == 0;
+      assert findAllCorruptReplicas().size() == 0;
+      assert findAllInvalidatedBlocks().size() == 0;
+      assert findAllUnderReplicatedBlocks().size() == 0;
+
+    } finally {
+      if (cloud != null) {
+        cloud.shutdown();
+      }
+    }
+    return true;
+  }
+
+  public static Map<Long, CloudBlock> getAllCloudBlocks(CloudPersistenceProvider cloud)
+          throws IOException {
+    return cloud.getAll("");
+  }
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DataNodeCluster.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DataNodeCluster.java
index 9736e42ff9f..35ba824d4fb 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DataNodeCluster.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DataNodeCluster.java
@@ -218,6 +218,7 @@ public static void main(String[] args) throws InterruptedException {
             blocks[i] =
                 new Block(blkid++, blockSize, GenerationStamp.LAST_RESERVED_STAMP
                     //HOP                CreateEditsLog.BLOCK_GENERATION_STAMP
+                    , Block.NON_EXISTING_BUCKET_ID
                 );
           }
           for (int i = 1; i <= replication; ++i) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlockStoragePolicy.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlockStoragePolicy.java
index 10a78a9251b..874a2085fb2 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlockStoragePolicy.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlockStoragePolicy.java
@@ -73,6 +73,7 @@
   static final byte ONESSD  = HdfsConstants.ONESSD_STORAGE_POLICY_ID;
   static final byte ALLSSD  = HdfsConstants.ALLSSD_STORAGE_POLICY_ID;
   static final byte DB  = HdfsConstants.DB_STORAGE_POLICY_ID;
+  static final byte CLOUD  = HdfsConstants.CLOUD_STORAGE_POLICY_ID;
 
   @Test (timeout=300000)
   public void testConfigKeyEnabled() throws IOException {
@@ -131,6 +132,9 @@ public void testDefaultPolicies() {
     expectedPolicyStrings.put(DB, "BlockStoragePolicy{DB:" + DB +
             ", storageTypes=[DB], creationFallbacks=[SSD, DISK], " +
             "replicationFallbacks=[SSD, DISK]}");
+    expectedPolicyStrings.put(CLOUD, "BlockStoragePolicy{CLOUD:" + CLOUD +
+            ", storageTypes=[CLOUD], creationFallbacks=[CLOUD], " +
+            "replicationFallbacks=[CLOUD]}");
 
     for(byte i = 1; i < 16; i++) {
       final BlockStoragePolicy policy = POLICY_SUITE.getPolicy(i); 
@@ -1081,19 +1085,21 @@ public void testGetAllStoragePolicies() throws Exception {
     final DistributedFileSystem fs = cluster.getFileSystem();
     try {
       BlockStoragePolicy[] policies = fs.getStoragePolicies();
-      Assert.assertEquals(6, policies.length);
+      Assert.assertEquals(7, policies.length);
       Assert.assertEquals(POLICY_SUITE.getPolicy(COLD).toString(),
           policies[0].toString());
-      Assert.assertEquals(POLICY_SUITE.getPolicy(WARM).toString(),
+      Assert.assertEquals(POLICY_SUITE.getPolicy(CLOUD).toString(),
           policies[1].toString());
-      Assert.assertEquals(POLICY_SUITE.getPolicy(HOT).toString(),
+      Assert.assertEquals(POLICY_SUITE.getPolicy(WARM).toString(),
           policies[2].toString());
-      Assert.assertEquals(POLICY_SUITE.getPolicy(ONESSD).toString(),
+      Assert.assertEquals(POLICY_SUITE.getPolicy(HOT).toString(),
           policies[3].toString());
-      Assert.assertEquals(POLICY_SUITE.getPolicy(ALLSSD).toString(),
+      Assert.assertEquals(POLICY_SUITE.getPolicy(ONESSD).toString(),
           policies[4].toString());
+      Assert.assertEquals(POLICY_SUITE.getPolicy(ALLSSD).toString(),
+          policies[5].toString());
       Assert.assertEquals(POLICY_SUITE.getPolicy(DB).toString(),
-              policies[5].toString());
+              policies[6].toString());
     } finally {
       IOUtils.cleanup(null, fs);
       cluster.shutdown();
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientProtocolForPipelineRecovery.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientProtocolForPipelineRecovery.java
index ae126880ec2..1022754f61f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientProtocolForPipelineRecovery.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientProtocolForPipelineRecovery.java
@@ -69,7 +69,7 @@ public void testGetNewStamp() throws IOException {
         long newBlockId = firstBlock.getBlockId() + 1;
         ExtendedBlock newBlock =
             new ExtendedBlock(firstBlock.getBlockPoolId(), newBlockId, 0,
-                firstBlock.getGenerationStamp());
+                firstBlock.getGenerationStamp(), firstBlock.getCloudBucketID());
         namenode.updateBlockForPipeline(newBlock, "");
         Assert.fail("Cannot get a new GS from a non-existent block");
       } catch (IOException e) {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
index 539d9af0004..441117e555a 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
 import org.apache.hadoop.hdfs.protocol.LocatedBlock;
 import org.apache.hadoop.hdfs.protocol.LocatedBlocks;
+import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.security.alias.CredentialProvider;
 import org.apache.hadoop.security.alias.CredentialProviderFactory;
@@ -73,13 +74,13 @@ public void testLocatedBlocks2Locations() {
     ds[0] = d;
 
     // ok
-    ExtendedBlock b1 = new ExtendedBlock("bpid", 1, 1, 1);
+    ExtendedBlock b1 = new ExtendedBlock("bpid", 1, 1, 1, Block.NON_EXISTING_BUCKET_ID);
     LocatedBlock l1 = new LocatedBlock(b1, ds);
     l1.setStartOffset(0);
     l1.setCorrupt(false);
 
     // corrupt
-    ExtendedBlock b2 = new ExtendedBlock("bpid", 2, 1, 1);
+    ExtendedBlock b2 = new ExtendedBlock("bpid", 2, 1, 1, Block.NON_EXISTING_BUCKET_ID);
     LocatedBlock l2 = new LocatedBlock(b2, ds);
     l2.setStartOffset(0);
     l2.setCorrupt(true);
@@ -115,7 +116,7 @@ public void testLocatedBlockConstructorWithNullCachedLocs() {
     DatanodeInfo[] ds = new DatanodeInfo[1];
     ds[0] = d;
     
-    ExtendedBlock b1 = new ExtendedBlock("bpid", 1, 1, 1);
+    ExtendedBlock b1 = new ExtendedBlock("bpid", 1, 1, 1, Block.NON_EXISTING_BUCKET_ID);
     LocatedBlock l1 = new LocatedBlock(b1, ds, null, null, 0, false, null);
     final DatanodeInfo[] cachedLocs = l1.getCachedLocations();
     assertTrue(cachedLocs.length == 0);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
index b54d017d12e..2d539ab237d 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferProtocol.java
@@ -258,7 +258,7 @@ public void testOpWrite() throws IOException {
       long newBlockId = firstBlock.getBlockId() + BLOCK_ID_FUDGE;
       ExtendedBlock newBlock =
           new ExtendedBlock(firstBlock.getBlockPoolId(), newBlockId, 0,
-              firstBlock.getGenerationStamp());
+              firstBlock.getGenerationStamp(), firstBlock.getCloudBucketID());
 
       // test PIPELINE_SETUP_CREATE on a new block
       testWrite(newBlock, BlockConstructionStage.PIPELINE_SETUP_CREATE, 0L,
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java
index e246005ef92..d4a2f0850c9 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java
@@ -21,13 +21,8 @@
 import java.io.File;
 import java.io.IOException;
 import java.io.PrintStream;
-import java.io.PrintWriter;
-import java.io.RandomAccessFile;
-import java.io.StringReader;
-import java.io.StringWriter;
 import java.net.URI;
 import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
 import java.util.concurrent.Callable;
@@ -45,7 +40,6 @@
 import org.apache.hadoop.crypto.CryptoProtocolVersion;
 import org.apache.hadoop.crypto.key.JavaKeyStoreProvider;
 import org.apache.hadoop.crypto.key.KeyProvider;
-import org.apache.hadoop.crypto.key.KeyProviderCryptoExtension;
 import org.apache.hadoop.crypto.key.KeyProviderFactory;
 import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
 import org.apache.hadoop.fs.CreateFlag;
@@ -66,12 +60,10 @@
 import org.apache.hadoop.hdfs.protocol.EncryptionZone;
 import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;
 import org.apache.hadoop.hdfs.protocol.LocatedBlocks;
-import org.apache.hadoop.hdfs.protocol.HdfsConstants.SafeModeAction;
 import org.apache.hadoop.hdfs.server.namenode.EncryptionFaultInjector;
 import org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager;
 import org.apache.hadoop.hdfs.server.namenode.NamenodeFsck;
 import org.apache.hadoop.hdfs.tools.DFSck;
-import org.apache.hadoop.hdfs.web.WebHdfsFileSystem;
 import org.apache.hadoop.hdfs.web.WebHdfsTestUtil;
 import org.apache.hadoop.io.EnumSetWritable;
 import org.apache.hadoop.security.AccessControlException;
@@ -106,13 +98,7 @@
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
-import org.xml.sax.InputSource;
-import org.xml.sax.helpers.DefaultHandler;
-
-import javax.xml.parsers.SAXParser;
-import javax.xml.parsers.SAXParserFactory;
 import org.apache.hadoop.hdfs.web.WebHdfsConstants;
-import org.slf4j.LoggerFactory;
 
 public class TestEncryptionZones {
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java
index 359f1cb930c..b0f04e76384 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java
@@ -198,7 +198,8 @@ public static ExtendedBlock getBlock(String bpid, File dataDir) {
     File metadataFile = metadataFiles.get(0);
     File blockFile = Block.metaToBlockFile(metadataFile);
     return new ExtendedBlock(bpid, Block.getBlockId(blockFile.getName()),
-        blockFile.length(), Block.getGenerationStamp(metadataFile.getName()));
+        blockFile.length(), Block.getGenerationStamp(metadataFile.getName()),
+        Block.NON_EXISTING_BUCKET_ID);
   }
 
 }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java
index 71a06e4e413..61548406a94 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java
@@ -1903,7 +1903,6 @@ public void testRenameUnderReplicatedFile() throws Exception {
       cluster.shutdown();
     }
 
-
   }
 }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetBlocks.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetBlocks.java
index 1700b0ea995..0263f4652be 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetBlocks.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetBlocks.java
@@ -291,13 +291,14 @@ public void testBlockKey() {
     long[] blkids = new long[10];
     for (int i = 0; i < blkids.length; i++) {
       blkids[i] = 1000L + RAN.nextInt(100000);
-      map.put(new Block(blkids[i], 0, blkids[i]), blkids[i]);
+      map.put(new Block(blkids[i], 0, blkids[i], Block.NON_EXISTING_BUCKET_ID), blkids[i]);
     }
     System.out.println("map=" + map.toString().replace(",", "\n  "));
 
     for (int i = 0; i < blkids.length; i++) {
       Block b = new Block(blkids[i], 0,
-          HdfsConstantsClient.GRANDFATHER_GENERATION_STAMP);
+          HdfsConstantsClient.GRANDFATHER_GENERATION_STAMP, Block.NON_EXISTING_BUCKET_ID);
+
       Long v = map.get(b);
       System.out.println(b + " => " + v);
       assertEquals(v.longValue(), blkids[i]);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSmallFilesCreation.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSmallFilesCreation.java
index a984f6cbd27..e4997721c1f 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSmallFilesCreation.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSmallFilesCreation.java
@@ -61,7 +61,7 @@ public static void writeFile(FileSystem fs, String name, int size) throws IOExce
     os.close();
   }
 
-  static void writeData(FSDataOutputStream os, int existingSize, int size) throws IOException {
+  public static void writeData(FSDataOutputStream os, int existingSize, int size) throws IOException {
     byte[] data = new byte[size];
     for (int i = 0; i < size; i++, existingSize++) {
       byte number = (byte) (existingSize % 128);
@@ -74,6 +74,11 @@ static void writeData(FSDataOutputStream os, int existingSize, int size) throws
    * This method reads the file using different read methods.
    */
   public static void verifyFile(FileSystem dfs, String file, int size) throws IOException {
+    //verify size
+    long sizeDFS = dfs.getFileStatus(new Path(file)).getLen();
+    assertTrue("Expected: "+size+" Actual: "+sizeDFS, sizeDFS == size);
+
+    //verify content
     //reading one byte at a time.
     FSDataInputStream is = dfs.open(new Path(file));
     byte[] buffer = new byte[size];
@@ -1506,6 +1511,137 @@ void append(DistributedFileSystem dfs, String name, int existingSize, int newSiz
     out.close();
   }
 
+  @Test
+  public void TestConcat() throws IOException {
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int BLOCK_SIZE = 1024 * 1024;
+      conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+      cluster.waitActive();
+
+      final int ONDISK_SMALL_BUCKET_SIZE = FSNamesystem.getDBOnDiskSmallBucketSize();
+      final int ONDISK_MEDIUM_BUCKET_SIZE = FSNamesystem.getDBOnDiskMediumBucketSize();
+      final int MAX_SMALL_FILE_SIZE = FSNamesystem.getMaxSmallFileSize();
+      final int INMEMORY_BUCKET_SIZE = FSNamesystem.getDBInMemBucketSize();
+
+      Path paths[] = new Path[4];
+      for(int i = 0; i < 4; i++){
+        paths[i] = new Path("/dir/TEST-FLIE"+i);
+      }
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "DB");
+
+      writeFile(dfs,  paths[0].toString(), INMEMORY_BUCKET_SIZE);
+      verifyFile(dfs, paths[0].toString(), INMEMORY_BUCKET_SIZE);
+      writeFile(dfs,  paths[1].toString(), ONDISK_SMALL_BUCKET_SIZE);
+      verifyFile(dfs, paths[1].toString(), ONDISK_SMALL_BUCKET_SIZE);
+      writeFile(dfs,  paths[2].toString(), ONDISK_MEDIUM_BUCKET_SIZE);
+      verifyFile(dfs, paths[2].toString(), ONDISK_MEDIUM_BUCKET_SIZE);
+      writeFile(dfs,  paths[3].toString(), MAX_SMALL_FILE_SIZE);
+      verifyFile(dfs, paths[3].toString(), MAX_SMALL_FILE_SIZE);
+
+      assertTrue("Expecting 1 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 1);
+      assertTrue("Expecting 3 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 3);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 1);
+
+      //combine these files
+
+      Path merged = new Path("/dir/merged");
+      writeFile(dfs, merged.toString(), 0);
+      try {
+        dfs.concat(merged, paths);
+      } catch (IOException e){
+        if (!e.getMessage().contains("stored in DB")){
+          throw e;
+        }
+      }
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestTruncate() throws IOException {
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int BLOCK_SIZE = 1024 * 1024;
+      conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true).build();
+      cluster.waitActive();
+
+      final int ONDISK_SMALL_BUCKET_SIZE = FSNamesystem.getDBOnDiskSmallBucketSize();
+      final int ONDISK_MEDIUM_BUCKET_SIZE = FSNamesystem.getDBOnDiskMediumBucketSize();
+      final int MAX_SMALL_FILE_SIZE = FSNamesystem.getMaxSmallFileSize();
+      final int INMEMORY_BUCKET_SIZE = FSNamesystem.getDBInMemBucketSize();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "DB");
+
+      Path file = new Path("/dir/file");
+
+      writeFile(dfs,  file.toString(), MAX_SMALL_FILE_SIZE);
+      verifyFile(dfs, file.toString(), MAX_SMALL_FILE_SIZE);
+
+      assertTrue("Expecting 0 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 1);
+
+      assert dfs.truncate(file, ONDISK_MEDIUM_BUCKET_SIZE) == true;
+      verifyFile(dfs, file.toString(), ONDISK_MEDIUM_BUCKET_SIZE);
+
+      assertTrue("Expecting 0 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 0);
+
+      assert dfs.truncate(file, ONDISK_SMALL_BUCKET_SIZE) == true;
+      verifyFile(dfs, file.toString(), ONDISK_SMALL_BUCKET_SIZE);
+
+      assertTrue("Expecting 0 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 0);
+
+      assert dfs.truncate(file, INMEMORY_BUCKET_SIZE) == true;
+      verifyFile(dfs, file.toString(), INMEMORY_BUCKET_SIZE);
+
+      Thread.sleep(1000);
+      assertTrue("Expecting 1 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 0);
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
   @Test
   /** force format the database to release the extents
    *
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestBlockListAsLongs.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestBlockListAsLongs.java
index f70f8fee773..4cd9232adbc 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestBlockListAsLongs.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestBlockListAsLongs.java
@@ -64,10 +64,10 @@
 import org.apache.hadoop.hdfs.server.protocol.BlockReport;
 
 public class TestBlockListAsLongs {
-  static Block b1 = new Block(1, 11, 111);
-  static Block b2 = new Block(2, 22, 222);
-  static Block b3 = new Block(3, 33, 333);
-  static Block b4 = new Block(4, 44, 444);
+  static Block b1 = new Block(1, 11, 111, Block.NON_EXISTING_BUCKET_ID);
+  static Block b2 = new Block(2, 22, 222, Block.NON_EXISTING_BUCKET_ID);
+  static Block b3 = new Block(3, 33, 333, Block.NON_EXISTING_BUCKET_ID);
+  static Block b4 = new Block(4, 44, 444, Block.NON_EXISTING_BUCKET_ID);
 
   @Test
   public void testEmptyReport() {
@@ -126,7 +126,7 @@ public void testFuzz() throws InterruptedException {
     Replica[] replicas = new Replica[100000];
     Random rand = new Random(0);
     for (int i=0; i<replicas.length; i++) {
-      Block b = new Block(rand.nextLong(), i, i<<4);
+      Block b = new Block(rand.nextLong(), i, i<<4, Block.NON_EXISTING_BUCKET_ID);
       switch (rand.nextInt(2)) {
         case 0:
           replicas[i] = new FinalizedReplica(b, null, null);
@@ -214,7 +214,7 @@ public BlockReportResponseProto answer(InvocationOnMock invocation) {
     NamespaceInfo nsInfo = new NamespaceInfo(1, "cluster", "bp", 1);
     reg.setNamespaceInfo(nsInfo);
 
-    Replica r = new FinalizedReplica(new Block(1, 2, 3), null, null);
+    Replica r = new FinalizedReplica(new Block(1, 2, 3, Block.NON_EXISTING_BUCKET_ID), null, null);
     BlockReport bbl = BlockReport.builder(1).add(r).build();
     DatanodeStorage storage = new DatanodeStorage("s1");
     StorageBlockReport[] sbr = { new StorageBlockReport(storage, bbl) };    
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestExtendedBlock.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestExtendedBlock.java
index fc6be49e2b3..3006afc1ba7 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestExtendedBlock.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/TestExtendedBlock.java
@@ -26,9 +26,9 @@
 public class TestExtendedBlock {
   static final String POOL_A = "blockpool-a";
   static final String POOL_B = "blockpool-b";
-  static final Block BLOCK_1_GS1 = new Block(1L, 100L, 1L);
-  static final Block BLOCK_1_GS2 = new Block(1L, 100L, 2L);
-  static final Block BLOCK_2_GS1 = new Block(2L, 100L, 1L);
+  static final Block BLOCK_1_GS1 = new Block(1L, 100L, 1L, Block.NON_EXISTING_BUCKET_ID);
+  static final Block BLOCK_1_GS2 = new Block(1L, 100L, 2L, Block.NON_EXISTING_BUCKET_ID);
+  static final Block BLOCK_2_GS1 = new Block(2L, 100L, 1L, Block.NON_EXISTING_BUCKET_ID);
   
   @Test
   public void testEquals() {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocolPB/TestPBHelper.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocolPB/TestPBHelper.java
index 9e7d3e5ee3c..54fc2f6625c 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocolPB/TestPBHelper.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocolPB/TestPBHelper.java
@@ -169,7 +169,7 @@ void compare(DatanodeStorage dns1, DatanodeStorage dns2) {
 
   @Test
   public void testConvertBlock() {
-    Block b = new Block(1, 100, 3);
+    Block b = new Block(1, 100, 3, Block.NON_EXISTING_BUCKET_ID);
     BlockProto bProto = PBHelper.convert(b);
     Block b2 = PBHelper.convert(bProto);
     assertEquals(b, b2);
@@ -180,7 +180,7 @@ private static BlockWithLocations getBlockWithLocations(int bid) {
     final String[] storageIDs = {"s1", "s2", "s3"};
     final StorageType[] storageTypes = {
         StorageType.DISK, StorageType.DISK, StorageType.DISK};
-    return new BlockWithLocations(new Block(bid, 0, 1),
+    return new BlockWithLocations(new Block(bid, 0, 1, Block.NON_EXISTING_BUCKET_ID),
         datanodeUuids, storageIDs, storageTypes);
   }
 
@@ -258,7 +258,7 @@ public ExtendedBlock getExtendedBlock() {
   }
 
   public ExtendedBlock getExtendedBlock(long blkid) {
-    return new ExtendedBlock("bpid", blkid, 100, 2);
+    return new ExtendedBlock("bpid", blkid, 100, 2, Block.NON_EXISTING_BUCKET_ID);
   }
 
   private void compare(DatanodeInfo dn1, DatanodeInfo dn2) {
@@ -410,7 +410,7 @@ private LocatedBlock createLocatedBlock() {
         StorageType.DISK
     };
     LocatedBlock lb = new LocatedBlock(
-        new ExtendedBlock("bp12", 12345, 10, 53),
+        new ExtendedBlock("bp12", 12345, 10, 53, Block.NON_EXISTING_BUCKET_ID),
         dnInfos, storageIDs, media, 5, false, new DatanodeInfo[]{});
 
     lb.setBlockToken(new Token<BlockTokenIdentifier>(
@@ -429,7 +429,7 @@ private LocatedBlock createLocatedBlockNoStorageMedia() {
             AdminStates.NORMAL)
     };
     LocatedBlock lb = new LocatedBlock(
-        new ExtendedBlock("bp12", 12345, 10, 53), dnInfos);
+        new ExtendedBlock("bp12", 12345, 10, 53, Block.NON_EXISTING_BUCKET_ID), dnInfos);
     lb.setBlockToken(new Token<BlockTokenIdentifier>(
         "identifier".getBytes(), "password".getBytes(), new Text("kind"),
         new Text("service")));
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
index 53d26a80472..895323d7231 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java
@@ -149,7 +149,7 @@ static void createFile(MiniDFSCluster cluster, Path filePath, long fileLen,
       for(int i=0; i<numOfBlocks; i++) {
         ExtendedBlock b = locatedBlocks.get(i).getBlock();
         blocks[i] = new ExtendedBlock(b.getBlockPoolId(), b.getBlockId(), b
-            .getNumBytes(), b.getGenerationStamp());
+            .getNumBytes(), b.getGenerationStamp(), b.getCloudBucketID());
       }
 
       return blocks;
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerTestUtil.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerTestUtil.java
index 9c954e47f31..fbbc113477c 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerTestUtil.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManagerTestUtil.java
@@ -178,7 +178,7 @@ public static int getComputedDatanodeWork(final BlockManager blockManager)
   
   public static int computeInvalidationWork(BlockManager bm)
       throws IOException {
-    return bm.computeInvalidateWork(Integer.MAX_VALUE);
+    return bm.computeInvalidateWorkForDNs(Integer.MAX_VALUE);
   }
   
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java
index 23a17e075c7..87d01b1bb80 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfoUnderConstruction.java
@@ -141,7 +141,7 @@ public void acquireLock(TransactionLocks locks) throws IOException {
 
       @Override
       public Object performTask() throws IOException {
-        Block block = new Block(10, 0, GenerationStamp.LAST_RESERVED_STAMP);
+        Block block = new Block(10, 0, GenerationStamp.LAST_RESERVED_STAMP, Block.NON_EXISTING_BUCKET_ID);
         EntityManager.add(new BlockInfoContiguous(block,
             inodeIdentifier != null ? inodeIdentifier.getInodeId() : BlockInfoContiguous.NON_EXISTING_ID));
         BlockInfoContiguousUnderConstruction blockInfo = new BlockInfoContiguousUnderConstruction(
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java
index aae7e213800..01f646c6656 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java
@@ -702,7 +702,7 @@ public void testHighestPriReplSrcChosenDespiteMaxReplLimit()
     bm.replicationStreamsHardLimit = 1;
 
     final long blockId = 42;         // arbitrary
-    final Block aBlock = new Block(blockId, 0, 0);
+    final Block aBlock = new Block(blockId, 0, 0, Block.NON_EXISTING_BUCKET_ID);
 
     final List<DatanodeDescriptor> origNodes = getNodes(0, 1);
     
@@ -774,7 +774,7 @@ public void testFavorDecomUntilHardLimit() throws Exception {
     bm.replicationStreamsHardLimit = 1;
 
     long blockId = 42;         // arbitrary
-    final Block aBlock = new Block(blockId, 0, 0);
+    final Block aBlock = new Block(blockId, 0, 0, Block.NON_EXISTING_BUCKET_ID);
     final List<DatanodeDescriptor> origNodes = getNodes(0, 1);
     addNodes(origNodes);
     // Add the block to the first node.
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
index 455dc9d327c..33534296f14 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java
@@ -91,7 +91,7 @@ public void teardown() throws Exception {
   }
 
   /**
-   * Test if {@link BlockManager#computeInvalidateWork(int)}
+   * Test if {@link BlockManager#computeInvalidateWorkForDNs(int)}
    * can schedule invalidate work correctly 
    */
   @Test(timeout=120000)
@@ -101,23 +101,23 @@ public void testCompInvalidate() throws Exception {
       for (int i=0; i<nodes.length; i++) {
         for(int j=0; j<3*blockInvalidateLimit+1; j++) {
           Block block = new Block(i*(blockInvalidateLimit+1)+j, 0,
-              GenerationStamp.LAST_RESERVED_STAMP);
+              GenerationStamp.LAST_RESERVED_STAMP, Block.NON_EXISTING_BUCKET_ID);
           addToInvalidates(bm, block, nodes[i], namesystem);
         }
       }
       
       assertEquals(blockInvalidateLimit*NUM_OF_DATANODES,
-          bm.computeInvalidateWork(NUM_OF_DATANODES+1));
+          bm.computeInvalidateWorkForDNs(NUM_OF_DATANODES+1));
       assertEquals(blockInvalidateLimit*NUM_OF_DATANODES,
-          bm.computeInvalidateWork(NUM_OF_DATANODES));
+          bm.computeInvalidateWorkForDNs(NUM_OF_DATANODES));
       assertEquals(blockInvalidateLimit*(NUM_OF_DATANODES-1),
-          bm.computeInvalidateWork(NUM_OF_DATANODES-1));
-      int workCount = bm.computeInvalidateWork(1);
+          bm.computeInvalidateWorkForDNs(NUM_OF_DATANODES-1));
+      int workCount = bm.computeInvalidateWorkForDNs(1);
       if (workCount == 1) {
-        assertEquals(blockInvalidateLimit+1, bm.computeInvalidateWork(2));
+        assertEquals(blockInvalidateLimit+1, bm.computeInvalidateWorkForDNs(2));
       } else {
         assertEquals(workCount, blockInvalidateLimit);
-        assertEquals(2, bm.computeInvalidateWork(2));
+        assertEquals(2, bm.computeInvalidateWorkForDNs(2));
       }
   }
 
@@ -135,12 +135,12 @@ public void testDatanodeReformat() throws Exception {
       dnr = new DatanodeRegistration(UUID.randomUUID().toString(), dnr);
       cluster.stopDataNode(nodes[0].getXferAddr());
 
-      Block block = new Block(0, 0, GenerationStamp.LAST_RESERVED_STAMP);
+      Block block = new Block(0, 0, GenerationStamp.LAST_RESERVED_STAMP, Block.NON_EXISTING_BUCKET_ID);
       addToInvalidates(bm, block, nodes[0], namesystem);
       bm.getDatanodeManager().registerDatanode(dnr);
 
       // Since UUID has changed, the invalidation work should be skipped
-      assertEquals(0, bm.computeInvalidateWork(1));
+      assertEquals(0, bm.computeInvalidateWorkForDNs(1));
       assertEquals(0, bm.getPendingDeletionBlocksCount());
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestCorruptReplicaInfo.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestCorruptReplicaInfo.java
index de9bd5ccf9f..e4e277259ee 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestCorruptReplicaInfo.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestCorruptReplicaInfo.java
@@ -71,7 +71,7 @@
   private BlockInfoContiguous getBlock(Integer block_id) {
     if (!block_map.containsKey(block_id)) {
       block_map
-          .put(block_id, new BlockInfoContiguous(new Block(block_id, 0, 0), block_id));
+          .put(block_id, new BlockInfoContiguous(new Block(block_id, 0, 0, Block.NON_EXISTING_BUCKET_ID), block_id));
     }
     
     return block_map.get(block_id);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeDescriptor.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeDescriptor.java
index c4af84c33e4..22a4002c0f3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeDescriptor.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeDescriptor.java
@@ -61,7 +61,7 @@ public void testGetInvalidateBlocks() throws Exception {
     DatanodeDescriptor dd = DFSTestUtil.getLocalDatanodeDescriptor();
     ArrayList<Block> blockList = new ArrayList<>(MAX_BLOCKS);
     for (int i = 0; i < MAX_BLOCKS; i++) {
-      blockList.add(new Block(i, 0, GenerationStamp.LAST_RESERVED_STAMP));
+      blockList.add(new Block(i, 0, GenerationStamp.LAST_RESERVED_STAMP, Block.NON_EXISTING_BUCKET_ID));
     }
     dd.addBlocksToBeInvalidated(blockList);
     Block[] bc = dd.getInvalidateBlocks(MAX_LIMIT);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHeartbeatHandling.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHeartbeatHandling.java
index be623eb923b..13acc2d6802 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHeartbeatHandling.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHeartbeatHandling.java
@@ -101,7 +101,7 @@ public void testHeartbeat() throws Exception {
       synchronized (hm) {
         for (int i = 0; i < MAX_REPLICATE_BLOCKS; i++) {
           dd.addBlockToBeReplicated(
-              new Block(i, 0, GenerationStamp.LAST_RESERVED_STAMP), ONE_TARGET);
+              new Block(i, 0, GenerationStamp.LAST_RESERVED_STAMP, Block.NON_EXISTING_BUCKET_ID), ONE_TARGET);
         }
         DatanodeCommand[] cmds =
             NameNodeAdapter.sendHeartBeat(nodeReg, dd, namesystem)
@@ -114,7 +114,7 @@ public void testHeartbeat() throws Exception {
         ArrayList<Block> blockList =
             new ArrayList<>(MAX_INVALIDATE_BLOCKS);
         for (int i = 0; i < MAX_INVALIDATE_BLOCKS; i++) {
-          blockList.add(new Block(i, 0, GenerationStamp.LAST_RESERVED_STAMP));
+          blockList.add(new Block(i, 0, GenerationStamp.LAST_RESERVED_STAMP, Block.NON_EXISTING_BUCKET_ID));
         }
         dd.addBlocksToBeInvalidated(blockList);
         cmds = NameNodeAdapter.sendHeartBeat(nodeReg, dd, namesystem)
@@ -292,7 +292,7 @@ public void acquireLock(TransactionLocks locks) throws IOException {
 
       @Override
       public Object performTask() throws IOException {
-        Block block = new Block(10, 0, GenerationStamp.LAST_RESERVED_STAMP);
+        Block block = new Block(10, 0, GenerationStamp.LAST_RESERVED_STAMP, Block.NON_EXISTING_BUCKET_ID);
         EntityManager.add(new BlockInfoContiguous(block,
             inodeIdentifier != null ? inodeIdentifier.getInodeId() : BlockInfoContiguous.NON_EXISTING_ID));
         BlockInfoContiguousUnderConstruction blockInfo = new BlockInfoContiguousUnderConstruction(
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingDataNodeMessages.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingDataNodeMessages.java
index 2826d059528..659fc76fe6a 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingDataNodeMessages.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingDataNodeMessages.java
@@ -35,10 +35,10 @@
 public class TestPendingDataNodeMessages {
   PendingDataNodeMessages msgs = new PendingDataNodeMessages();
   
-  private final Block block1Gs1 = new Block(1, 0, 1);
-  private final Block block1Gs2 = new Block(1, 0, 2);
-  private final Block block1Gs2DifferentInstance = new Block(1, 0, 2);
-  private final Block block2Gs1 = new Block(2, 0, 1);
+  private final Block block1Gs1 = new Block(1, 0, 1, Block.NON_EXISTING_BUCKET_ID);
+  private final Block block1Gs2 = new Block(1, 0, 2, Block.NON_EXISTING_BUCKET_ID);
+  private final Block block1Gs2DifferentInstance = new Block(1, 0, 2, Block.NON_EXISTING_BUCKET_ID);
+  private final Block block2Gs1 = new Block(2, 0, 1, Block.NON_EXISTING_BUCKET_ID);
 
   @Test
   public void testQueues() throws IOException {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReplication.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReplication.java
index cb587ea1dbb..ae7e7c3b560 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReplication.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingReplication.java
@@ -81,7 +81,7 @@ public void testPendingReplication() throws IOException, StorageException {
     //
     DatanodeStorageInfo[] storages = DFSTestUtil.createDatanodeStorageInfos(10);
     for (int i = 0; i < 10; i++) {
-      BlockInfoContiguous block = newBlockInfo(new Block(i, i, 0), i);
+      BlockInfoContiguous block = newBlockInfo(new Block(i, i, 0, Block.NON_EXISTING_BUCKET_ID), i);
       DatanodeStorageInfo[] targets = new DatanodeStorageInfo[i];
       System.arraycopy(storages, 0, targets, 0, i);
       increment(pendingReplications, block, DatanodeStorageInfo.toDatanodeDescriptors(targets));
@@ -94,7 +94,7 @@ public void testPendingReplication() throws IOException, StorageException {
     //
     // remove one item and reinsert it
     //
-    BlockInfoContiguous blk = newBlockInfo(new Block(8, 8, 0), 8);
+    BlockInfoContiguous blk = newBlockInfo(new Block(8, 8, 0, Block.NON_EXISTING_BUCKET_ID), 8);
     decrement(pendingReplications, blk, storages[7].getDatanodeDescriptor());             // removes one replica
     assertEquals("pendingReplications.getNumReplicas ", 7,
         getNumReplicas(pendingReplications, blk));
@@ -112,7 +112,7 @@ public void testPendingReplication() throws IOException, StorageException {
     // are sane.
     //
     for (int i = 0; i < 10; i++) {
-      BlockInfoContiguous block = newBlockInfo(new Block(i, i, 0), i);
+      BlockInfoContiguous block = newBlockInfo(new Block(i, i, 0, Block.NON_EXISTING_BUCKET_ID), i);
       int numReplicas = getNumReplicas(pendingReplications, block);
       assertTrue(numReplicas == i);
     }
@@ -131,7 +131,7 @@ public void testPendingReplication() throws IOException, StorageException {
     }
 
     for (int i = 10; i < 15; i++) {
-      BlockInfoContiguous block = newBlockInfo(new Block(i, i, 0), i);
+      BlockInfoContiguous block = newBlockInfo(new Block(i, i, 0, Block.NON_EXISTING_BUCKET_ID), i);
       increment(pendingReplications, block, DatanodeStorageInfo.toDatanodeDescriptors(
               DFSTestUtil.createDatanodeStorageInfos(i)));
     }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java
index fc5823d617f..18da0b59760 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestRBWBlockInvalidation.java
@@ -246,7 +246,7 @@ public void testRWRInvalidation() throws Exception {
 
         // Compute and send invalidations, waiting until they're fully processed.
         cluster.getNameNode().getNamesystem().getBlockManager()
-            .computeInvalidateWork(2);
+            .computeInvalidateWorkForDNs(2);
         cluster.triggerHeartbeats();
         HATestUtil.waitForDNDeletions(cluster);
         cluster.triggerDeletionReports();
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java
index 1adff4bb1b0..93f8af5e9d8 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java
@@ -232,7 +232,7 @@ public void blockReport_01() throws IOException {
         LOG.debug("Setting new length");
       }
       tempLen = rand.nextInt(BLOCK_SIZE);
-      b.setNoPersistance(b.getBlockId(), tempLen, b.getGenerationStamp());
+      b.setNoPersistance(b.getBlockId(), tempLen, b.getGenerationStamp(), b.getCloudBucketID());
       if (LOG.isDebugEnabled()) {
         LOG.debug("Block " + b.getBlockName() + " after\t " + "Size " +
             b.getNumBytes());
@@ -400,7 +400,7 @@ public void blockReport_04() throws IOException {
 
     // Create a bogus new block which will not be present on the namenode.
     ExtendedBlock b = new ExtendedBlock(
-        poolId, rand.nextLong(), 1024L, rand.nextLong());
+        poolId, rand.nextLong(), 1024L, rand.nextLong(), Block.NON_EXISTING_BUCKET_ID );
     dn.getFSDataset().createRbw(StorageType.DEFAULT, b);
 
     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
index 65a375f2c03..f8e390b5c73 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java
@@ -582,6 +582,11 @@ public synchronized void finalizeBlock(ExtendedBlock b) throws IOException {
     binfo.finalizeBlock(b.getBlockPoolId(), b.getNumBytes());
   }
 
+  @Override
+  public void postFinalize(ExtendedBlock b) throws IOException {
+
+  }
+
   @Override // FsDatasetSpi
   public synchronized void unfinalizeBlock(ExtendedBlock b) throws IOException{
     if (isValidRbw(b)) {
@@ -712,10 +717,10 @@ public synchronized long getLength(ExtendedBlock b) throws IOException {
 
   @Override
   @Deprecated
-  public Replica getReplica(String bpid, long blockId) {
-    final Map<Block, BInfo> map = blockMap.get(bpid);
+  public Replica getReplica(ExtendedBlock block) {
+    final Map<Block, BInfo> map = blockMap.get(block.getBlockPoolId());
     if (map != null) {
-      return map.get(new Block(blockId));
+      return map.get(new Block(block.getBlockId()));
     }
     return null;
   }
@@ -738,7 +743,7 @@ public Block getStoredBlock(String bpid, long blkid) throws IOException {
       if (binfo == null) {
         return null;
       }
-      return new Block(blkid, binfo.getGenerationStamp(), binfo.getNumBytes());
+      return new Block(blkid, binfo.getGenerationStamp(), binfo.getNumBytes(), Block.NON_EXISTING_BUCKET_ID);
     }
     return null;
   }
@@ -901,7 +906,12 @@ public String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen)
     map.put(binfo.theBlock, binfo);
     return binfo.getStorageUuid();
   }
-  
+
+  @Override
+  public void preFinalize(ExtendedBlock b) throws IOException {
+
+  }
+
   @Override // FsDatasetSpi
   public synchronized ReplicaHandler recoverRbw(
       ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd)
@@ -1172,7 +1182,7 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)
     }
 
     return new ReplicaRecoveryInfo(binfo.getBlockId(), binfo.getBytesOnDisk(),
-        binfo.getGenerationStamp(),
+        binfo.getGenerationStamp(), Block.NON_EXISTING_BUCKET_ID,
         binfo.isFinalized() ? ReplicaState.FINALIZED : ReplicaState.RBW);
   }
 
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
index 928a3a92786..cacd020d4e9 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java
@@ -36,11 +36,7 @@
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.HdfsConfiguration;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.hdfs.protocol.DatanodeID;
-import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
-import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
-import org.apache.hadoop.hdfs.protocol.LocatedBlock;
-import org.apache.hadoop.hdfs.protocol.RecoveryInProgressException;
+import org.apache.hadoop.hdfs.protocol.*;
 import org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.ReplicaState;
 import org.apache.hadoop.hdfs.server.datanode.DataNode.BlockRecord;
@@ -110,8 +106,9 @@
   private final static long BLOCK_LEN = 3000L;
   private final static long REPLICA_LEN1 = 6000L;
   private final static long REPLICA_LEN2 = 5000L;
+  private final static short NON_EXISTING_BUCKET_ID = Block.NON_EXISTING_BUCKET_ID;
   private final static ExtendedBlock block =
-      new ExtendedBlock(POOL_ID, BLOCK_ID, BLOCK_LEN, GEN_STAMP);
+      new ExtendedBlock(POOL_ID, BLOCK_ID, BLOCK_LEN, GEN_STAMP, NON_EXISTING_BUCKET_ID);
   
   static {
     ((Log4JLogger) LogFactory.getLog(FSNamesystem.class)).getLogger()
@@ -295,10 +292,10 @@ public void testFinalizedReplicas() throws IOException {
       LOG.debug("Running " + GenericTestUtils.getMethodName());
     }
     ReplicaRecoveryInfo replica1 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1, NON_EXISTING_BUCKET_ID,
             ReplicaState.FINALIZED);
     ReplicaRecoveryInfo replica2 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 2,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 2, NON_EXISTING_BUCKET_ID,
             ReplicaState.FINALIZED);
 
     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);
@@ -311,9 +308,9 @@ public void testFinalizedReplicas() throws IOException {
         REPLICA_LEN1);
 
     // two finalized replicas have different length
-    replica1 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+    replica1 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,NON_EXISTING_BUCKET_ID,
         ReplicaState.FINALIZED);
-    replica2 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,
+    replica2 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,NON_EXISTING_BUCKET_ID,
         ReplicaState.FINALIZED);
 
     try {
@@ -341,10 +338,10 @@ public void testFinalizedRbwReplicas() throws IOException {
     
     // rbw and finalized replicas have the same length
     ReplicaRecoveryInfo replica1 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,NON_EXISTING_BUCKET_ID,
             ReplicaState.FINALIZED);
     ReplicaRecoveryInfo replica2 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 2,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 2,NON_EXISTING_BUCKET_ID,
             ReplicaState.RBW);
 
     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);
@@ -357,9 +354,9 @@ public void testFinalizedRbwReplicas() throws IOException {
         REPLICA_LEN1);
     
     // rbw replica has a different length from the finalized one
-    replica1 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+    replica1 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,NON_EXISTING_BUCKET_ID,
         ReplicaState.FINALIZED);
-    replica2 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,
+    replica2 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,NON_EXISTING_BUCKET_ID,
         ReplicaState.RBW);
 
     dn1 = mock(InterDatanodeProtocol.class);
@@ -386,10 +383,10 @@ public void testFinalizedRwrReplicas() throws IOException {
     
     // rbw and finalized replicas have the same length
     ReplicaRecoveryInfo replica1 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,NON_EXISTING_BUCKET_ID,
             ReplicaState.FINALIZED);
     ReplicaRecoveryInfo replica2 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 2,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 2,NON_EXISTING_BUCKET_ID,
             ReplicaState.RWR);
 
     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);
@@ -402,9 +399,9 @@ public void testFinalizedRwrReplicas() throws IOException {
         block, RECOVERY_ID, BLOCK_ID, REPLICA_LEN1);
     
     // rbw replica has a different length from the finalized one
-    replica1 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+    replica1 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,NON_EXISTING_BUCKET_ID,
         ReplicaState.FINALIZED);
-    replica2 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,
+    replica2 = new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,NON_EXISTING_BUCKET_ID,
         ReplicaState.RBW);
 
     dn1 = mock(InterDatanodeProtocol.class);
@@ -430,10 +427,10 @@ public void testRBWReplicas() throws IOException {
       LOG.debug("Running " + GenericTestUtils.getMethodName());
     }
     ReplicaRecoveryInfo replica1 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,NON_EXISTING_BUCKET_ID,
             ReplicaState.RBW);
     ReplicaRecoveryInfo replica2 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,NON_EXISTING_BUCKET_ID,
             ReplicaState.RBW);
 
     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);
@@ -458,10 +455,10 @@ public void testRBW_RWRReplicas() throws IOException {
       LOG.debug("Running " + GenericTestUtils.getMethodName());
     }
     ReplicaRecoveryInfo replica1 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,NON_EXISTING_BUCKET_ID,
             ReplicaState.RBW);
     ReplicaRecoveryInfo replica2 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 2,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 2,NON_EXISTING_BUCKET_ID,
             ReplicaState.RWR);
 
     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);
@@ -487,10 +484,10 @@ public void testRWRReplicas() throws IOException {
       LOG.debug("Running " + GenericTestUtils.getMethodName());
     }
     ReplicaRecoveryInfo replica1 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN1, GEN_STAMP - 1,NON_EXISTING_BUCKET_ID,
             ReplicaState.RWR);
     ReplicaRecoveryInfo replica2 =
-        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,
+        new ReplicaRecoveryInfo(BLOCK_ID, REPLICA_LEN2, GEN_STAMP - 2,NON_EXISTING_BUCKET_ID,
             ReplicaState.RWR);
 
     InterDatanodeProtocol dn1 = mock(InterDatanodeProtocol.class);
@@ -575,7 +572,7 @@ public void testZeroLenReplicas() throws IOException, InterruptedException {
     }
     DataNode spyDN = spy(dn);
     doReturn(new ReplicaRecoveryInfo(block.getBlockId(), 0,
-        block.getGenerationStamp(), ReplicaState.FINALIZED)).when(spyDN).
+        block.getGenerationStamp(),NON_EXISTING_BUCKET_ID, ReplicaState.FINALIZED)).when(spyDN).
         initReplicaRecovery(any(RecoveringBlock.class));
     Daemon d = spyDN.recoverBlocks("fake NN", initRecoveringBlocks());
     d.join();
@@ -591,7 +588,7 @@ public void testZeroLenReplicas() throws IOException, InterruptedException {
         dn.getDNRegistrationForBP(block.getBlockPoolId());
     BlockRecord blockRecord = new BlockRecord(new DatanodeID(dnR), spyDN,
         new ReplicaRecoveryInfo(block.getBlockId(), block.getNumBytes(),
-            block.getGenerationStamp(), ReplicaState.FINALIZED));
+            block.getGenerationStamp(),NON_EXISTING_BUCKET_ID, ReplicaState.FINALIZED));
     blocks.add(blockRecord);
     return blocks;
   }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBlockReports.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBlockReports.java
index c4f992d2058..2629f0ea8ab 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBlockReports.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBlockReports.java
@@ -52,6 +52,7 @@
   private static final long DUMMY_BLOCK_ID = 5678;
   private static final long DUMMY_BLOCK_LENGTH = 1024 * 1024;
   private static final long DUMMY_BLOCK_GENSTAMP = 1000;
+  private static final short NON_EXISTING_BUCKET_ID  = Block.NON_EXISTING_BUCKET_ID;
 
   private MiniDFSCluster cluster = null;
   private DistributedFileSystem fs;
@@ -75,7 +76,7 @@ public void startCluster() throws IOException {
   }
 
   private static Block getDummyBlock() {
-    return new Block(DUMMY_BLOCK_ID, DUMMY_BLOCK_LENGTH, DUMMY_BLOCK_GENSTAMP);
+    return new Block(DUMMY_BLOCK_ID, DUMMY_BLOCK_LENGTH, DUMMY_BLOCK_GENSTAMP, NON_EXISTING_BUCKET_ID);
   }
 
   /**
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java
index a06d8b79baa..a374149013c 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java
@@ -223,7 +223,7 @@ public void testDataNodeDoesNotSplitReports()
   }
 
   private static Block getDummyBlock() {
-    return new Block(0L, 100L, 1048576L);
+    return new Block(0L, 100L, 1048576L, Block.NON_EXISTING_BUCKET_ID);
   }
 
   private static StorageReceivedDeletedBlocks[] makeReportForReceivedBlock(
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java
index 950c8b34710..78411e32f87 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams;
 import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory;
 import org.apache.hadoop.hdfs.server.protocol.BlockReport;
+import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.util.DataChecksum;
 import org.junit.Before;
 import org.junit.Test;
@@ -74,7 +75,7 @@ static int addSomeBlocks(SimulatedFSDataset fsdataset, long startingBlockId,
     int bytesAdded = 0;
     for (long i = startingBlockId; i < startingBlockId+NUMBLOCKS; ++i) {
       long blkID = negativeBlkID ? i * -1 : i;
-      ExtendedBlock b = new ExtendedBlock(bpid, blkID, 0, 0);
+      ExtendedBlock b = new ExtendedBlock(bpid, blkID, 0, 0, Block.NON_EXISTING_BUCKET_ID);
       // we pass expected len as zero, - fsdataset should use the sizeof actual
       // data written
       ReplicaInPipelineInterface bInfo = fsdataset.createRbw(
@@ -104,7 +105,7 @@ static void readSomeBlocks(SimulatedFSDataset fsdataset,
       boolean negativeBlkID) throws IOException {
     for (long i = FIRST_BLK_ID; i <= NUMBLOCKS; ++i) {
       long blkID = negativeBlkID ? i * -1 : i;
-      ExtendedBlock b = new ExtendedBlock(bpid, blkID, 0, 0);
+      ExtendedBlock b = new ExtendedBlock(bpid, blkID, 0, 0, Block.NON_EXISTING_BUCKET_ID);
       assertTrue(fsdataset.isValidBlock(b));
       assertEquals(blockIdToLen(i), fsdataset.getLength(b));
       checkBlockDataAndSize(fsdataset, b, blockIdToLen(i));
@@ -127,7 +128,7 @@ public void testFSDatasetFactory() {
   @Test
   public void testGetMetaData() throws IOException {
     final SimulatedFSDataset fsdataset = getSimulatedFSDataset();
-    ExtendedBlock b = new ExtendedBlock(bpid, FIRST_BLK_ID, 5, 0);
+    ExtendedBlock b = new ExtendedBlock(bpid, FIRST_BLK_ID, 5, 0, Block.NON_EXISTING_BUCKET_ID);
     try {
       assertTrue(fsdataset.getMetaDataInputStream(b) == null);
       assertTrue("Expected an IO exception", false);
@@ -135,7 +136,7 @@ public void testGetMetaData() throws IOException {
       // ok - as expected
     }
     addSomeBlocks(fsdataset); // Only need to add one but ....
-    b = new ExtendedBlock(bpid, FIRST_BLK_ID, 0, 0);
+    b = new ExtendedBlock(bpid, FIRST_BLK_ID, 0, 0, Block.NON_EXISTING_BUCKET_ID);
     InputStream metaInput = fsdataset.getMetaDataInputStream(b);
     DataInputStream metaDataInput = new DataInputStream(metaInput);
     short version = metaDataInput.readShort();
@@ -305,12 +306,12 @@ public void checkInvalidBlock(ExtendedBlock b) {
   @Test
   public void testInValidBlocks() throws IOException {
     final SimulatedFSDataset fsdataset = getSimulatedFSDataset();
-    ExtendedBlock b = new ExtendedBlock(bpid, FIRST_BLK_ID, 5, 0);
+    ExtendedBlock b = new ExtendedBlock(bpid, FIRST_BLK_ID, 5, 0, Block.NON_EXISTING_BUCKET_ID);
     checkInvalidBlock(b);
     
     // Now check invlaid after adding some blocks
     addSomeBlocks(fsdataset);
-    b = new ExtendedBlock(bpid, NUMBLOCKS + 99, 5, 0);
+    b = new ExtendedBlock(bpid, NUMBLOCKS + 99, 5, 0, Block.NON_EXISTING_BUCKET_ID);
     checkInvalidBlock(b);
   }
 
@@ -319,8 +320,8 @@ public void testInvalidate() throws IOException {
     final SimulatedFSDataset fsdataset = getSimulatedFSDataset();
     int bytesAdded = addSomeBlocks(fsdataset);
     Block[] deleteBlocks = new Block[2];
-    deleteBlocks[0] = new Block(1, 0, 0);
-    deleteBlocks[1] = new Block(2, 0, 0);
+    deleteBlocks[0] = new Block(1, 0, 0, Block.NON_EXISTING_BUCKET_ID);
+    deleteBlocks[1] = new Block(2, 0, 0, Block.NON_EXISTING_BUCKET_ID);
     fsdataset.invalidate(bpid, deleteBlocks);
     checkInvalidBlock(new ExtendedBlock(bpid, deleteBlocks[0]));
     checkInvalidBlock(new ExtendedBlock(bpid, deleteBlocks[1]));
@@ -331,7 +332,7 @@ public void testInvalidate() throws IOException {
     
     // Now make sure the rest of the blocks are valid
     for (int i = 3; i <= NUMBLOCKS; ++i) {
-      Block b = new Block(i, 0, 0);
+      Block b = new Block(i, 0, 0, Block.NON_EXISTING_BUCKET_ID);
       assertTrue(fsdataset.isValidBlock(new ExtendedBlock(bpid, b)));
     }
   }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTransferRbw.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTransferRbw.java
index df843e5f117..129a6b65ba2 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTransferRbw.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTransferRbw.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.HdfsConfiguration;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
 import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
 import org.apache.hadoop.hdfs.protocol.HdfsConstants.DatanodeReportType;
@@ -133,7 +134,7 @@ public void testTransferRbw() throws Exception {
         //transfer RBW
         final ExtendedBlock b =
             new ExtendedBlock(bpid, oldrbw.getBlockId(), oldrbw.getBytesAcked(),
-                oldrbw.getGenerationStamp());
+                oldrbw.getGenerationStamp(), Block.NON_EXISTING_BUCKET_ID);
         final BlockOpResponseProto s = DFSTestUtil
             .transferRbw(b, DFSClientAdapter.getDFSClient(fs), oldnodeinfo,
                 newnodeinfo);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTriggerBlockReport.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTriggerBlockReport.java
index 6d55ce2866b..474bb2105a2 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTriggerBlockReport.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestTriggerBlockReport.java
@@ -95,7 +95,8 @@ private void testTriggerBlockReport(boolean incremental) throws Exception {
         datanode.getAllBpOs().get(0);
     String storageUuid =
         datanode.getFSDataset().getVolumes().get(0).getStorageID();
-    ExtendedBlock rdbi = new ExtendedBlock(service.getBlockPoolId(), 5678, 512, 1000);
+    ExtendedBlock rdbi = new ExtendedBlock(service.getBlockPoolId(), 5678, 512,
+            1000, Block.NON_EXISTING_BUCKET_ID);
     service.notifyNamenodeDeletedBlock(rdbi, storageUuid);
 
     // Manually trigger a block report.
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
index 77fb585c896..108289eeb77 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/extdataset/ExternalDatasetImpl.java
@@ -110,7 +110,7 @@ public long getLength(ExtendedBlock b) throws IOException {
 
   @Override
   @Deprecated
-  public Replica getReplica(String bpid, long blockId) {
+  public Replica getReplica(ExtendedBlock block) {
     return new ExternalReplica();
   }
 
@@ -178,10 +178,20 @@ public String recoverClose(ExtendedBlock b, long newGS, long expectedBlockLen)
     return null;
   }
 
+  @Override
+  public void preFinalize(ExtendedBlock b) throws IOException {
+
+  }
+
   @Override
   public void finalizeBlock(ExtendedBlock b) throws IOException {
   }
 
+  @Override
+  public void postFinalize(ExtendedBlock b) throws IOException {
+
+  }
+
   @Override
   public void unfinalizeBlock(ExtendedBlock b) throws IOException {
   }
@@ -264,7 +274,7 @@ public long getReplicaVisibleLength(ExtendedBlock block) throws IOException {
   @Override
   public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)
       throws IOException {
-    return new ReplicaRecoveryInfo(0, 0, 0, ReplicaState.FINALIZED);
+    return new ReplicaRecoveryInfo(0, 0, 0, Block.NON_EXISTING_BUCKET_ID, ReplicaState.FINALIZED);
   }
 
   @Override
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
index 3bc24ba7858..50232600be2 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java
@@ -320,7 +320,9 @@ public void testAddVolumeFailureReleasesInUseLock() throws IOException {
     File badDir = new File(BASE_DIR, "bad");
     badDir.mkdirs();
     doReturn(mockVolume).when(spyDataset)
-        .createFsVolume(anyString(), any(File.class), any(StorageType.class));
+        .getNewFsVolumeImpl(any(FsDatasetImpl.class), anyString(), any(File.class),
+                any(Configuration.class),
+                any(StorageType.class));
     doThrow(new IOException("Failed to getVolumeMap()"))
       .when(mockVolume).getVolumeMap(
         anyString(),
@@ -362,7 +364,7 @@ public void testDeletingBlocks() throws IOException {
       ReplicaInfo info;
       List<Block> blockList = new ArrayList<Block>();
       for (int i = 1; i <= 63; i++) {
-        eb = new ExtendedBlock(BLOCKPOOL, i, 1, 1000 + i);
+        eb = new ExtendedBlock(BLOCKPOOL, i, 1, 1000 + i, Block.NON_EXISTING_BUCKET_ID);
         info = new FinalizedReplica(
             eb.getLocalBlock(), vol, vol.getCurrentDir().getParentFile());
         ds.volumeMap.add(BLOCKPOOL, info);
@@ -379,7 +381,7 @@ public void testDeletingBlocks() throws IOException {
       assertTrue(ds.isDeletingBlock(BLOCKPOOL, blockList.get(0).getBlockId()));
 
       blockList.clear();
-      eb = new ExtendedBlock(BLOCKPOOL, 64, 1, 1064);
+      eb = new ExtendedBlock(BLOCKPOOL, 64, 1, 1064, Block.NON_EXISTING_BUCKET_ID);
       info = new FinalizedReplica(
           eb.getLocalBlock(), vol, vol.getCurrentDir().getParentFile());
       ds.volumeMap.add(BLOCKPOOL, info);
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestInterDatanodeProtocol.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestInterDatanodeProtocol.java
index 168221105be..74486705816 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestInterDatanodeProtocol.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestInterDatanodeProtocol.java
@@ -205,14 +205,15 @@ private void checkBlockMetaDataInfo(boolean useDnHostname) throws Exception {
 
       //verify updateBlock
       ExtendedBlock newblock = new ExtendedBlock(b.getBlockPoolId(),
-          b.getBlockId(), b.getNumBytes()/2, b.getGenerationStamp()+1);
+          b.getBlockId(), b.getNumBytes()/2, b.getGenerationStamp()+1,
+          Block.NON_EXISTING_BUCKET_ID);
       idp.updateReplicaUnderRecovery(b, recoveryId, b.getBlockId(),
           newblock.getNumBytes());
       checkMetaInfo(newblock, datanode);
       
       // Verify correct null response trying to init recovery for a missing block
       ExtendedBlock badBlock =
-          new ExtendedBlock("fake-pool", b.getBlockId(), 0, 0);
+          new ExtendedBlock("fake-pool", b.getBlockId(), 0, 0, Block.NON_EXISTING_BUCKET_ID);
       assertNull(idp.initReplicaRecovery(
           new RecoveringBlock(badBlock, locatedblock.getLocations(),
               recoveryId)));
@@ -247,83 +248,101 @@ public void testInitReplicaRecovery() throws IOException {
     final long firstblockid = 10000L;
     final long gs = 7777L;
     final long length = 22L;
-    final ReplicaMap map = new ReplicaMap(this);
-    String bpid = "BP-TEST";
     final Block[] blocks = new Block[5];
-    for (int i = 0; i < blocks.length; i++) {
-      blocks[i] = new Block(firstblockid + i, length, gs);
-      map.add(bpid, createReplicaInfo(blocks[i]));
-    }
-    
-    {
+
+
+    MiniDFSCluster cluster = null;
+
+    try {
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
+      cluster.waitActive();
+      String bpid = cluster.getNamesystem().getBlockPoolId();
+
+      FsDatasetImpl data = (FsDatasetImpl)cluster.getDataNodes().get(0).getFSDataset();
+      ReplicaMap map = data.getVolumeMap();
+      for (int i = 0; i < blocks.length; i++) {
+        blocks[i] = new Block(firstblockid + i, length, gs, Block.NON_EXISTING_BUCKET_ID );
+        map.add(bpid, createReplicaInfo(blocks[i]));
+      }
+
+      {
       //normal case
-      final Block b = blocks[0];
-      final ReplicaInfo originalInfo = map.get(bpid, b);
-
-      final long recoveryid = gs + 1;
-      final ReplicaRecoveryInfo recoveryInfo = FsDatasetImpl.initReplicaRecovery(bpid, map, blocks[0], recoveryid,
-          DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
-      assertEquals(originalInfo, recoveryInfo);
-
-      final ReplicaUnderRecovery updatedInfo =
-          (ReplicaUnderRecovery) map.get(bpid, b);
-      Assert.assertEquals(originalInfo.getBlockId(), updatedInfo.getBlockId());
-      Assert.assertEquals(recoveryid, updatedInfo.getRecoveryID());
-
-      //recover one more time 
-      final long recoveryid2 = gs + 2;
-      final ReplicaRecoveryInfo recoveryInfo2 = FsDatasetImpl.initReplicaRecovery(bpid, map, blocks[0], recoveryid2,
-          DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
-      assertEquals(originalInfo, recoveryInfo2);
-
-      final ReplicaUnderRecovery updatedInfo2 =
-          (ReplicaUnderRecovery) map.get(bpid, b);
-      Assert.assertEquals(originalInfo.getBlockId(), updatedInfo2.getBlockId());
-      Assert.assertEquals(recoveryid2, updatedInfo2.getRecoveryID());
-      
-      //case RecoveryInProgressException
-      try {
-        FsDatasetImpl.initReplicaRecovery(bpid, map, b, recoveryid,
-            DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
-        Assert.fail();
-      } catch (RecoveryInProgressException ripe) {
-        System.out.println("GOOD: getting " + ripe);
+        final Block b = blocks[0];
+        final ReplicaInfo originalInfo = map.get(bpid, b);
+
+        final long recoveryid = gs + 1;
+
+        final ReplicaRecoveryInfo recoveryInfo = data.initReplicaRecovery(bpid, map, blocks[0], recoveryid,
+                DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
+        assertEquals(originalInfo, recoveryInfo);
+
+        final ReplicaUnderRecovery updatedInfo =
+                (ReplicaUnderRecovery) map.get(bpid, b);
+        Assert.assertEquals(originalInfo.getBlockId(), updatedInfo.getBlockId());
+        Assert.assertEquals(recoveryid, updatedInfo.getRecoveryID());
+
+        //recover one more time
+        final long recoveryid2 = gs + 2;
+        final ReplicaRecoveryInfo recoveryInfo2 = data.initReplicaRecovery(bpid, map, blocks[0], recoveryid2,
+                DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
+        assertEquals(originalInfo, recoveryInfo2);
+
+        final ReplicaUnderRecovery updatedInfo2 =
+                (ReplicaUnderRecovery) map.get(bpid, b);
+        Assert.assertEquals(originalInfo.getBlockId(), updatedInfo2.getBlockId());
+        Assert.assertEquals(recoveryid2, updatedInfo2.getRecoveryID());
+
+        //case RecoveryInProgressException
+        try {
+          data.initReplicaRecovery(bpid, map, b, recoveryid,
+                  DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
+          Assert.fail();
+        } catch (RecoveryInProgressException ripe) {
+          System.out.println("GOOD: getting " + ripe);
+        }
       }
-    }
-
-    { // BlockRecoveryFI_01: replica not found
-      final long recoveryid = gs + 1;
-      final Block b = new Block(firstblockid - 1, length, gs);
-      ReplicaRecoveryInfo r = FsDatasetImpl.initReplicaRecovery(bpid, map, b, recoveryid,
-          DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
-      Assert.assertNull("Data-node should not have this replica.", r);
-    }
-    
-    { // BlockRecoveryFI_02: "THIS IS NOT SUPPOSED TO HAPPEN" with recovery id < gs  
-      final long recoveryid = gs - 1;
-      final Block b = new Block(firstblockid + 1, length, gs);
-      try {
-        FsDatasetImpl.initReplicaRecovery(bpid, map, b, recoveryid,
-            DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
-        Assert.fail();
-      } catch (IOException ioe) {
-        System.out.println("GOOD: getting " + ioe);
+
+      { // BlockRecoveryFI_01: replica not found
+        final long recoveryid = gs + 1;
+        final Block b = new Block(firstblockid - 1, length, gs, Block.NON_EXISTING_BUCKET_ID);
+        ReplicaRecoveryInfo r = data.initReplicaRecovery(bpid, map, b, recoveryid,
+                DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
+        Assert.assertNull("Data-node should not have this replica.", r);
       }
-    }
+
+
+      { // BlockRecoveryFI_02: "THIS IS NOT SUPPOSED TO HAPPEN" with recovery id < gs
+        final long recoveryid = gs - 1;
+        final Block b = new Block(firstblockid + 1, length, gs, Block.NON_EXISTING_BUCKET_ID);
+        try {
+          data.initReplicaRecovery(bpid, map, b, recoveryid,
+                  DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
+          Assert.fail();
+        } catch (IOException ioe) {
+          System.out.println("GOOD: getting " + ioe);
+        }
+      }
+
 
     // BlockRecoveryFI_03: Replica's gs is less than the block's gs
-    {
-      final long recoveryid = gs + 1;
-      final Block b = new Block(firstblockid, length, gs + 1);
-      try {
-        FsDatasetImpl.initReplicaRecovery(bpid, map, b, recoveryid,
-            DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
-        fail("InitReplicaRecovery should fail because replica's " +
-            "gs is less than the block's gs");
-      } catch (IOException e) {
-        e.getMessage().startsWith(
-            "replica.getGenerationStamp() < block.getGenerationStamp(), block=");
+      {
+        final long recoveryid = gs + 1;
+        final Block b = new Block(firstblockid, length, gs + 1, Block.NON_EXISTING_BUCKET_ID);
+        try {
+          data.initReplicaRecovery(bpid, map, b, recoveryid,
+                  DFSConfigKeys.DFS_DATANODE_XCEIVER_STOP_TIMEOUT_MILLIS_DEFAULT);
+          fail("InitReplicaRecovery should fail because replica's " +
+                  "gs is less than the block's gs");
+        } catch (IOException e) {
+          e.getMessage().startsWith(
+                  "replica.getGenerationStamp() < block.getGenerationStamp(), block=");
+        }
       }
+    } catch (Exception e){
+      fail();
+    } finally {
+      if(cluster != null)
+      cluster.shutdown();
     }
   }
 
@@ -333,7 +352,7 @@ public void testInitReplicaRecovery() throws IOException {
    * long)}
    */
   @Test
-  public void testUpdateReplicaUnderRecovery() throws IOException {
+  public void testUpdateRepcicaUnderRecovery() throws IOException {
     MiniDFSCluster cluster = null;
 
     try {
@@ -374,7 +393,9 @@ public void testUpdateReplicaUnderRecovery() throws IOException {
       Assert.assertEquals(ReplicaState.RUR, replica.getState());
 
       //check meta data before update
-      FsDatasetImpl.checkReplicaFiles(replica);
+
+      FsDatasetImpl data = (FsDatasetImpl)cluster.getDataNodes().get(0).getFSDataset();
+      data.checkReplicaFiles(replica);
 
       //case "THIS IS NOT SUPPOSED TO HAPPEN"
       //with (block length) != (stored replica's on disk length). 
@@ -382,7 +403,7 @@ public void testUpdateReplicaUnderRecovery() throws IOException {
         //create a block with same id and gs but different length.
         final ExtendedBlock tmp =
             new ExtendedBlock(b.getBlockPoolId(), rri.getBlockId(),
-                rri.getNumBytes() - 1, rri.getGenerationStamp());
+                rri.getNumBytes() - 1, rri.getGenerationStamp(), Block.NON_EXISTING_BUCKET_ID);
         try {
           //update should fail
           fsdataset.updateReplicaUnderRecovery(tmp, recoveryid,
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestReplicaMap.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestReplicaMap.java
index 5ecafb72124..ec54c7d4323 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestReplicaMap.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestReplicaMap.java
@@ -32,7 +32,7 @@
 public class TestReplicaMap {
   private final ReplicaMap map = new ReplicaMap(TestReplicaMap.class);
   private final String bpid = "BP-TEST";
-  private final Block block = new Block(1234, 1234, 1234);
+  private final Block block = new Block(1234, 1234, 1234, Block.NON_EXISTING_BUCKET_ID);
   
   @Before
   public void setup() {
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestWriteToReplica.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestWriteToReplica.java
index fbeb2fe4605..07cea87f69a 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestWriteToReplica.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestWriteToReplica.java
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hdfs.HdfsConfiguration;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hdfs.MiniDFSNNTopology;
+import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.ExtendedBlock;
 import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.ReplicaState;
 import org.apache.hadoop.hdfs.server.datanode.DataNode;
@@ -170,13 +171,14 @@ public void testWriteToTemporary() throws Exception {
     // setup replicas map
     
     ExtendedBlock[] blocks =
-        new ExtendedBlock[]{new ExtendedBlock(bpid, 1, 1, 2001),
-            new ExtendedBlock(bpid, 2, 1, 2002),
-            new ExtendedBlock(bpid, 3, 1, 2003),
-            new ExtendedBlock(bpid, 4, 1, 2004),
-            new ExtendedBlock(bpid, 5, 1, 2005),
-            new ExtendedBlock(bpid, 6, 1, 2006)};
-    
+        new ExtendedBlock[]{
+            new ExtendedBlock(bpid, 1, 1, 2001, Block.NON_EXISTING_BUCKET_ID),
+            new ExtendedBlock(bpid, 2, 1, 2002, Block.NON_EXISTING_BUCKET_ID),
+            new ExtendedBlock(bpid, 3, 1, 2003, Block.NON_EXISTING_BUCKET_ID),
+            new ExtendedBlock(bpid, 4, 1, 2004, Block.NON_EXISTING_BUCKET_ID),
+            new ExtendedBlock(bpid, 5, 1, 2005, Block.NON_EXISTING_BUCKET_ID),
+            new ExtendedBlock(bpid, 6, 1, 2006, Block.NON_EXISTING_BUCKET_ID)};
+
     ReplicaMap replicasMap = dataSet.volumeMap;
     FsVolumeImpl vol = (FsVolumeImpl) dataSet.volumes
         .getNextVolume(StorageType.DEFAULT, 0).getVolume();
@@ -188,7 +190,8 @@ public void testWriteToTemporary() throws Exception {
     
     replicasMap.add(bpid, new ReplicaInPipeline(
         blocks[TEMPORARY].getBlockId(),
-        blocks[TEMPORARY].getGenerationStamp(), vol,
+        blocks[TEMPORARY].getGenerationStamp(),
+        blocks[TEMPORARY].getCloudBucketID(), vol,
         vol.createTmpFile(bpid, blocks[TEMPORARY].getLocalBlock()).getParentFile(), 0));
     
     replicaInfo = new ReplicaBeingWritten(blocks[RBW].getLocalBlock(), vol,
@@ -652,23 +655,24 @@ private void createReplicas(List<String> bpList, List<FsVolumeImpl> volumes,
     long id = 1; // This variable is used as both blockId and genStamp
     for (String bpId: bpList) {
       for (FsVolumeImpl volume: volumes) {
-        ReplicaInfo finalizedReplica = new FinalizedReplica(id, 1, id, volume,
+        ReplicaInfo finalizedReplica = new FinalizedReplica(id, 1, id,
+           Block.NON_EXISTING_BUCKET_ID, volume,
             DatanodeUtil.idToBlockDir(volume.getFinalizedDir(bpId), id));
         volumeMap.add(bpId, finalizedReplica);
         id++;
         
-        ReplicaInfo rbwReplica = new ReplicaBeingWritten(id, 1, id, volume, 
-            volume.getRbwDir(bpId), null, 100);
+        ReplicaInfo rbwReplica = new ReplicaBeingWritten(id, 1, id,
+                Block.NON_EXISTING_BUCKET_ID, volume, volume.getRbwDir(bpId), null, 100);
         volumeMap.add(bpId, rbwReplica);
         id++;
 
-        ReplicaInfo rwrReplica = new ReplicaWaitingToBeRecovered(id, 1, id, 
-            volume, volume.getRbwDir(bpId));
+        ReplicaInfo rwrReplica = new ReplicaWaitingToBeRecovered(id, 1, id,
+                Block.NON_EXISTING_BUCKET_ID, volume, volume.getRbwDir(bpId));
         volumeMap.add(bpId, rwrReplica);
         id++;
         
-        ReplicaInfo ripReplica = new ReplicaInPipeline(id, id, volume, 
-            volume.getTmpDir(bpId), 0);
+        ReplicaInfo ripReplica = new ReplicaInPipeline(id, id, Block.NON_EXISTING_BUCKET_ID,
+                volume, volume.getTmpDir(bpId), 0);
         volumeMap.add(bpId, ripReplica);
         id++;
       }
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/CloudBlockReportTestHelper.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/CloudBlockReportTestHelper.java
new file mode 100644
index 00000000000..3ff3df35e95
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/CloudBlockReportTestHelper.java
@@ -0,0 +1,124 @@
+package org.apache.hadoop.hdfs.server.namenode;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.CloudBlock;
+import org.apache.hadoop.hdfs.server.blockmanagement.ProvidedBlocksChecker;
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.hadoop.hdfs.server.common.CloudHelper;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudPersistenceProviderFactory;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+public class CloudBlockReportTestHelper {
+  static final Log LOG = LogFactory.getLog(CloudBlockReportTestHelper.class);
+
+  public static long waitForBRCompletion(ProvidedBlocksChecker pbc, long count) throws IOException {
+    try {
+      long waitForFirstBR = 30;
+      long value = -1;
+      do {
+        value = pbc.getProvidedBlockReportsCount();
+        if (value == count) {
+          value = count;
+          break;
+        }
+
+        LOG.info("HopsFS-Cloud. BR waiting for block report counter to increase");
+        Thread.sleep(1000);
+        waitForFirstBR--;
+      } while (waitForFirstBR > 0);
+
+      if (value != count) {
+        return -1;
+      }
+
+      waitForFirstBR = 30;
+      do {
+        if (pbc.getAllTasks().size() == 0 && !pbc.isBRInProgress()) {
+          return value;
+        }
+        LOG.info("HopsFS-Cloud. BR waiting for block report tasks to finish");
+        Thread.sleep(1000);
+      } while (--waitForFirstBR > 0);
+
+    } catch (InterruptedException e) {
+
+    }
+    return -1;
+  }
+
+  public static void changeGSOfCloudObjs(Configuration conf, int count) throws IOException {
+    int prefixSize = conf.getInt(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY,
+            DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_DEFAULT);
+    CloudPersistenceProvider cloudConnector = CloudPersistenceProviderFactory.getCloudClient(conf);
+    Map<Long, CloudBlock> objMap = cloudConnector.getAll("");
+    int corrupted = 0;
+    for (CloudBlock blk : objMap.values()) {
+      if (blk.isPartiallyListed()) {
+        continue;
+      }
+      short srcBucketID = blk.getBlock().getCloudBucketID();
+      short dstBucketID = srcBucketID;
+
+      Block b = blk.getBlock();
+      String srcBlkKey = CloudHelper.getBlockKey(prefixSize, b);
+      String srcMetaKey = CloudHelper.getMetaFileKey(prefixSize, b);
+
+      b.setGenerationStampNoPersistance(8888);
+      String dstBlkKey = CloudHelper.getBlockKey(prefixSize, b);
+      String dstMetaKey = CloudHelper.getMetaFileKey(prefixSize, b);
+
+      cloudConnector.renameObject(srcBucketID, dstBucketID, srcBlkKey, dstBlkKey);
+      cloudConnector.renameObject(srcBucketID, dstBucketID, srcMetaKey, dstMetaKey);
+
+      if (++corrupted >= count) {
+        return;
+      }
+    }
+
+  }
+
+  public static void deleteMetaObjects(Configuration conf, int count) throws IOException {
+    int prefixSize = conf.getInt(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY,
+            DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_DEFAULT);
+    CloudPersistenceProvider cloudConnector = CloudPersistenceProviderFactory.getCloudClient(conf);
+    Map<Long, CloudBlock> objMap = cloudConnector.getAll("");
+    int corrupted = 0;
+    for (CloudBlock blk : objMap.values()) {
+      short srcBucketID = blk.getBlock().getCloudBucketID();
+      Block b = blk.getBlock();
+      String srcMetaKey = CloudHelper.getMetaFileKey(prefixSize, b);
+      cloudConnector.deleteObject(srcBucketID, srcMetaKey);
+      if (++corrupted >= count) {
+        return;
+      }
+    }
+  }
+
+  public static void deleteBlocksAndMetaObjs(Configuration conf, int count) throws IOException {
+    int prefixSize = conf.getInt(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY,
+            DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_DEFAULT);
+    CloudPersistenceProvider cloudConnector = CloudPersistenceProviderFactory.getCloudClient(conf);
+    Map<Long, CloudBlock> objMap = cloudConnector.getAll("");
+    int corrupted = 0;
+    for (CloudBlock blk : objMap.values()) {
+      short srcBucketID = blk.getBlock().getCloudBucketID();
+      Block b = blk.getBlock();
+      String srcMetaKey = CloudHelper.getMetaFileKey(prefixSize, b);
+      String srcBlockKey = CloudHelper.getBlockKey(prefixSize, b);
+      cloudConnector.deleteObject(srcBucketID, srcMetaKey);
+      cloudConnector.deleteObject(srcBucketID, srcBlockKey);
+      if (++corrupted >= count) {
+        return;
+      }
+    }
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/HopsFSCloudTestHelper.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/HopsFSCloudTestHelper.java
new file mode 100644
index 00000000000..148e6045452
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/HopsFSCloudTestHelper.java
@@ -0,0 +1,122 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.hdfs.DFSConfigKeys;
+import org.apache.hadoop.hdfs.DistributedFileSystem;
+import org.apache.hadoop.hdfs.HdfsConfiguration;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudPersistenceProviderFactory;
+import org.apache.hadoop.io.IOUtils;
+import org.junit.FixMethodOrder;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TestName;
+import org.junit.runners.MethodSorters;
+
+import java.io.IOException;
+import java.util.Date;
+
+import static org.junit.Assert.fail;
+
+@FixMethodOrder(MethodSorters.NAME_ASCENDING)
+public class HopsFSCloudTestHelper {
+
+  static final Log LOG = LogFactory.getLog(HopsFSCloudTestHelper.class);
+  static final String testBucketPrefix = "hopsfs.unittesting.";
+  @Rule
+  public TestName name = new TestName();
+
+  static void writeFile(DistributedFileSystem dfs, String name, int size, boolean overwrite) throws IOException {
+    FSDataOutputStream os = (FSDataOutputStream) dfs.create(new Path(name), overwrite);
+    writeData(os, 0, size);
+    os.close();
+  }
+
+  public static void writeFile(FileSystem fs, String name, int size) throws IOException {
+    FSDataOutputStream os = (FSDataOutputStream) fs.create(new Path(name), (short) 1);
+    writeData(os, 0, size);
+    os.close();
+  }
+
+  static void writeData(FSDataOutputStream os, int existingSize, int size) throws IOException {
+    byte[] data = new byte[size];
+    for (int i = 0; i < size; i++, existingSize++) {
+      byte number = (byte) (existingSize % 128);
+      data[i] = number;
+    }
+    os.write(data);
+  }
+
+  /**
+   * This method reads the file using different read methods.
+   */
+  public static void verifyFile(FileSystem dfs, String file, int size) throws IOException {
+
+    //verify size
+    long actuallen = dfs.getFileStatus(new Path(file)).getLen();
+//    assertTrue("File size Mismatch. Expecting: "+size+" Got: "+actuallen, size == actuallen);
+
+    //reading one byte at a time.
+    FSDataInputStream is = dfs.open(new Path(file));
+    byte[] buffer = new byte[size];
+    IOUtils.readFully(is, buffer, 0, size);
+    is.close();
+    for (int i = 0; i < size; i++) {
+      if ((i % 128) != buffer[i]) {
+        fail("Data is corrupted. Expecting: " + (i % 128) + " got: " + buffer[i] +
+                " index: " +
+                "" + i);
+      }
+    }
+  }
+
+  public static StorageType[][] genStorageTypes(int numDataNodes) {
+
+    StorageType[][] types = new StorageType[numDataNodes][];
+    for (int i = 0; i < numDataNodes; i++) {
+      types[i] = new StorageType[]{StorageType.CLOUD,StorageType.DISK};
+    }
+    return types;
+  }
+
+  public static void purgeS3() throws IOException {
+   purgeS3(testBucketPrefix);
+  }
+
+  public static void purgeS3(String prefix) throws IOException {
+    LOG.info("HopsFS-Cloud. Purging all buckets");
+    Configuration conf = new HdfsConfiguration();
+    conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+    conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+
+    CloudPersistenceProvider cloudConnector =
+            CloudPersistenceProviderFactory.getCloudClient(conf);
+    cloudConnector.deleteAllBuckets(prefix);
+    cloudConnector.shutdown();
+  }
+
+  public static void setRandomBucketPrefix(Configuration conf, TestName name) {
+    Date date = new Date();
+    String prefix = testBucketPrefix + name.getMethodName() +
+            "." + date.getHours() + date.getMinutes() + date.getSeconds();
+    conf.set(DFSConfigKeys.S3_BUCKET_PREFIX, prefix.toLowerCase());
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java
index 23c07e59eb4..4acb2deac86 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java
@@ -1023,7 +1023,7 @@ boolean addBlock(Block blk) {
     void formBlockReport() {
       // fill remaining slots with blocks that do not exist
       for (int idx = blocks.size()-1; idx >= nrBlocks; idx--) {
-        Block block = new Block(blocks.size() - idx, 0, 0);
+        Block block = new Block(blocks.size() - idx, 0, 0, Block.NON_EXISTING_BUCKET_ID);
         blocks.set(idx, new BlockReportReplica(block));
       }
       blockReportList = BlockReport.builder(NUM_BUCKETS).build();
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudBlockReport.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudBlockReport.java
new file mode 100644
index 00000000000..86c08d70721
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudBlockReport.java
@@ -0,0 +1,844 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs.server.namenode;
+
+import io.hops.metadata.HdfsStorageFactory;
+import io.hops.metadata.hdfs.dal.BlockInfoDataAccess;
+import io.hops.metadata.hdfs.dal.INodeDataAccess;
+import io.hops.transaction.handler.HDFSOperationType;
+import io.hops.transaction.handler.LightWeightRequestHandler;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.hdfs.*;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.CloudBlock;
+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous;
+import org.apache.hadoop.hdfs.server.blockmanagement.ProvidedBlocksChecker;
+import org.apache.hadoop.hdfs.server.common.CloudHelper;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudFsDatasetImpl;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudPersistenceProviderFactory;
+import org.apache.hadoop.hdfs.server.protocol.*;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.*;
+import org.junit.rules.TestName;
+import org.junit.runners.MethodSorters;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static junit.framework.TestCase.assertTrue;
+import static org.apache.hadoop.hdfs.TestSmallFilesCreation.*;
+import static org.junit.Assert.fail;
+
+public class TestCloudBlockReport {
+
+  static final Log LOG = LogFactory.getLog(TestCloudBlockReport.class);
+  @Rule
+  public TestName testname = new TestName();
+
+  @Before
+  public void setup() {
+    Logger.getLogger(ProvidedBlocksChecker.class).setLevel(Level.DEBUG);
+  }
+
+  /**
+   * Simple block report testing
+   * <p>
+   * Write some files --> Trigger block report --> Make sure every thing is fine and dandy
+   *
+   * @throws IOException
+   */
+  @Test
+  public void TestBlockReportSimple() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      long ret = CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assertTrue("Exptected 1. Got: " + ret, 1 == ret);
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 10; i++) {
+        writeFile(dfs, "/dir/file" + i, BLKSIZE * 2);
+      }
+      CloudTestHelper.matchMetadata(conf);
+
+      pbc.scheduleBlockReportNow();
+      ret = CloudBlockReportTestHelper.waitForBRCompletion(pbc, 2);
+      assertTrue("Exptected 2. Got: " + ret, 2 == ret);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /**
+   * Block report detects and deletes abandoned blocks.
+   * Abandoned blocks happen if block delete request is
+   * lost due to network or DN failure
+   * <p>
+   * Write some files --> remove all the metadata for some files to
+   * simulate file delete operation where delete obj requset
+   * to s3 is lost --> Block report deleted the abondoned blocks
+   *
+   * @throws IOException
+   */
+  @Test
+  public void TestBlockReportAbandonedBlocks() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      long ret = CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assertTrue("Exptected 1. Got: " + ret, 1 == ret);
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 3; i++) {
+        writeFile(dfs, "/dir/file" + i, BLKSIZE * 10);
+      }
+      CloudTestHelper.matchMetadata(conf);
+
+      pbc.scheduleBlockReportNow();
+      ret = CloudBlockReportTestHelper.waitForBRCompletion(pbc, 2);
+      assertTrue("Exptected 2. Got: " + ret, ret == 2);
+
+      // creating abandoned blocks
+      deleteFileMetadata("file0");
+
+      CloudPersistenceProvider cloudConnector =
+              CloudPersistenceProviderFactory.getCloudClient(conf);
+      Map<Long, CloudBlock> cloudBlocksMap = cloudConnector.getAll("");
+      Map<Long, BlockInfoContiguous> dbBlocksMap = pbc.findAllBlocksRange(0, 1000);
+
+      assert cloudBlocksMap.size() == 30;
+      assert dbBlocksMap.size() == 20;
+
+      List<BlockInfoContiguous> toMissing = new ArrayList<>();
+      List<ProvidedBlocksChecker.BlockToMarkCorrupt> toCorrupt = new ArrayList<>();
+      List<CloudBlock> toDelete = new ArrayList<>();
+      pbc.reportDiff(dbBlocksMap, cloudBlocksMap, toMissing, toCorrupt, toDelete);
+
+      assertTrue("Exptected 10. Got: " + toDelete.size(), toDelete.size() == 10);
+      assertTrue("Exptected 0. Got: " + toMissing.size(), toMissing.size() == 0);
+      assertTrue("Exptected 0. Got: " + toCorrupt.size(), toCorrupt.size() == 0);
+
+      long brCount = pbc.getProvidedBlockReportsCount();
+      pbc.scheduleBlockReportNow();
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, brCount + 1);
+      assert pbc.getProvidedBlockReportsCount() == brCount + 1;
+
+      //Now the blocks are put in the invalidated list
+      //wait for some time to make sure that the cloud has removed the
+      //delete blocks
+
+      Thread.sleep(10000);
+
+      CloudTestHelper.matchMetadata(conf);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /**
+   * Block report detects corrupt blocks and adds the
+   * block to the URB list with corrupt priority
+   * <p>
+   * Write some files --> change GS of some blocks in the cloud
+   * --> Block report detects corrupt blocks
+   *
+   * @throws IOException
+   */
+  @Test
+  public void TestBlockReportCorruptBlocks() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assert pbc.getProvidedBlockReportsCount() == 1;
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 1; i++) {
+        writeFile(dfs, "/dir/file" + i, BLKSIZE * 10);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+
+      CloudBlockReportTestHelper.changeGSOfCloudObjs(conf, 5);
+
+      CloudPersistenceProvider cloudConnector =
+              CloudPersistenceProviderFactory.getCloudClient(conf);
+      Map<Long, CloudBlock> cloudBlocksMap = cloudConnector.getAll("");
+      Map<Long, BlockInfoContiguous> dbBlocksMap = pbc.findAllBlocksRange(0, 1000);
+
+      assert cloudBlocksMap.size() == 10;
+      assert dbBlocksMap.size() == 10;
+
+      List<BlockInfoContiguous> toMissing = new ArrayList<>();
+      List<ProvidedBlocksChecker.BlockToMarkCorrupt> toCorrupt = new ArrayList<>();
+      List<CloudBlock> toDelete = new ArrayList<>();
+      pbc.reportDiff(dbBlocksMap, cloudBlocksMap, toMissing, toCorrupt, toDelete);
+
+      assertTrue("Exptected 0. Got: " + toDelete.size(), toDelete.size() == 0);
+      assertTrue("Exptected 0. Got: " + toMissing.size(), toMissing.size() == 0);
+      assertTrue("Exptected 5. Got: " + toCorrupt.size(), toCorrupt.size() == 5);
+
+      long brCount = pbc.getProvidedBlockReportsCount();
+      pbc.scheduleBlockReportNow();
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, brCount + 1);
+      assert pbc.getProvidedBlockReportsCount() == brCount + 1;
+
+      //check
+      assert cluster.getNamesystem().getMissingBlocksCount() == 5;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /**
+   * Testing user deleting complete blocks from S3 bucket
+   *
+   * @throws IOException
+   */
+
+  @Test
+  public void TestManuallyDeletedBlocks() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_MARK_PARTIALLY_LISTED_BLOCKS_CORRUPT_AFTER_KEY,
+              30 * 1000);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assert pbc.getProvidedBlockReportsCount() == 1;
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 1; i++) {
+        writeFile(dfs, "/dir/file" + i, BLKSIZE * 10);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+
+      CloudBlockReportTestHelper.deleteBlocksAndMetaObjs(conf, 5);
+
+      long brCount = pbc.getProvidedBlockReportsCount();
+      pbc.scheduleBlockReportNow();
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, brCount + 1);
+      assert pbc.getProvidedBlockReportsCount() == brCount + 1;
+
+      assert cluster.getNamesystem().getMissingBlocksCount() == 0;
+
+      Thread.sleep(30000);
+
+      brCount = pbc.getProvidedBlockReportsCount();
+      pbc.scheduleBlockReportNow();
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, brCount + 1);
+      assert pbc.getProvidedBlockReportsCount() == brCount + 1;
+
+      //check
+      assert cluster.getNamesystem().getMissingBlocksCount() == 5;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /**
+   * Testing partial listing (when only the block or meta obj is found in S3)
+   *
+   * @throws IOException
+   */
+
+  @Test
+  public void TestBlockReportPartialListing() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_MARK_PARTIALLY_LISTED_BLOCKS_CORRUPT_AFTER_KEY,
+              30 * 1000);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assert pbc.getProvidedBlockReportsCount() == 1;
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 1; i++) {
+        writeFile(dfs, "/dir/file" + i, BLKSIZE * 10);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+
+      CloudBlockReportTestHelper.deleteMetaObjects(conf, 5);
+
+      long brCount = pbc.getProvidedBlockReportsCount();
+      pbc.scheduleBlockReportNow();
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, brCount + 1);
+      assert pbc.getProvidedBlockReportsCount() == brCount + 1;
+
+      assert cluster.getNamesystem().getMissingBlocksCount() == 0;
+
+      Thread.sleep(30000);
+
+      brCount = pbc.getProvidedBlockReportsCount();
+      pbc.scheduleBlockReportNow();
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, brCount + 1);
+      assert pbc.getProvidedBlockReportsCount() == brCount + 1;
+
+      //check
+      assert cluster.getNamesystem().getMissingBlocksCount() == 5;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+
+  @Test
+  public void TestBlockReportMultipleErrors() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_MARK_PARTIALLY_LISTED_BLOCKS_CORRUPT_AFTER_KEY,
+              20 * 1000);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assert pbc.getProvidedBlockReportsCount() == 1;
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 20; i++) {
+        writeFile(dfs, "/dir/file" + i, BLKSIZE * 1);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+
+      deleteFileMetadata("file9");
+      deleteFileMetadata("file8");
+      CloudBlockReportTestHelper.deleteMetaObjects(conf, 2);
+      CloudBlockReportTestHelper.changeGSOfCloudObjs(conf, 2);
+
+      long brCount = pbc.getProvidedBlockReportsCount();
+      pbc.scheduleBlockReportNow();
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, brCount + 1);
+      assert pbc.getProvidedBlockReportsCount() == brCount + 1;
+
+      long count = cluster.getNamesystem().getMissingBlocksCount();
+      assertTrue("Exptected: " + 2 + " Got: " + count, count == 2);
+
+      //partially listed blocks takes 30 sec
+      Thread.sleep(25000);
+
+      brCount = pbc.getProvidedBlockReportsCount();
+      pbc.scheduleBlockReportNow();
+      CloudBlockReportTestHelper.waitForBRCompletion(pbc, brCount + 1);
+      assert pbc.getProvidedBlockReportsCount() == brCount + 1;
+
+      //check
+      long missingBlkCount = cluster.getNamesystem().getMissingBlocksCount();
+      assertTrue("Expected : 4 Got: " + missingBlkCount, missingBlkCount == 4);
+
+      count = CloudTestHelper.getAllCloudBlocks(CloudPersistenceProviderFactory
+              .getCloudClient(conf)).size();
+      assertTrue(" Expected : " + 18 + " Got: " + count, 18 == count);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  private static List<INode> deleteFileMetadata(final String name) throws IOException {
+    LightWeightRequestHandler handler =
+            new LightWeightRequestHandler(HDFSOperationType.TEST) {
+              @Override
+              public Object performTask() throws IOException {
+                INodeDataAccess ida = (INodeDataAccess) HdfsStorageFactory
+                        .getDataAccess(INodeDataAccess.class);
+                BlockInfoDataAccess bda = (BlockInfoDataAccess) HdfsStorageFactory
+                        .getDataAccess(BlockInfoDataAccess.class);
+                List<INode> inodes = ida.findINodes(name);
+                assert inodes.size() == 1;
+                ida.deleteInode(name);
+                bda.deleteBlocksForFile(inodes.get(0).getId());
+                return null;
+              }
+            };
+    return (List<INode>) handler.handle();
+  }
+
+  @Test
+  public void TestBlockReportOpenFiles() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+      final int corruptAfter = 10 * 1000;
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_MARK_PARTIALLY_LISTED_BLOCKS_CORRUPT_AFTER_KEY,
+              corruptAfter);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      long ret = CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assertTrue("Exptected 1. Got: " + ret, 1 == ret);
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      FSDataOutputStream os = (FSDataOutputStream) dfs.create(new Path("/dir/file"), (short) 1);
+      byte[] data = new byte[BLKSIZE];
+      os.write(data);
+
+      CloudTestHelper.matchMetadata(conf, true);
+
+      Thread.sleep(corruptAfter + 1000);
+
+      CloudPersistenceProvider cloudConnector =
+              CloudPersistenceProviderFactory.getCloudClient(conf);
+      Map<Long, CloudBlock> cloudBlocksMap = cloudConnector.getAll("");
+      Map<Long, BlockInfoContiguous> dbBlocksMap = pbc.findAllBlocksRange(0, 1000);
+
+      assert cloudBlocksMap.size() == 0;
+      assert dbBlocksMap.size() == 1;
+
+      List<BlockInfoContiguous> toMissing = new ArrayList<>();
+      List<ProvidedBlocksChecker.BlockToMarkCorrupt> toCorrupt = new ArrayList<>();
+      List<CloudBlock> toDelete = new ArrayList<>();
+      pbc.reportDiff(dbBlocksMap, cloudBlocksMap, toMissing, toCorrupt, toDelete);
+
+      assertTrue("Exptected 0. Got: " + toDelete.size(), toDelete.size() == 0);
+      assertTrue("Exptected 0. Got: " + toMissing.size(), toMissing.size() == 0);
+      assertTrue("Exptected 0. Got: " + toCorrupt.size(), toCorrupt.size() == 0);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestCloudRBWBR() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 64 * 1024 * 1024;
+      final int NUM_DN = 1;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      long ret = CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assertTrue("Exptected 1. Got: " + ret, 1 == ret);
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      FSDataOutputStream out = (FSDataOutputStream) dfs.create(new Path("/dir/file"), (short) 1);
+      byte[] data = new byte[BLKSIZE + BLKSIZE / 2]; // 1 1/2 blocks
+      out.write(data);
+
+      String poolId = cluster.getNamesystem().getBlockPoolId();
+      Map<DatanodeStorage, BlockReport> brs =
+              cluster.getDataNodes().get(0).getFSDataset().getBlockReports(poolId);
+
+      //there should be only one block in the BR for CLOUD volume
+      for (DatanodeStorage storage : brs.keySet()) {
+        BlockReport br = brs.get(storage);
+        if (storage.getStorageType() == StorageType.CLOUD) {
+          assert br.getNumberOfBlocks() == 1;
+        } else {
+          assert br.getNumberOfBlocks() == 0;
+        }
+      }
+
+      CloudTestHelper.matchMetadata(conf, true);
+
+      CloudPersistenceProvider cloudConnector =
+              CloudPersistenceProviderFactory.getCloudClient(conf);
+      Map<Long, CloudBlock> cloudBlocksMap = cloudConnector.getAll("");
+      assert cloudBlocksMap.size() == 1;
+
+      cluster.getDataNodes().get(0).scheduleAllBlockReport(0);
+      Thread.sleep(10000);
+
+      out.close();
+
+      brs = cluster.getDataNodes().get(0).getFSDataset().getBlockReports(poolId);
+
+      //there should be only one block in the BR for CLOUD volume
+      for (DatanodeStorage storage : brs.keySet()) {
+        BlockReport br = brs.get(storage);
+        if (storage.getStorageType() == StorageType.CLOUD) {
+          assert br.getNumberOfBlocks() == 0;
+        } else {
+          assert br.getNumberOfBlocks() == 0;
+        }
+      }
+
+      cloudBlocksMap = cloudConnector.getAll("");
+      assert cloudBlocksMap.size() == 2;
+
+      CloudTestHelper.matchMetadata(conf);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /*
+  * Handling of incremental BR of a block that doest not belong to any file
+   */
+  @Test
+  public void TestCloudDanglingIBR() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 64 * 1024 * 1024;
+      final int NUM_DN = 1;
+      final int prefixSize = 10;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, prefixSize);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      File file = new File(cluster.getDataDirectory()+"/tmp-blk");
+      FileWriter outblk = new FileWriter(file);
+      outblk.close();
+
+      Block blk = new Block(1, 0, 1, (short)0);
+      String blkKey = CloudHelper.getBlockKey( prefixSize, blk);
+      String metaKey = CloudHelper.getMetaFileKey( prefixSize, blk);
+
+      Map<String, String> metadata = new HashMap<>();
+      CloudPersistenceProvider cloudConnector =
+              CloudPersistenceProviderFactory.getCloudClient(conf);
+      cloudConnector.uploadObject((short)0, blkKey, file, metadata);
+      cloudConnector.uploadObject((short)0, metaKey, file, metadata);
+
+
+      String bpid = cluster.getNamesystem().getBlockPoolId();
+      DatanodeRegistration nodeReg = cluster.getDataNodes().get(0).getDNRegistrationForBP(bpid);
+      String storageID =
+              ((CloudFsDatasetImpl)cluster.getDataNodes().get(0).getFSDataset()).getCloudVolume().getStorageID();
+      DatanodeStorage datanodeStorage =
+              ((CloudFsDatasetImpl)cluster.getDataNodes().get(0).getFSDataset()).getStorage(storageID);
+      assert storageID != null;
+      StorageReceivedDeletedBlocks[] receivedAndDeletedBlocks =
+              new StorageReceivedDeletedBlocks[1];
+
+      ReceivedDeletedBlockInfo[] blocks = new ReceivedDeletedBlockInfo[1];
+      blocks[0] = new ReceivedDeletedBlockInfo(blk,
+              ReceivedDeletedBlockInfo.BlockStatus.RECEIVED_BLOCK, null );
+      receivedAndDeletedBlocks[0] = new StorageReceivedDeletedBlocks(datanodeStorage, blocks);
+
+      cluster.getNameNodeRpc().blockReceivedAndDeleted(nodeReg, bpid, receivedAndDeletedBlocks);
+
+      Thread.sleep(10000);
+
+      assert cloudConnector.getAll("").size() == 0;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @AfterClass
+  public static void TestZDeleteAllBuckets() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+  }
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudDiskCache.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudDiskCache.java
new file mode 100644
index 00000000000..582168aed15
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudDiskCache.java
@@ -0,0 +1,292 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs.server.namenode;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CloudProvider;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.*;
+import org.apache.hadoop.hdfs.server.datanode.DataNode;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.*;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.AfterClass;
+import org.junit.FixMethodOrder;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TestName;
+import org.junit.runners.MethodSorters;
+import org.mockito.Mockito;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collection;
+
+import static org.apache.hadoop.hdfs.TestSmallFilesCreation.verifyFile;
+import static org.junit.Assert.fail;
+import static org.mockito.Matchers.*;
+
+public class TestCloudDiskCache {
+
+  static final Log LOG = LogFactory.getLog(TestCloudDiskCache.class);
+  @Rule
+  public TestName testname = new TestName();
+
+  @Test
+  public void TestDiskCache() throws IOException {
+
+    Logger.getLogger(ProvidedBlocksCacheCleaner.class).setLevel(Level.DEBUG);
+
+    HopsFSCloudTestHelper.purgeS3();
+    final Logger logger = Logger.getRootLogger();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 1;
+      final int BLKSIZE = 1 * 1024 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_ACTIVATION_PRECENTAGE_KEY, 90);
+      conf.setInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_BATCH_SIZE_KEY, 1);
+      conf.setInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_CHECK_INTERVAL_KEY, 1000);
+      conf.setInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_WAIT_KEY, 10000);  //The cached block
+      // has to be atlease 10 sec old before it can be deleted
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      FsVolumeImpl v =
+              ((CloudFsDatasetImpl) cluster.getDataNodes().get(0).getFSDataset()).getCloudVolume();
+      BlockPoolSlice slice = v.getBlockPoolSlice(cluster.getNamesystem(0).getBlockPoolId());
+      ProvidedBlocksCacheCleaner cleaner = slice.getProvidedBlocksCacheCleaner();
+      ProvidedBlocksCacheDiskUtilization uti = cleaner.getDiskUtilizationCalc();
+      ProvidedBlocksCacheDiskUtilization mockedCalc = Mockito.spy(uti);
+      cleaner.setDiskUtilizationMock(mockedCalc);
+
+      LOG.info("HopsFS-Cloud. Setting new mock obj with fixed disk utilization of 0%");
+      Answer diskUtilization = new Answer() {
+        @Override
+        public Object answer(InvocationOnMock invocationOnMock) throws Throwable {
+          LOG.info("HopsFS-Cloud. Mocked. Cache disk utilization is 0%");
+          return new Double(0);
+        }
+      };
+      Mockito.doAnswer(diskUtilization).when(mockedCalc).getDiskUtilization();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+      dfs.mkdirs(new Path("/dir"));
+      String file1 = "/dir/file";
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      int totalBlks = 10;
+      LOG.info("HopsFS-Cloud. Writing File1");
+      HopsFSCloudTestHelper.writeFile(dfs, file1, BLKSIZE * totalBlks);
+      assert cleaner.getCachedFilesCount() == totalBlks * 2; // blocks + meta files
+
+      diskUtilization = new Answer() {
+        @Override
+        public Object answer(InvocationOnMock invocationOnMock) throws Throwable {
+          LOG.info("HopsFS-Cloud. Mocked. Cache disk utilization is 90%");
+          return new Double(90); //disk it full cleaner will empty the cache to make room for
+          // new blocks;
+        }
+      };
+
+      Mockito.doAnswer(diskUtilization).when(mockedCalc).getDiskUtilization();
+      cleaner.setDiskUtilizationMock(mockedCalc);
+      LOG.info("HopsFS-Cloud. Setting new mock obj with fixed disk utilization of 90%");
+
+      String file2 = "/dir/file2";
+      HopsFSCloudTestHelper.writeFile(dfs, file2, BLKSIZE * totalBlks);
+
+      Thread.sleep(20000); // wait for the cleaner to remove cached blocks
+
+      assert cleaner.getCachedFilesCount() == 0;
+
+      LOG.info("HopsFS-Cloud. Reading File.");
+      verifyFile(dfs, file1, BLKSIZE * totalBlks);
+      verifyFile(dfs, file2, BLKSIZE * totalBlks);
+
+      Thread.sleep(20000); // wait for the cleaner to remove cached blocks
+      assert cleaner.getCachedFilesCount() == 0;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /*
+  make sure that when a file is read multiple time it is redirected to
+  same datanode that contains the block in its cache
+   */
+  @Test
+  public void TestDiskCache2() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    final Logger logger = Logger.getRootLogger();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 1 * 1024 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_BATCH_SIZE_KEY, 1);
+      conf.setInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_CHECK_INTERVAL_KEY, 1000);
+
+      // DFS_DN_CLOUD_CACHE_DELETE_ACTIVATION_PRECENTAGE_KEY to high number to
+      // prevent cache cleaner from deleting block.
+      conf.setInt(DFSConfigKeys.DFS_DN_CLOUD_CACHE_DELETE_ACTIVATION_PRECENTAGE_KEY, 99);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      for (int i = 0; i < NUM_DN; i++) {
+        CloudFsDatasetImpl data = (CloudFsDatasetImpl) cluster.getDataNodes().get(i).getFSDataset();
+        CloudPersistenceProvider cloud = data.getCloudConnector();
+
+        final CloudPersistenceProvider cloudMock = Mockito.spy(cloud);
+        data.installMockCloudConnector(cloudMock);
+
+        Answer checker = new Answer() {
+          @Override
+          public Object answer(InvocationOnMock invocationOnMock) throws Throwable {
+            String error = "Every thing should have been read from the cache.";
+            LOG.error(error);
+            throw new IllegalStateException(error);
+          }
+        };
+
+        Mockito.doAnswer(checker).when(cloudMock).downloadObject(anyShort(), anyString(),
+                (File) anyObject());
+      }
+
+      int totalBlks = 10;
+      DistributedFileSystem dfs = cluster.getFileSystem();
+      dfs.mkdirs(new Path("/dir"));
+      String file = "/dir/file";
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+      HopsFSCloudTestHelper.writeFile(dfs, file, BLKSIZE * totalBlks);
+
+      LOG.info("HopsFS-Cloud. Reading File.");
+      verifyFile(dfs, file, BLKSIZE * totalBlks);
+
+      LOG.info("HopsFS-Cloud. Reading File Again");
+      verifyFile(dfs, file, BLKSIZE * totalBlks);
+
+      LOG.info("HopsFS-Cloud. " + conf.get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY));
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /*
+  make sure that block delete requests are send to datanodes that
+  store cached copies of the blocks
+   */
+  @Test
+  public void TestDeleteFile() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      final int BLK_SIZE = 128 * 1024;
+      final int BLK_PER_FILE = 3;
+      final int FILESIZE = BLK_PER_FILE * BLK_SIZE;
+      final int NUM_DN = 5;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLK_SIZE);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 10; i++) {
+        HopsFSCloudTestHelper.writeFile(dfs, "/dir/file" + i, FILESIZE);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+
+      dfs.delete(new Path("/dir"), true);
+
+      Thread.sleep(10000);
+
+      CloudTestHelper.matchMetadata(conf);
+
+      for (DataNode dn : cluster.getDataNodes()) {
+        CloudFsDatasetImpl data = (CloudFsDatasetImpl) dn.getFSDataset();
+        CloudFsVolumeImpl vol = (CloudFsVolumeImpl) data.getCloudVolume();
+        File dir = vol.getCacheDir(cluster.getNamesystem().getBlockPoolId());
+
+        Collection<File> files = FileUtils.listFiles(dir, null, true);
+        if (files.size() != 0) {
+          LOG.info("HopsFS-Cloud. Cached Files : " + Arrays.toString(files.toArray()));
+          fail();
+        }
+        assert vol.getBlockPoolSlice(cluster.getNamesystem().getBlockPoolId()).
+                getProvidedBlocksCacheCleaner().getCachedFilesCount() == 0;
+      }
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @AfterClass
+  public static void TestZDeleteAllBuckets() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudFailures.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudFailures.java
new file mode 100644
index 00000000000..caae8e7e05c
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudFailures.java
@@ -0,0 +1,148 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CloudProvider;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.StorageType;
+import org.apache.hadoop.hdfs.*;
+import org.apache.hadoop.hdfs.client.HdfsClientConfigKeys;
+import org.apache.hadoop.hdfs.protocol.CloudBlock;
+import org.apache.hadoop.hdfs.server.blockmanagement.ProvidedBlocksChecker;
+import org.apache.hadoop.hdfs.server.datanode.DataNode;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.CloudPersistenceProvider;
+import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CloudPersistenceProviderFactory;
+import org.apache.hadoop.hdfs.server.protocol.BlockReport;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeStorage;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.util.ExitUtil;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.*;
+import org.junit.rules.TestName;
+import org.junit.runners.MethodSorters;
+
+import java.io.IOException;
+import java.util.Map;
+
+import static junit.framework.TestCase.assertTrue;
+import static org.apache.hadoop.hdfs.TestSmallFilesCreation.writeFile;
+import static org.junit.Assert.fail;
+
+public class TestCloudFailures {
+
+  static final Log LOG = LogFactory.getLog(TestCloudFailures.class);
+
+  @Rule
+  public TestName testname = new TestName();
+
+  @Before
+  public void setup() {
+//    Logger.getRootLogger().setLevel(Level.DEBUG);
+    Logger.getLogger(ProvidedBlocksChecker.class).setLevel(Level.DEBUG);
+    Logger.getLogger(CloudTestHelper.class).setLevel(Level.DEBUG);
+  }
+
+  Configuration getConf() {
+    Configuration conf = new Configuration();
+    conf.setInt(DFSConfigKeys.DFS_CLIENT_FAILOVER_MAX_ATTEMPTS_KEY, /*default 15*/ 1);
+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_MAX_ATTEMPTS_KEY, /*default 10*/ 1);
+    conf.setInt(DFSConfigKeys.DFS_CLIENT_FAILOVER_SLEEPTIME_BASE_KEY, /*default 500*/ 500);
+    conf.setInt(DFSConfigKeys.DFS_CLIENT_FAILOVER_SLEEPTIME_MAX_KEY, /*default 15000*/1000);
+    conf.setInt(DFSConfigKeys.DFS_CLIENT_FAILOVER_CONNECTION_RETRIES_KEY, /*default 0*/ 0);
+    conf.setInt(DFSConfigKeys.DFS_CLIENT_FAILOVER_CONNECTION_RETRIES_ON_SOCKET_TIMEOUTS_KEY,
+            /*default 0*/0);
+    conf.setInt(DFSConfigKeys.IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SOCKET_TIMEOUTS_KEY, /*default
+    45*/ 2);
+    conf.setInt(DFSConfigKeys.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, /*default 10*/ 1);
+    conf.set(HdfsClientConfigKeys.Retry.POLICY_SPEC_KEY, "1000,2");
+
+    return conf;
+  }
+
+  @Test
+  public void TestClientFailure() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = getConf();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+      cluster.setLeasePeriod(3 * 1000, 5 * 1000);
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      long ret = CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assertTrue("Exptected 1. Got: " + ret, 1 == ret);
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      FSDataOutputStream out = (FSDataOutputStream) dfs.create(new Path("/dir/file"), (short) 1);
+      byte[] data = new byte[BLKSIZE]; //write more than 64 KB to allocate a block
+      out.write(data);
+
+      Thread.sleep(6000);
+      dfs.getClient().getLeaseRenewer().interruptAndJoin();
+      dfs.getClient().abort();
+
+      LOG.info("HopsFS-Cloud. Aborted the client");
+      Thread.sleep(10000);
+
+      //this will check that the number of blocks in the cloud and DB are same
+      CloudTestHelper.matchMetadata(conf);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @AfterClass
+  public static void TestZDeleteAllBuckets() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+  }
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudFileCreation.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudFileCreation.java
new file mode 100644
index 00000000000..94f0c5700ae
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudFileCreation.java
@@ -0,0 +1,1093 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.hdfs.*;
+import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;
+import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;
+import org.apache.hadoop.hdfs.protocol.HdfsConstants;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;
+import org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks;
+import org.apache.log4j.Level;
+import org.junit.AfterClass;
+import org.junit.FixMethodOrder;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TestName;
+import org.junit.runners.MethodSorters;
+import org.mockito.Mockito;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+
+import java.io.IOException;
+import java.util.*;
+
+import static org.apache.hadoop.hdfs.TestSmallFilesCreation.*;
+import static org.junit.Assert.*;
+import static org.mockito.Matchers.*;
+
+import org.apache.log4j.Logger;
+import org.apache.log4j.spi.LoggingEvent;
+
+public class TestCloudFileCreation {
+
+  static final Log LOG = LogFactory.getLog(TestCloudFileCreation.class);
+  @Rule
+  public TestName testname = new TestName();
+
+
+  /**
+   * Simple read and write test
+   *
+   * @throws IOException
+   */
+  @Test
+  public void TestSimpleReadAndWrite() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int FILESIZE = 2 * BLKSIZE;
+
+      final String FILE_NAME1 = "/dir/TEST-FLIE1";
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf,testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      writeFile(dfs, FILE_NAME1, FILESIZE);
+      verifyFile(dfs, FILE_NAME1, FILESIZE);
+
+      CloudTestHelper.matchMetadata(conf);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestEnv() {
+    String path = System.getenv("AWS_CREDENTIAL_PROFILES_FILE");
+
+    assertTrue("Unit test could not read AWS credential file", path != null);
+  }
+
+  @Test
+  public void TestListing() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      final int BLK_SIZE = 128 * 1024;
+      final int BLK_PER_FILE = 3;
+      final int FILESIZE = BLK_PER_FILE * BLK_SIZE;
+      final int NUM_FILES = 1;
+      final int NUM_DN = 5;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLK_SIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < NUM_FILES; i++) {
+        writeFile(dfs, "/dir/file" + i, FILESIZE);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+
+      FileStatus[] filesStatus = dfs.listStatus(new Path("/dir"));
+
+      assert filesStatus.length == NUM_FILES;
+
+      RemoteIterator<LocatedFileStatus> locatedFilesStatus = dfs.listLocatedStatus(new Path("/dir"));
+
+      int count = 0;
+      while (locatedFilesStatus.hasNext()) {
+        LocatedFileStatus locFileStatus = locatedFilesStatus.next();
+
+        assert locFileStatus.getBlockLocations().length == BLK_PER_FILE;
+
+        count += locFileStatus.getBlockLocations().length;
+      }
+
+      assert count == NUM_FILES * BLK_PER_FILE;
+
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+
+  }
+
+  @Test
+  public void TestDeleteFile() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      final int BLK_SIZE = 128 * 1024;
+      final int BLK_PER_FILE = 3;
+      final int FILESIZE = BLK_PER_FILE * BLK_SIZE;
+      final int NUM_DN = 5;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLK_SIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      writeFile(dfs, "/dir/file", FILESIZE);
+
+      CloudTestHelper.matchMetadata(conf);
+
+      dfs.delete(new Path("/dir/file"), true);
+
+      Thread.sleep(10000);
+
+      CloudTestHelper.matchMetadata(conf);
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+
+  @Test
+  public void TestDeleteDir() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int BLK_SIZE = 128 * 1024;
+      final int BLK_PER_FILE = 3;
+      final int FILESIZE = BLK_PER_FILE * BLK_SIZE;
+      final int NUM_DN = 5;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLK_SIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.mkdirs(new Path("/dir/dir1"));
+      dfs.mkdirs(new Path("/dir/dir2"));
+
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 5; i++) {
+        String file = "/dir/dir1/dir-dir1-file-" + i;
+        writeFile(dfs, file, FILESIZE);
+        verifyFile(dfs, file, FILESIZE);
+      }
+
+      for (int i = 0; i < 5; i++) {
+        String file = "/dir/dir2/dir-dir2-file-" + i;
+        writeFile(dfs, file, FILESIZE);
+        verifyFile(dfs, file, FILESIZE);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size() == 10 * 3;
+
+      dfs.delete(new Path("/dir"), true);
+
+      // sleep to make sure that the objects from the cloud storage
+      // have been deleted
+      Thread.sleep(20000);
+
+      CloudTestHelper.matchMetadata(conf);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestRename() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int BLK_SIZE = 128 * 1024;
+      final int BLK_PER_FILE = 3;
+      final int FILESIZE = BLK_PER_FILE * BLK_SIZE;
+      final int NUM_DN = 5;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLK_SIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 5; i++) {
+        writeFile(dfs, "/dir/file-" + i, FILESIZE);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size() == 5 * 3;
+
+      for (int i = 0; i < 5; i++) {
+        dfs.rename(new Path("/dir/file-" + i), new Path("/dir/file-new-" + i));
+      }
+
+      for (int i = 0; i < 5; i++) {
+        verifyFile(dfs, "/dir/file-new-" + i, FILESIZE);
+      }
+
+      CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size() == 5 * 3;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestOverWrite() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int BLK_SIZE = 128 * 1024;
+      final int BLK_PER_FILE = 3;
+      final int FILESIZE = BLK_PER_FILE * BLK_SIZE;
+      final int NUM_DN = 5;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLK_SIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      writeFile(dfs, "/dir/file1", FILESIZE);
+
+      CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size() == BLK_PER_FILE;
+
+      // now overrite this file
+      writeFile(dfs, "/dir/file1", FILESIZE);
+      assert CloudTestHelper.findAllBlocks().size() == BLK_PER_FILE;
+
+      Thread.sleep(20000); //wait so that cloud bocks are deleted
+
+      CloudTestHelper.matchMetadata(conf);
+
+      writeFile(dfs, "/dir/file2", FILESIZE);
+
+      assert CloudTestHelper.findAllBlocks().size() == 2 * BLK_PER_FILE;
+      CloudTestHelper.matchMetadata(conf);
+
+      dfs.rename(new Path("/dir/file1"), new Path("/dir/file2"), Options.Rename.OVERWRITE);
+      assert CloudTestHelper.findAllBlocks().size() == 1 * BLK_PER_FILE;
+
+      Thread.sleep(20000); //wait so that cloud bocks are deleted
+      CloudTestHelper.matchMetadata(conf);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestAppend() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int BLK_SIZE = 128 * 1024;
+      final int BLK_PER_FILE = 1;
+      final int FILESIZE = BLK_PER_FILE * BLK_SIZE;
+      final int NUM_DN = 5;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLK_SIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      int initialSize = FSNamesystem.getMaxSmallFileSize() + 1;
+      writeFile(dfs, "/dir/file1", initialSize);  // write to cloud
+
+      CloudTestHelper.matchMetadata(conf);
+
+      final int APPEND_SIZE = 512;
+      final int TIMES = 10;
+      //append 1 byte ten times
+      for (int i = 0; i < TIMES; i++) {
+        FSDataOutputStream out = dfs.append(new Path("/dir/file1"));
+        HopsFSCloudTestHelper.writeData(out, initialSize + APPEND_SIZE * i, APPEND_SIZE);
+        out.close();
+      }
+
+      verifyFile(dfs, "/dir/file1", initialSize + APPEND_SIZE * TIMES);
+
+      CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size() == TIMES + 1;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /*
+  appending to a file stored in DB.
+   */
+  @Test
+  public void TestAppend2() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int BLK_SIZE = 128 * 1024;
+      final int BLK_PER_FILE = 1;
+      final int NUM_DN = 5;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLK_SIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      final int ONDISK_SMALL_BUCKET_SIZE = FSNamesystem.getDBOnDiskSmallBucketSize();
+      final int ONDISK_MEDIUM_BUCKET_SIZE = FSNamesystem.getDBOnDiskMediumBucketSize();
+      final int MAX_SMALL_FILE_SIZE = FSNamesystem.getMaxSmallFileSize();
+      final int INMEMORY_BUCKET_SIZE = FSNamesystem.getDBInMemBucketSize();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      String FILE_NAME1 = "/dir/file";
+      writeFile(dfs, FILE_NAME1, INMEMORY_BUCKET_SIZE);
+      verifyFile(dfs, FILE_NAME1, INMEMORY_BUCKET_SIZE);
+
+      //validate
+      assertTrue("Expecting 1 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 0);
+      CloudTestHelper.matchMetadata(conf);
+
+      FSDataOutputStream out = dfs.append(new Path(FILE_NAME1));
+      writeData(out, INMEMORY_BUCKET_SIZE, ONDISK_SMALL_BUCKET_SIZE - INMEMORY_BUCKET_SIZE);
+      out.close();
+      verifyFile(dfs, FILE_NAME1, ONDISK_SMALL_BUCKET_SIZE);
+
+      //validate
+      assertTrue("Expecting 0 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 0);
+      CloudTestHelper.matchMetadata(conf);
+
+      out = dfs.append(new Path(FILE_NAME1));
+      writeData(out, ONDISK_SMALL_BUCKET_SIZE, ONDISK_MEDIUM_BUCKET_SIZE - ONDISK_SMALL_BUCKET_SIZE);
+      out.close();
+      verifyFile(dfs, FILE_NAME1, ONDISK_MEDIUM_BUCKET_SIZE);
+      //validate
+      assertTrue("Expecting 0 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 0);
+      CloudTestHelper.matchMetadata(conf);
+
+      out = dfs.append(new Path(FILE_NAME1));
+      writeData(out, ONDISK_MEDIUM_BUCKET_SIZE, MAX_SMALL_FILE_SIZE - ONDISK_MEDIUM_BUCKET_SIZE);
+      out.close();
+      verifyFile(dfs, FILE_NAME1, MAX_SMALL_FILE_SIZE);
+      //validate
+      assertTrue("Expecting 0 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 1);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 0);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 1);
+      CloudTestHelper.matchMetadata(conf);
+
+
+      out = dfs.append(new Path(FILE_NAME1));
+      writeData(out, MAX_SMALL_FILE_SIZE, 1);
+      out.close();
+      verifyFile(dfs, FILE_NAME1, MAX_SMALL_FILE_SIZE+1);
+      //validate
+      assertTrue("Expecting 0 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 0);
+      assertTrue("Expecting 0 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 0);
+      CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size()==1;
+
+      out = dfs.append(new Path(FILE_NAME1));
+      writeData(out, MAX_SMALL_FILE_SIZE+1, 1);
+      out.close();
+      verifyFile(dfs, FILE_NAME1, MAX_SMALL_FILE_SIZE+2);
+      CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size()==2;
+
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+
+  // store small files in DB and large files in cloud
+  @Test
+  public void TestSmallAndLargeFiles() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      final int ONDISK_SMALL_BUCKET_SIZE = FSNamesystem.getDBOnDiskSmallBucketSize();
+      final int ONDISK_MEDIUM_BUCKET_SIZE = FSNamesystem.getDBOnDiskMediumBucketSize();
+      final int MAX_SMALL_FILE_SIZE = FSNamesystem.getMaxSmallFileSize();
+      final int INMEMORY_BUCKET_SIZE = FSNamesystem.getDBInMemBucketSize();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+      dfs.mkdirs(new Path("/dir2"));
+      dfs.setStoragePolicy(new Path("/dir2"), "DB");
+
+      final String FILE_NAME1 = "/dir/TEST-FLIE1";
+      final String FILE_NAME2 = "/dir/TEST-FLIE2";
+      final String FILE_NAME3 = "/dir/TEST-FLIE3";
+      final String FILE_NAME4 = "/dir/TEST-FLIE4";
+      final String FILE_NAME5 = "/dir/TEST-FLIE5";
+      final String FILE_NAME6 = "/dir/TEST-FLIE6";
+
+      //write small files
+      writeFile(dfs, FILE_NAME1, INMEMORY_BUCKET_SIZE);
+      verifyFile(dfs, FILE_NAME1, INMEMORY_BUCKET_SIZE);
+      writeFile(dfs, FILE_NAME2, ONDISK_SMALL_BUCKET_SIZE);
+      verifyFile(dfs, FILE_NAME2, ONDISK_SMALL_BUCKET_SIZE);
+      writeFile(dfs, FILE_NAME3, ONDISK_MEDIUM_BUCKET_SIZE);
+      verifyFile(dfs, FILE_NAME3, ONDISK_MEDIUM_BUCKET_SIZE);
+      writeFile(dfs, FILE_NAME4, MAX_SMALL_FILE_SIZE);
+      verifyFile(dfs, FILE_NAME4, MAX_SMALL_FILE_SIZE);
+
+      //validate
+      assertTrue("Expecting 1 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 1);
+      assertTrue("Expecting 3 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 3);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 1);
+
+
+      //now write large files. these should be stored in the cloud
+      writeFile(dfs, FILE_NAME5, MAX_SMALL_FILE_SIZE + 1); // 1 block
+      verifyFile(dfs, FILE_NAME5, MAX_SMALL_FILE_SIZE + 1);
+
+      writeFile(dfs, FILE_NAME6, 10 * BLKSIZE);  // 10 blocks
+      verifyFile(dfs, FILE_NAME6, 10 * BLKSIZE);
+      assert CloudTestHelper.findAllBlocks().size() == 11;  // 11 blocks so far
+      CloudTestHelper.matchMetadata(conf);
+
+      final String FILE_NAME7 = "/dir2/TEST-FLIE7";
+      final String FILE_NAME8 = "/dir2/TEST-FLIE8";
+      final String FILE_NAME9 = "/dir2/TEST-FLIE9";
+
+      writeFile(dfs, FILE_NAME7, MAX_SMALL_FILE_SIZE);  // goes to DB
+      verifyFile(dfs, FILE_NAME7, MAX_SMALL_FILE_SIZE);
+
+      writeFile(dfs, FILE_NAME8, MAX_SMALL_FILE_SIZE + 1);  // goes to DNs disks. 1 blk
+      verifyFile(dfs, FILE_NAME8, MAX_SMALL_FILE_SIZE + 1);
+
+      writeFile(dfs, FILE_NAME9, BLKSIZE * 10); // goes to DNs disks. 10 blks. total of 22 blks so far
+      verifyFile(dfs, FILE_NAME9, BLKSIZE * 10);
+
+      assert CloudTestHelper.findAllBlocks().size() == 22;
+//      assert CloudTestHelper.getAllCloudBlocks(cloud).size() == 11;
+
+
+      assertTrue("Expecting 1 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 1);
+      assertTrue("Expecting 4 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 4);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 1);
+      assertTrue("Expecting 2 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 2);
+
+      dfs.delete(new Path("/dir"), true);
+      dfs.delete(new Path("/dir2"), true);
+      Thread.sleep(20000);
+      CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size() == 0;
+
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestSetReplication() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      final int MAX_SMALL_FILE_SIZE = FSNamesystem.getMaxSmallFileSize();
+
+      final int S = FSNamesystem.getDBInMemBucketSize();
+      final String FILE_NAME1 = "/dir/TEST-FLIE1";
+      final String FILE_NAME2 = "/dir/TEST-FLIE2";
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      writeFile(dfs, FILE_NAME1, MAX_SMALL_FILE_SIZE);
+      writeFile(dfs, FILE_NAME2, BLKSIZE);
+
+      assertTrue("Count of db file should be 1", countAllOnDiskDBFiles() == 1);
+
+      dfs.setReplication(new Path(FILE_NAME1), (short) 10);
+      dfs.setReplication(new Path(FILE_NAME2), (short) 10);
+
+      if (dfs.getFileStatus(new Path(FILE_NAME1)).getReplication() != 10) {
+        fail("Unable to set replication for a small file");
+      }
+
+      if (dfs.getFileStatus(new Path(FILE_NAME2)).getReplication() != 10) {
+        fail("Unable to set replication for a large file");
+      }
+
+      assert CloudTestHelper.matchMetadata(conf);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestSmallFileHsync() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    TestHsyncORFlush(true);
+  }
+
+  @Test
+  public void TestSmallFileHflush() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    TestHsyncORFlush(false);
+  }
+
+  public void TestHsyncORFlush(boolean hsync) throws IOException {
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      final String FILE_NAME = "/dir/TEST-FLIE";
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      final int TIMES = 10;
+      final int SYNC_SIZE = 1024;
+      FSDataOutputStream out = dfs.create(new Path(FILE_NAME), (short) 3);
+      HopsFSCloudTestHelper.writeData(out, 0, SYNC_SIZE);
+
+      for (int i = 1; i <= TIMES; i++) {
+        if (hsync) {
+          out.hsync();   // syncs and closes a block
+        } else {
+          out.hflush();
+        }
+        HopsFSCloudTestHelper.writeData(out, i * SYNC_SIZE, 1024);  //written to new block
+      }
+      out.close();
+
+      verifyFile(dfs, FILE_NAME, SYNC_SIZE * (TIMES + 1));
+
+      assert CloudTestHelper.matchMetadata(conf);
+      assert CloudTestHelper.findAllBlocks().size() == TIMES + 1;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestDataRace() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      final FSNamesystem fsNamesystem = cluster.getNameNode().getNamesystem();
+      final FSNamesystem fsNamesystemSpy = Mockito.spy(fsNamesystem);
+      NameNodeRpcServer rpcServer = (NameNodeRpcServer) cluster.getNameNode().getRpcServer();
+      rpcServer.setFSNamesystem(fsNamesystemSpy);
+
+      Answer delayer = new Answer() {
+        @Override
+        public Object answer(InvocationOnMock invocationOnMock) throws Throwable {
+          LOG.info("Delaying the FSYNC a bit to create a race condition");
+          Thread.sleep(2000);
+          return invocationOnMock.callRealMethod();
+        }
+      };
+
+      Mockito.doAnswer(delayer).when(fsNamesystemSpy).fsync(anyString(), anyLong(), anyString(),
+              anyLong());
+
+      final String FILE_NAME = "/dir/TEST-FLIE";
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+
+      final int TIMES = 20;
+      final int SYNC_SIZE = 1024;
+      FSDataOutputStream out = dfs.create(new Path(FILE_NAME), (short) 3);
+      HopsFSCloudTestHelper.writeData(out, 0, SYNC_SIZE);
+
+      for (int i = 1; i <= TIMES; i++) {
+        ((DFSOutputStream) out.getWrappedStream()).hsync(EnumSet
+                .of(HdfsDataOutputStream.SyncFlag.END_BLOCK));
+        HopsFSCloudTestHelper.writeData(out, i * SYNC_SIZE, 1024);  //written to new block
+      }
+      out.close();
+
+      verifyFile(dfs, FILE_NAME, SYNC_SIZE * (TIMES + 1));
+
+      assert CloudTestHelper.findAllBlocks().size() == TIMES + 1;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestConcat() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      Path paths[] = new Path[5];
+      for (int i = 0; i < paths.length; i++) {
+        paths[i] = new Path("/dir/TEST-FLIE" + i);
+        writeFile(dfs, paths[i].toString(), BLKSIZE);
+      }
+
+      //combine these files
+      int targetFileSize = 0;
+      Path merged = new Path("/dir/merged");
+      writeFile(dfs, merged.toString(), targetFileSize);
+
+      dfs.concat(merged, paths);
+
+      verifyFile(dfs, merged.toString(), BLKSIZE * paths.length + targetFileSize);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestTruncate() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      Path path = new Path("/dir/file");
+      writeFile(dfs, path.toString(), BLKSIZE * 2);
+
+      boolean isReady = dfs.truncate(path, BLKSIZE + BLKSIZE / 2);
+
+      if (!isReady) {
+        TestFileTruncate.checkBlockRecovery(path, dfs);
+      }
+
+      verifyFile(dfs, path.toString(), BLKSIZE + BLKSIZE / 2);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestTruncateSlowIncrementalBR() throws IOException {
+    Logger.getRootLogger().setLevel(Level.DEBUG);
+    HopsFSCloudTestHelper.purgeS3();
+    final Logger logger = Logger.getRootLogger();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      final FSNamesystem fsNamesystem = cluster.getNameNode().getNamesystem();
+      final FSNamesystem fsNamesystemSpy = Mockito.spy(fsNamesystem);
+      NameNodeRpcServer rpcServer = (NameNodeRpcServer) cluster.getNameNode().getRpcServer();
+      rpcServer.setFSNamesystem(fsNamesystemSpy);
+
+      Answer delayer = new Answer() {
+        @Override
+        public Object answer(InvocationOnMock invocationOnMock) throws Throwable {
+          LOG.info("Delaying the incremental block report so that" +
+                  " sync / close file does not succeed on the first try");
+          Thread.sleep(1000);
+          return invocationOnMock.callRealMethod();
+        }
+      };
+
+      Mockito.doAnswer(delayer).when(fsNamesystemSpy).
+              processIncrementalBlockReport(any(DatanodeRegistration.class),
+                      any(StorageReceivedDeletedBlocks.class));
+      final String FILE_NAME = "/dir/TEST-FLIE";
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      final LogVerificationAppender appender1 = new LogVerificationAppender();
+      logger.addAppender(appender1);
+      writeFile(dfs, FILE_NAME, BLKSIZE * 2);
+      verifyFile(dfs, FILE_NAME, BLKSIZE * 2);
+      assertTrue(getExceptionCount(appender1.getLog(), NotReplicatedYetException.class) != 0);
+
+      LOG.info("HopsFS-Cloud. Truncating file");
+      final LogVerificationAppender appender2 = new LogVerificationAppender();
+      logger.addAppender(appender2);
+      boolean isReady = dfs.truncate(new Path(FILE_NAME), BLKSIZE + (BLKSIZE / 2));
+      if (!isReady) {
+        TestFileTruncate.checkBlockRecovery(new Path(FILE_NAME), dfs);
+      }
+      assertTrue(getExceptionCount(appender2.getLog(), NotReplicatedYetException.class) != 0);
+
+      LOG.info("HopsFS-Cloud. Truncate op has completed");
+
+      verifyFile(dfs, FILE_NAME, BLKSIZE + BLKSIZE / 2);
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  /*
+  Check that upon format the default storage policy for the root
+  is set to CLOUD
+   */
+  @Test
+  public void TestFormat() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+      BlockStoragePolicy policy = dfs.getStoragePolicy(new Path("/"));
+      assertTrue("Expected: "+ HdfsConstants.CLOUD_STORAGE_POLICY_NAME+" Got: "+policy.getName(),
+              policy.getName().compareTo("CLOUD") == 0);
+
+      writeFile(dfs, "/file1", BLKSIZE);
+      policy = dfs.getStoragePolicy(new Path("/file1"));
+      assertTrue("Expected: "+ HdfsConstants.CLOUD_STORAGE_POLICY_NAME+" Got: "+policy.getName(),
+              policy.getName().compareTo("CLOUD") == 0);
+
+
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  int getExceptionCount(List<LoggingEvent> log, Class e) {
+    int count = 0;
+    for (int i = 0; i < log.size(); i++) {
+      if (log.get(i).getMessage().toString().contains(e.getSimpleName())) {
+        count++;
+      }
+    }
+    return count;
+  }
+
+  @AfterClass
+  public static void TestZDeleteAllBuckets() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+  }
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudMixStorageTypes.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudMixStorageTypes.java
new file mode 100644
index 00000000000..dee298653b4
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudMixStorageTypes.java
@@ -0,0 +1,171 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import org.apache.commons.logging.Log;
+import static org.apache.hadoop.hdfs.TestSmallFilesCreation.*;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CloudProvider;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.*;
+import org.apache.hadoop.hdfs.client.HdfsClientConfigKeys;
+import org.apache.hadoop.hdfs.server.blockmanagement.ProvidedBlocksChecker;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.*;
+import org.junit.rules.TestName;
+import org.junit.runners.MethodSorters;
+
+import java.io.IOException;
+
+import static junit.framework.TestCase.assertTrue;
+import static org.junit.Assert.fail;
+
+public class TestCloudMixStorageTypes {
+
+  static final Log LOG = LogFactory.getLog(TestCloudMixStorageTypes.class);
+
+  @Rule
+  public TestName testname = new TestName();
+
+  @Before
+  public void setup() {
+    //Logger.getLogger(ProvidedBlocksChecker.class).setLevel(Level.DEBUG);
+    //Logger.getLogger(CloudTestHelper.class).setLevel(Level.DEBUG);
+  }
+
+  @Test
+  public void TestSmallAndLargeFiles() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 5;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      final int ONDISK_SMALL_BUCKET_SIZE = FSNamesystem.getDBOnDiskSmallBucketSize();
+      final int ONDISK_MEDIUM_BUCKET_SIZE = FSNamesystem.getDBOnDiskMediumBucketSize();
+      final int MAX_SMALL_FILE_SIZE = FSNamesystem.getMaxSmallFileSize();
+      final int INMEMORY_BUCKET_SIZE = FSNamesystem.getDBInMemBucketSize();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/"), "CLOUD");
+      dfs.setStoragePolicy(new Path("/dir"), "DB");
+
+      final String FILE_NAME1 = "/dir/TEST-FLIE1";
+      final String FILE_NAME2 = "/dir/TEST-FLIE2";
+      final String FILE_NAME3 = "/dir/TEST-FLIE3";
+      final String FILE_NAME4 = "/dir/TEST-FLIE4";
+      final String FILE_NAME5 = "/dir/TEST-FLIE5";
+      final String FILE_NAME6 = "/dir/TEST-FLIE6";
+
+      //write small files
+      writeFile(dfs, FILE_NAME1, INMEMORY_BUCKET_SIZE);
+      verifyFile(dfs, FILE_NAME1, INMEMORY_BUCKET_SIZE);
+      writeFile(dfs, FILE_NAME2, ONDISK_SMALL_BUCKET_SIZE);
+      verifyFile(dfs, FILE_NAME2, ONDISK_SMALL_BUCKET_SIZE);
+      writeFile(dfs, FILE_NAME3, ONDISK_MEDIUM_BUCKET_SIZE);
+      verifyFile(dfs, FILE_NAME3, ONDISK_MEDIUM_BUCKET_SIZE);
+      writeFile(dfs, FILE_NAME4, MAX_SMALL_FILE_SIZE);
+      verifyFile(dfs, FILE_NAME4, MAX_SMALL_FILE_SIZE);
+
+      //now write large files. these should be stored in the cloud
+      writeFile(dfs, FILE_NAME5, MAX_SMALL_FILE_SIZE + 1); // 1 block
+      verifyFile(dfs, FILE_NAME5, MAX_SMALL_FILE_SIZE + 1);
+      writeFile(dfs, FILE_NAME6, 10 * BLKSIZE);  // 10 blocks
+      verifyFile(dfs, FILE_NAME6, 10 * BLKSIZE);
+      assert CloudTestHelper.findAllBlocks().size() == 11;  // 11 blocks so far
+
+      //validate
+      assertTrue("Expecting 1 in-memory file. Got: " + countInMemoryDBFiles(), countInMemoryDBFiles() == 1);
+      assertTrue("Expecting 3 on-disk file(s). Got:" + countAllOnDiskDBFiles(), countAllOnDiskDBFiles() == 3);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskSmallDBFiles(), countOnDiskSmallDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskMediumDBFiles(), countOnDiskMediumDBFiles() == 1);
+      assertTrue("Expecting 1 on-disk file(s). Got:" + countOnDiskLargeDBFiles(), countOnDiskLargeDBFiles() == 1);
+
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @Test
+  public void TestHotStorage() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+      Configuration conf = new HdfsConfiguration();
+      final int NUM_DN = 1;
+      final int BLKSIZE = 128 * 1024;
+
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN).
+              storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/"), "CLOUD");
+      dfs.setStoragePolicy(new Path("/dir"), "HOT");
+
+      final String FILE_NAME1 = "/dir/TEST-FLIE1";
+
+      writeFile(dfs, FILE_NAME1, BLKSIZE*10); //replication is 1
+      verifyFile(dfs, FILE_NAME1, BLKSIZE*10);
+
+      assert CloudTestHelper.findAllBlocks().size() == 10;
+      //hot files are stored on DN.
+      assert CloudTestHelper.findAllReplicas().size() == 10;
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @AfterClass
+  public static void TestZDeleteAllBuckets() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+  }
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudRestart.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudRestart.java
new file mode 100644
index 00000000000..1fb7ee198f5
--- /dev/null
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCloudRestart.java
@@ -0,0 +1,116 @@
+/*
+ * Copyright (C) 2019 LogicalClocks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CloudProvider;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.*;
+import org.apache.hadoop.hdfs.server.blockmanagement.ProvidedBlocksChecker;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.junit.*;
+import org.junit.rules.TestName;
+import org.junit.runners.MethodSorters;
+
+import java.io.IOException;
+
+import static junit.framework.TestCase.assertTrue;
+import static org.apache.hadoop.hdfs.TestSmallFilesCreation.writeFile;
+import static org.apache.hadoop.hdfs.server.namenode.HopsFSCloudTestHelper.verifyFile;
+import static org.junit.Assert.fail;
+
+public class TestCloudRestart {
+
+  static final Log LOG = LogFactory.getLog(TestCloudRestart.class);
+  @Rule
+  public TestName testname = new TestName();
+
+  @Before
+  public void setup() {
+    Logger.getLogger(ProvidedBlocksChecker.class).setLevel(Level.DEBUG);
+  }
+
+  @Test
+  public void TestSimpleRestart() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+    MiniDFSCluster cluster = null;
+    try {
+
+      final int BLKSIZE = 128 * 1024;
+      final int NUM_DN = 3;
+
+      Configuration conf = new HdfsConfiguration();
+      conf.setBoolean(DFSConfigKeys.DFS_ENABLE_CLOUD_PERSISTENCE, true);
+      conf.set(DFSConfigKeys.DFS_CLOUD_PROVIDER, CloudProvider.AWS.name());
+      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLKSIZE);
+
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_THREAD_SLEEP_INTERVAL_KEY, 1000);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_PREFIX_SIZE_KEY, 10);
+      conf.setInt(DFSConfigKeys.DFS_CLOUD_AWS_S3_NUM_BUCKETS, 2);
+      conf.setLong(DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_KEY,
+              DFSConfigKeys.DFS_CLOUD_BLOCK_REPORT_DELAY_DEFAULT);
+      conf.setLong(DFSConfigKeys.DFS_NAMENODE_BLOCKID_BATCH_SIZE, 10);
+
+      HopsFSCloudTestHelper.setRandomBucketPrefix(conf, testname);
+
+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DN)
+              .storageTypes(HopsFSCloudTestHelper.genStorageTypes(NUM_DN)).format(true).build();
+      cluster.waitActive();
+
+      DistributedFileSystem dfs = cluster.getFileSystem();
+
+      ProvidedBlocksChecker pbc =
+              cluster.getNamesystem().getBlockManager().getProvidedBlocksChecker();
+
+      long ret = CloudBlockReportTestHelper.waitForBRCompletion(pbc, 1);
+      assertTrue("Exptected 1. Got: " + ret, 1 == ret);
+
+      dfs.mkdirs(new Path("/dir"));
+      dfs.setStoragePolicy(new Path("/dir"), "CLOUD");
+
+      for (int i = 0; i < 10; i++) {
+        writeFile(dfs, "/dir/file" + i, BLKSIZE * 2);
+        verifyFile(dfs, "/dir/file" + i, BLKSIZE * 2);
+      }
+      CloudTestHelper.matchMetadata(conf);
+
+      cluster.restartNameNodes();
+      cluster.waitActive();
+
+      for (int i = 0; i < 10; i++) {
+        writeFile(dfs, "/dir/file-afterrestart" + i, BLKSIZE * 2);
+        verifyFile(dfs, "/dir/file-afterrestart" + i, BLKSIZE * 2);
+      }
+
+    } catch (Exception e) {
+      e.printStackTrace();
+      fail(e.getMessage());
+    } finally {
+      if (cluster != null) {
+        cluster.shutdown();
+      }
+    }
+  }
+
+  @AfterClass
+  public static void TestZDeleteAllBuckets() throws IOException {
+    HopsFSCloudTestHelper.purgeS3();
+  }
+
+}
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
index f63db924d34..25be0e6a942 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java
@@ -55,6 +55,7 @@
   private static final long blockId = 100;
   private static final long length = 200;
   private static final long genStamp = 300;
+  private static final short NON_EXISTING_BUCKET_ID = Block.NON_EXISTING_BUCKET_ID;
 
   private MiniDFSCluster cluster;
   @Before
@@ -152,7 +153,7 @@ private INodeFile mockFileUnderConstruction() {
   @Test
   public void testCommitBlockSynchronization() throws IOException {
     INodeFile file = mockFileUnderConstruction();
-    Block block = new Block(blockId, length, genStamp);
+    Block block = new Block(blockId, length, genStamp, Block.NON_EXISTING_BUCKET_ID);
     FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);
     DatanodeID[] newTargets = new DatanodeID[0];
 
@@ -220,7 +221,7 @@ public Object performTask() throws IOException {
   public void testCommitBlockSynchronization2() throws IOException {
     INodeFile file = mockFileUnderConstruction();
     
-    Block block = new Block(blockId, length, genStamp);
+    Block block = new Block(blockId, length, genStamp, NON_EXISTING_BUCKET_ID);
     FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);
     DatanodeID[] newTargets = new DatanodeID[0];
 
@@ -245,7 +246,7 @@ public void testCommitBlockSynchronization2() throws IOException {
   @Test
   public void testCommitBlockSynchronizationWithDelete() throws IOException {
     INodeFile file = mockFileUnderConstruction();
-    Block block = new Block(blockId, length, genStamp);
+    Block block = new Block(blockId, length, genStamp, NON_EXISTING_BUCKET_ID);
     FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);
     DatanodeDescriptor[] targets = new DatanodeDescriptor[0];
     DatanodeID[] newTargets = new DatanodeID[0];
@@ -266,7 +267,7 @@ public void testCommitBlockSynchronizationWithDelete() throws IOException {
   @Test
   public void testCommitBlockSynchronizationWithClose() throws IOException {
     INodeFile file = mockFileUnderConstruction();
-    Block block = new Block(blockId, length, genStamp);
+    Block block = new Block(blockId, length, genStamp, NON_EXISTING_BUCKET_ID);
     FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);
     DatanodeDescriptor[] targets = new DatanodeDescriptor[0];
     DatanodeID[] newTargets = new DatanodeID[0];
@@ -299,7 +300,7 @@ public void testCommitBlockSynchronizationWithClose() throws IOException {
   public void testCommitBlockSynchronizationWithCloseAndNonExistantTarget()
       throws IOException {
     INodeFile file = mockFileUnderConstruction();
-    Block block = new Block(blockId, length, genStamp);
+    Block block = new Block(blockId, length, genStamp, NON_EXISTING_BUCKET_ID);
     FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);
     DatanodeID[] newTargets = new DatanodeID[]{
         new DatanodeID("0.0.0.0", "nonexistantHost", "1", 0, 0, 0, 0)};
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/log4j.properties b/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/log4j.properties
index 5d76ad74fae..1fe485cdaf3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/log4j.properties
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/log4j.properties
@@ -17,12 +17,13 @@
 # log4j configuration used during build and unit tests
 
 # Define the root logger to the system property "hadoop.root.logger".
-hadoop.root.logger=info,stdout
+hadoop.root.logger=INFO,stdout
 log4j.threshold=ALL
 log4j.rootLogger=${hadoop.root.logger}
 log4j.hadoop.root.logger=${hadoop.root.logger}
 
 log4j.logger.org.apache.hadoop.hdfs.server.datanode.*=${hadoop.root.logger}
+log4j.logger.com.amazonaws=ERROR
 #log4j.logger.org.apache.hadoop.hdfs.StateChange=${hadoop.root.logger}
 #log4j.additivity.org.apache.hadoop.hdfs.StateChange=false
 #log4j.additivity.org.apache.hadoop.hdfs.BlockStateChange=false
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index f380585cc12..225195ba05d 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -467,7 +467,7 @@
       <dependency>
         <groupId>org.apache.httpcomponents</groupId>
         <artifactId>httpclient</artifactId>
-        <version>4.5.2</version>
+        <version>4.5.9</version>
       </dependency>
       <dependency>
         <groupId>org.apache.httpcomponents</groupId>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
index b20b1e2fc56..6c344610d4b 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.yarn.server.resourcemanager.scheduler;
 
-import com.sun.tools.javac.util.Convert;
 import java.nio.charset.StandardCharsets;
 import java.security.MessageDigest;
 import java.security.NoSuchAlgorithmException;
